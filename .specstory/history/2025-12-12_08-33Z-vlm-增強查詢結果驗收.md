<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 01e462d8-23a9-44de-8abd-5b714ebd28a4 (2025-12-12 08:33Z) -->

# VLM 增強查詢結果驗收 (2025-12-12 08:33Z)

_**User**_

@eval/demo-251212-vlm.log @eval/integration/results/vlm_enhanced_query/20251212-162407/results.json 
請協助根據結果驗收下列項目
- VLM 增強查詢能正確識別圖片內容
- VLM 增強查詢的回應品質優於純文字查詢
- 圖片路徑轉換與編碼正確率 
- 無圖片時能正確降級，不產生錯誤
- 多圖片查詢能正確處理

---

_**Agent (model default, mode Agent)**_



---


讀取日誌與結果檔，開始驗收。


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: eval/demo-251212-vlm.log</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: eval/integration/results/vlm_enhanced_query/20251212-162407/results.json</summary>

</details>
</tool-use>

---



---


分析結果並整理驗收報告。


---

<tool-use data-tool-type="unknown" data-tool-name="semantic_search_full">
<details>
<summary>Tool use: **semantic_search_full**</summary>


Parameters:

```json
{
  "query": "How does VLM enhanced query handle image path conversion and encoding?",
  "topK": 10,
  "explanation": ""
}
```
Result:

```json
{
  "codeResults": [
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:\ndef replace_image_path(match):\n  # Keep original if validation fails\n\n            try:\n                # Encode image to base64 using utility function\n                self.logger.debug(f\"Attempting to encode image: {image_path}\")\n                image_base64 = encode_image_to_base64(image_path)\n                if image_base64:\n                    images_processed += 1\n                    # Save base64 to instance variable for later use\n                    self._current_images_base64.append(image_base64)\n\n                    # Keep original path info and add VLM marker\n                    result = f\"Image Path: {image_path}\\n[VLM_IMAGE_{images_processed}]\"\n                    self.logger.debug(\n                        f\"Successfully processed image {images_processed}: {image_path}\"\n                    )\n                    return result\n                else:\n                    self.logger.error(f\"Failed to encode image: {image_path}\")\n                    return match.group(0)  # Keep original if encoding failed\n\n            except Exception as e:\n                self.logger.error(f\"Failed to process image {image_path}: {e}\")\n                return match.group(0)  # Keep original\n\n        # Execute replacement\n        enhanced_prompt = re.sub(image_path_pattern, replace_image_path, enhanced_prompt)\n\n        return enhanced_prompt, images_processed\n\n    def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 482,
                "column": 5
              },
              "endPosition": {
                "line": 483,
                "column": 9
              }
            },
            {
              "startPosition": {
                "line": 511,
                "column": 9
              },
              "endPosition": {
                "line": 512,
                "column": 13
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:",
            "lineNumber": 482
          },
          {
            "text": "def replace_image_path(match):",
            "lineNumber": 511
          },
          {
            "text": "  # Keep original if validation fails",
            "lineNumber": 529
          },
          {
            "lineNumber": 530
          },
          {
            "text": "            try:",
            "lineNumber": 531
          },
          {
            "text": "                # Encode image to base64 using utility function",
            "lineNumber": 532
          },
          {
            "text": "                self.logger.debug(f\"Attempting to encode image: {image_path}\")",
            "lineNumber": 533
          },
          {
            "text": "                image_base64 = encode_image_to_base64(image_path)",
            "lineNumber": 534
          },
          {
            "text": "                if image_base64:",
            "lineNumber": 535
          },
          {
            "text": "                    images_processed += 1",
            "lineNumber": 536
          },
          {
            "text": "                    # Save base64 to instance variable for later use",
            "lineNumber": 537
          },
          {
            "text": "                    self._current_images_base64.append(image_base64)",
            "lineNumber": 538
          },
          {
            "lineNumber": 539
          },
          {
            "text": "                    # Keep original path info and add VLM marker",
            "lineNumber": 540
          },
          {
            "text": "                    result = f\"Image Path: {image_path}\\n[VLM_IMAGE_{images_processed}]\"",
            "lineNumber": 541
          },
          {
            "text": "                    self.logger.debug(",
            "lineNumber": 542
          },
          {
            "text": "                        f\"Successfully processed image {images_processed}: {image_path}\"",
            "lineNumber": 543
          },
          {
            "text": "                    )",
            "lineNumber": 544
          },
          {
            "text": "                    return result",
            "lineNumber": 545
          },
          {
            "text": "                else:",
            "lineNumber": 546
          },
          {
            "text": "                    self.logger.error(f\"Failed to encode image: {image_path}\")",
            "lineNumber": 547
          },
          {
            "text": "                    return match.group(0)  # Keep original if encoding failed",
            "lineNumber": 548
          },
          {
            "lineNumber": 549
          },
          {
            "text": "            except Exception as e:",
            "lineNumber": 550
          },
          {
            "text": "                self.logger.error(f\"Failed to process image {image_path}: {e}\")",
            "lineNumber": 551
          },
          {
            "text": "                return match.group(0)  # Keep original",
            "lineNumber": 552
          },
          {
            "lineNumber": 553
          },
          {
            "text": "        # Execute replacement",
            "lineNumber": 554
          },
          {
            "text": "        enhanced_prompt = re.sub(image_path_pattern, replace_image_path, enhanced_prompt)",
            "lineNumber": 555
          },
          {
            "lineNumber": 556
          },
          {
            "text": "        return enhanced_prompt, images_processed",
            "lineNumber": 557
          },
          {
            "lineNumber": 558
          },
          {
            "text": "    def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:",
            "lineNumber": 559
          },
          {
            "text": "        ",
            "lineNumber": 560
          }
        ]
      },
      "score": 0.6045122742652893
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\n\n\n    async def aquery_vlm_enhanced(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:\n        \"\"\"\n        VLM enhanced query - replaces image paths in retrieved context with base64 encoded images for VLM processing\n\n        Args:\n            query: User query\n            mode: Underlying LightRAG query mode\n            **kwargs: Other query parameters\n\n        Returns:\n            str: VLM query result\n        \"\"\"\n        # Ensure VLM is available\n        if not hasattr(self, \"vision_model_func\") or not self.vision_model_func:\n            raise ValueError(\n                \"VLM enhanced query requires vision_model_func. \"\n                \"Please provide a vision model function when initializing RAGAnything.\"\n            )\n\n        # Ensure LightRAG is initialized\n        await self._ensure_lightrag_initialized()\n\n        self.logger.info(f\"Executing VLM enhanced query: {query[:100]}...\")\n\n        # Clear previous image cache\n        if hasattr(self, \"_current_images_base64\"):\n            delattr(self, \"_current_images_base64\")\n\n        # 1. Get original retrieval prompt (without generating final answer)\n        query_param = QueryParam(mode=mode, only_need_prompt=True, **kwargs)\n        raw_prompt = await self.lightrag.aquery(query, param=query_param)\n\n        self.logger.debug(\"Retrieved raw prompt from LightRAG\")\n\n        # Check if raw_prompt is None or empty\n        if not raw_prompt or not isinstance(raw_prompt, str):\n            self.logger.warning(\"Raw prompt is None or not a string, falling back to normal query\")\n            query_param = QueryParam(mode=mode, **kwargs)\n            return await self.lightrag.aquery(query, param=query_param)\n\n        # 2. Extract and process image paths\n        enhanced_prompt, images_found = await self._process_image_paths_for_vlm(raw_prompt)\n\n        if not images_found:\n            self.logger.info",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "lineNumber": 267
          },
          {
            "lineNumber": 268
          },
          {
            "text": "    async def aquery_vlm_enhanced(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:",
            "lineNumber": 269
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 270
          },
          {
            "text": "        VLM enhanced query - replaces image paths in retrieved context with base64 encoded images for VLM processing",
            "lineNumber": 271
          },
          {
            "lineNumber": 272
          },
          {
            "text": "        Args:",
            "lineNumber": 273
          },
          {
            "text": "            query: User query",
            "lineNumber": 274
          },
          {
            "text": "            mode: Underlying LightRAG query mode",
            "lineNumber": 275
          },
          {
            "text": "            **kwargs: Other query parameters",
            "lineNumber": 276
          },
          {
            "lineNumber": 277
          },
          {
            "text": "        Returns:",
            "lineNumber": 278
          },
          {
            "text": "            str: VLM query result",
            "lineNumber": 279
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 280
          },
          {
            "text": "        # Ensure VLM is available",
            "lineNumber": 281
          },
          {
            "text": "        if not hasattr(self, \"vision_model_func\") or not self.vision_model_func:",
            "lineNumber": 282
          },
          {
            "text": "            raise ValueError(",
            "lineNumber": 283
          },
          {
            "text": "                \"VLM enhanced query requires vision_model_func. \"",
            "lineNumber": 284
          },
          {
            "text": "                \"Please provide a vision model function when initializing RAGAnything.\"",
            "lineNumber": 285
          },
          {
            "text": "            )",
            "lineNumber": 286
          },
          {
            "lineNumber": 287
          },
          {
            "text": "        # Ensure LightRAG is initialized",
            "lineNumber": 288
          },
          {
            "text": "        await self._ensure_lightrag_initialized()",
            "lineNumber": 289
          },
          {
            "lineNumber": 290
          },
          {
            "text": "        self.logger.info(f\"Executing VLM enhanced query: {query[:100]}...\")",
            "lineNumber": 291
          },
          {
            "lineNumber": 292
          },
          {
            "text": "        # Clear previous image cache",
            "lineNumber": 293
          },
          {
            "text": "        if hasattr(self, \"_current_images_base64\"):",
            "lineNumber": 294
          },
          {
            "text": "            delattr(self, \"_current_images_base64\")",
            "lineNumber": 295
          },
          {
            "lineNumber": 296
          },
          {
            "text": "        # 1. Get original retrieval prompt (without generating final answer)",
            "lineNumber": 297
          },
          {
            "text": "        query_param = QueryParam(mode=mode, only_need_prompt=True, **kwargs)",
            "lineNumber": 298
          },
          {
            "text": "        raw_prompt = await self.lightrag.aquery(query, param=query_param)",
            "lineNumber": 299
          },
          {
            "lineNumber": 300
          },
          {
            "text": "        self.logger.debug(\"Retrieved raw prompt from LightRAG\")",
            "lineNumber": 301
          },
          {
            "lineNumber": 302
          },
          {
            "text": "        # Check if raw_prompt is None or empty",
            "lineNumber": 303
          },
          {
            "text": "        if not raw_prompt or not isinstance(raw_prompt, str):",
            "lineNumber": 304
          },
          {
            "text": "            self.logger.warning(\"Raw prompt is None or not a string, falling back to normal query\")",
            "lineNumber": 305
          },
          {
            "text": "            query_param = QueryParam(mode=mode, **kwargs)",
            "lineNumber": 306
          },
          {
            "text": "            return await self.lightrag.aquery(query, param=query_param)",
            "lineNumber": 307
          },
          {
            "lineNumber": 308
          },
          {
            "text": "        # 2. Extract and process image paths",
            "lineNumber": 309
          },
          {
            "text": "        enhanced_prompt, images_found = await self._process_image_paths_for_vlm(raw_prompt)",
            "lineNumber": 310
          },
          {
            "lineNumber": 311
          },
          {
            "text": "        if not images_found:",
            "lineNumber": 312
          },
          {
            "text": "            self.logger.info",
            "lineNumber": 313
          }
        ]
      },
      "score": 0.5808663964271545
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def _describe_generic_for_query(\n        self, processor, content: Dict[str, Any], content_type: str\n    ) -&gt; str:\n\n        )\n\n        return description\n\n    async def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:\n        \"\"\"\n        Process image paths in prompt, keeping original paths and adding VLM markers\n\n        Args:\n            prompt: Original prompt\n\n        Returns:\n            tuple: (processed prompt, image count)\n        \"\"\"\n        # Validate prompt\n        if not prompt or not isinstance(prompt, str):\n            self.logger.warning(\"Invalid prompt provided to _process_image_paths_for_vlm\")\n            return \"\", 0\n\n        enhanced_prompt = prompt\n        images_processed = 0\n\n        # Initialize image cache\n        self._current_images_base64 = []\n\n        # Enhanced regex pattern for matching image paths\n        # Matches only the path ending with image file extensions\n        image_path_pattern = r\"Image Path:\\s*([^\\r\\n]*?\\.(?:jpg|jpeg|png|gif|bmp|webp|tiff|tif))\"\n\n        # First, let's see what matches we find\n        matches = re.findall(image_path_pattern, prompt)\n        self.logger.info(f\"Found {len(matches)} image path matches in prompt\")\n\n        def replace_image_path(match):\n            nonlocal images_processed",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def _describe_generic_for_query(",
            "lineNumber": 465
          },
          {
            "text": "        self, processor, content: Dict[str, Any], content_type: str",
            "lineNumber": 466
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 467
          },
          {
            "lineNumber": 477
          },
          {
            "text": "        )",
            "lineNumber": 478
          },
          {
            "lineNumber": 479
          },
          {
            "text": "        return description",
            "lineNumber": 480
          },
          {
            "lineNumber": 481
          },
          {
            "text": "    async def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:",
            "lineNumber": 482
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 483
          },
          {
            "text": "        Process image paths in prompt, keeping original paths and adding VLM markers",
            "lineNumber": 484
          },
          {
            "lineNumber": 485
          },
          {
            "text": "        Args:",
            "lineNumber": 486
          },
          {
            "text": "            prompt: Original prompt",
            "lineNumber": 487
          },
          {
            "lineNumber": 488
          },
          {
            "text": "        Returns:",
            "lineNumber": 489
          },
          {
            "text": "            tuple: (processed prompt, image count)",
            "lineNumber": 490
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 491
          },
          {
            "text": "        # Validate prompt",
            "lineNumber": 492
          },
          {
            "text": "        if not prompt or not isinstance(prompt, str):",
            "lineNumber": 493
          },
          {
            "text": "            self.logger.warning(\"Invalid prompt provided to _process_image_paths_for_vlm\")",
            "lineNumber": 494
          },
          {
            "text": "            return \"\", 0",
            "lineNumber": 495
          },
          {
            "lineNumber": 496
          },
          {
            "text": "        enhanced_prompt = prompt",
            "lineNumber": 497
          },
          {
            "text": "        images_processed = 0",
            "lineNumber": 498
          },
          {
            "lineNumber": 499
          },
          {
            "text": "        # Initialize image cache",
            "lineNumber": 500
          },
          {
            "text": "        self._current_images_base64 = []",
            "lineNumber": 501
          },
          {
            "lineNumber": 502
          },
          {
            "text": "        # Enhanced regex pattern for matching image paths",
            "lineNumber": 503
          },
          {
            "text": "        # Matches only the path ending with image file extensions",
            "lineNumber": 504
          },
          {
            "text": "        image_path_pattern = r\"Image Path:\\s*([^\\r\\n]*?\\.(?:jpg|jpeg|png|gif|bmp|webp|tiff|tif))\"",
            "lineNumber": 505
          },
          {
            "lineNumber": 506
          },
          {
            "text": "        # First, let's see what matches we find",
            "lineNumber": 507
          },
          {
            "text": "        matches = re.findall(image_path_pattern, prompt)",
            "lineNumber": 508
          },
          {
            "text": "        self.logger.info(f\"Found {len(matches)} image path matches in prompt\")",
            "lineNumber": 509
          },
          {
            "lineNumber": 510
          },
          {
            "text": "        def replace_image_path(match):",
            "lineNumber": 511
          },
          {
            "text": "            nonlocal images_processed",
            "lineNumber": 512
          }
        ]
      },
      "score": 0.5285106301307678
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:\n\n\n        def replace_image_path(match):\n            nonlocal images_processed\n\n            image_path = match.group(1).strip()\n            self.logger.debug(f\"Processing image path: '{image_path}'\")\n\n            # Validate path format (basic check)\n            if not image_path or len(image_path) &lt; 3:\n                self.logger.warning(f\"Invalid image path format: {image_path}\")\n                return match.group(0)  # Keep original\n\n            # Use utility function to validate image file\n            self.logger.debug(f\"Calling validate_image_file for: {image_path}\")\n            is_valid = validate_image_file(image_path)\n            self.logger.debug(f\"Validation result for {image_path}: {is_valid}\")\n\n            if not is_valid:\n                self.logger.warning(f\"Image validation failed for: {image_path}\")\n                return match.group(0)  # Keep original if validation fails\n\n            try:",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 482,
                "column": 5
              },
              "endPosition": {
                "line": 483,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:",
            "lineNumber": 482
          },
          {
            "lineNumber": 509
          },
          {
            "lineNumber": 510
          },
          {
            "text": "        def replace_image_path(match):",
            "lineNumber": 511
          },
          {
            "text": "            nonlocal images_processed",
            "lineNumber": 512
          },
          {
            "lineNumber": 513
          },
          {
            "text": "            image_path = match.group(1).strip()",
            "lineNumber": 514
          },
          {
            "text": "            self.logger.debug(f\"Processing image path: '{image_path}'\")",
            "lineNumber": 515
          },
          {
            "lineNumber": 516
          },
          {
            "text": "            # Validate path format (basic check)",
            "lineNumber": 517
          },
          {
            "text": "            if not image_path or len(image_path) &lt; 3:",
            "lineNumber": 518
          },
          {
            "text": "                self.logger.warning(f\"Invalid image path format: {image_path}\")",
            "lineNumber": 519
          },
          {
            "text": "                return match.group(0)  # Keep original",
            "lineNumber": 520
          },
          {
            "lineNumber": 521
          },
          {
            "text": "            # Use utility function to validate image file",
            "lineNumber": 522
          },
          {
            "text": "            self.logger.debug(f\"Calling validate_image_file for: {image_path}\")",
            "lineNumber": 523
          },
          {
            "text": "            is_valid = validate_image_file(image_path)",
            "lineNumber": 524
          },
          {
            "text": "            self.logger.debug(f\"Validation result for {image_path}: {is_valid}\")",
            "lineNumber": 525
          },
          {
            "lineNumber": 526
          },
          {
            "text": "            if not is_valid:",
            "lineNumber": 527
          },
          {
            "text": "                self.logger.warning(f\"Image validation failed for: {image_path}\")",
            "lineNumber": 528
          },
          {
            "text": "                return match.group(0)  # Keep original if validation fails",
            "lineNumber": 529
          },
          {
            "lineNumber": 530
          },
          {
            "text": "            try:",
            "lineNumber": 531
          }
        ]
      },
      "score": 0.4759601354598999
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\n\n\n    def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:\n        \"\"\"\n        Build VLM message format, using markers to correspond images with text positions\n\n        Args:\n            enhanced_prompt: Enhanced prompt with image markers\n            user_query: User query\n\n        Returns:\n            List[Dict]: VLM message format\n        \"\"\"\n        images_base64 = getattr(self, \"_current_images_base64\", [])\n\n        if not images_base64:\n            # Pure text mode\n            return [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Context:\\n{enhanced_prompt}\\n\\nUser Question: {user_query}\",\n                }\n            ]\n\n        # Build multimodal content\n        content_parts = []\n\n        # Split text at image markers and insert images\n        text_parts = enhanced_prompt.split(\"[VLM_IMAGE_\")\n\n        for i, text_part in enumerate(text_parts):",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "lineNumber": 557
          },
          {
            "lineNumber": 558
          },
          {
            "text": "    def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:",
            "lineNumber": 559
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 560
          },
          {
            "text": "        Build VLM message format, using markers to correspond images with text positions",
            "lineNumber": 561
          },
          {
            "lineNumber": 562
          },
          {
            "text": "        Args:",
            "lineNumber": 563
          },
          {
            "text": "            enhanced_prompt: Enhanced prompt with image markers",
            "lineNumber": 564
          },
          {
            "text": "            user_query: User query",
            "lineNumber": 565
          },
          {
            "lineNumber": 566
          },
          {
            "text": "        Returns:",
            "lineNumber": 567
          },
          {
            "text": "            List[Dict]: VLM message format",
            "lineNumber": 568
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 569
          },
          {
            "text": "        images_base64 = getattr(self, \"_current_images_base64\", [])",
            "lineNumber": 570
          },
          {
            "lineNumber": 571
          },
          {
            "text": "        if not images_base64:",
            "lineNumber": 572
          },
          {
            "text": "            # Pure text mode",
            "lineNumber": 573
          },
          {
            "text": "            return [",
            "lineNumber": 574
          },
          {
            "text": "                {",
            "lineNumber": 575
          },
          {
            "text": "                    \"role\": \"user\",",
            "lineNumber": 576
          },
          {
            "text": "                    \"content\": f\"Context:\\n{enhanced_prompt}\\n\\nUser Question: {user_query}\",",
            "lineNumber": 577
          },
          {
            "text": "                }",
            "lineNumber": 578
          },
          {
            "text": "            ]",
            "lineNumber": 579
          },
          {
            "lineNumber": 580
          },
          {
            "text": "        # Build multimodal content",
            "lineNumber": 581
          },
          {
            "text": "        content_parts = []",
            "lineNumber": 582
          },
          {
            "lineNumber": 583
          },
          {
            "text": "        # Split text at image markers and insert images",
            "lineNumber": 584
          },
          {
            "text": "        text_parts = enhanced_prompt.split(\"[VLM_IMAGE_\")",
            "lineNumber": 585
          },
          {
            "lineNumber": 586
          },
          {
            "text": "        for i, text_part in enumerate(text_parts):",
            "lineNumber": 587
          }
        ]
      },
      "score": 0.4489581882953644
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\ndef _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:\n\n\n        for i, text_part in enumerate(text_parts):\n            if i == 0:\n                # First text part\n                if text_part.strip():\n                    content_parts.append({\"type\": \"text\", \"text\": text_part})\n            else:\n                # Find marker number and insert corresponding image\n                marker_match = re.match(r\"(\\d+)\\](.*)\", text_part, re.DOTALL)\n                if marker_match:\n                    image_num = int(marker_match.group(1)) - 1  # Convert to 0-based index\n                    remaining_text = marker_match.group(2)\n\n                    # Insert corresponding image\n                    if 0 &lt;= image_num &lt; len(images_base64):\n                        content_parts.append(\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{images_base64[image_num]}\"\n                                },\n                            }\n                        )\n\n                    # Insert remaining text\n                    if remaining_text.strip():\n                        content_parts.append({\"type\": \"text\", \"text\": remaining_text})\n\n        # Add user question\n        content_parts.append(\n            {\n                \"type\": \"text\",\n                \"text\": f\"\\n\\nUser Question: {user_query}\\n\\nPlease answer based on the context and images provided.\",\n            }\n        )\n\n        return [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that can analyze both text and image content to provide comprehensive answers.\",\n            },\n            {\"role\": \"user\", \"content\": content_parts},\n        ]\n\n    async def _call_vlm_with_multimodal_content(self, messages: List[Dict]) -&gt; str:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 559,
                "column": 5
              },
              "endPosition": {
                "line": 560,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:",
            "lineNumber": 559
          },
          {
            "lineNumber": 585
          },
          {
            "lineNumber": 586
          },
          {
            "text": "        for i, text_part in enumerate(text_parts):",
            "lineNumber": 587
          },
          {
            "text": "            if i == 0:",
            "lineNumber": 588
          },
          {
            "text": "                # First text part",
            "lineNumber": 589
          },
          {
            "text": "                if text_part.strip():",
            "lineNumber": 590
          },
          {
            "text": "                    content_parts.append({\"type\": \"text\", \"text\": text_part})",
            "lineNumber": 591
          },
          {
            "text": "            else:",
            "lineNumber": 592
          },
          {
            "text": "                # Find marker number and insert corresponding image",
            "lineNumber": 593
          },
          {
            "text": "                marker_match = re.match(r\"(\\d+)\\](.*)\", text_part, re.DOTALL)",
            "lineNumber": 594
          },
          {
            "text": "                if marker_match:",
            "lineNumber": 595
          },
          {
            "text": "                    image_num = int(marker_match.group(1)) - 1  # Convert to 0-based index",
            "lineNumber": 596
          },
          {
            "text": "                    remaining_text = marker_match.group(2)",
            "lineNumber": 597
          },
          {
            "lineNumber": 598
          },
          {
            "text": "                    # Insert corresponding image",
            "lineNumber": 599
          },
          {
            "text": "                    if 0 &lt;= image_num &lt; len(images_base64):",
            "lineNumber": 600
          },
          {
            "text": "                        content_parts.append(",
            "lineNumber": 601
          },
          {
            "text": "                            {",
            "lineNumber": 602
          },
          {
            "text": "                                \"type\": \"image_url\",",
            "lineNumber": 603
          },
          {
            "text": "                                \"image_url\": {",
            "lineNumber": 604
          },
          {
            "text": "                                    \"url\": f\"data:image/jpeg;base64,{images_base64[image_num]}\"",
            "lineNumber": 605
          },
          {
            "text": "                                },",
            "lineNumber": 606
          },
          {
            "text": "                            }",
            "lineNumber": 607
          },
          {
            "text": "                        )",
            "lineNumber": 608
          },
          {
            "lineNumber": 609
          },
          {
            "text": "                    # Insert remaining text",
            "lineNumber": 610
          },
          {
            "text": "                    if remaining_text.strip():",
            "lineNumber": 611
          },
          {
            "text": "                        content_parts.append({\"type\": \"text\", \"text\": remaining_text})",
            "lineNumber": 612
          },
          {
            "lineNumber": 613
          },
          {
            "text": "        # Add user question",
            "lineNumber": 614
          },
          {
            "text": "        content_parts.append(",
            "lineNumber": 615
          },
          {
            "text": "            {",
            "lineNumber": 616
          },
          {
            "text": "                \"type\": \"text\",",
            "lineNumber": 617
          },
          {
            "text": "                \"text\": f\"\\n\\nUser Question: {user_query}\\n\\nPlease answer based on the context and images provided.\",",
            "lineNumber": 618
          },
          {
            "text": "            }",
            "lineNumber": 619
          },
          {
            "text": "        )",
            "lineNumber": 620
          },
          {
            "lineNumber": 621
          },
          {
            "text": "        return [",
            "lineNumber": 622
          },
          {
            "text": "            {",
            "lineNumber": 623
          },
          {
            "text": "                \"role\": \"system\",",
            "lineNumber": 624
          },
          {
            "text": "                \"content\": \"You are a helpful assistant that can analyze both text and image content to provide comprehensive answers.\",",
            "lineNumber": 625
          },
          {
            "text": "            },",
            "lineNumber": 626
          },
          {
            "text": "            {\"role\": \"user\", \"content\": content_parts},",
            "lineNumber": 627
          },
          {
            "text": "        ]",
            "lineNumber": 628
          },
          {
            "lineNumber": 629
          },
          {
            "text": "    async def _call_vlm_with_multimodal_content(self, messages: List[Dict]) -&gt; str:",
            "lineNumber": 630
          },
          {
            "text": "        ",
            "lineNumber": 631
          }
        ]
      },
      "score": 0.44745609164237976
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {
            "line": 140
          },
          "endPosition": {
            "line": 247
          }
        },
        "contents": "async def main(args: argparse.Namespace) -&gt; int:\n\n\n    results: Dict[str, Any] = {\n        \"meta\": {\n            \"docs\": [str(p) for p in docs],\n            \"working_dir\": str(Path(cfg.working_dir).resolve()),\n            \"mode\": args.mode,\n            \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n            \"llm_model\": os.getenv(\"LLM_MODEL\", \"\"),\n            \"vlm_model\": os.getenv(\"VLM_MODEL\", \"\"),\n        },\n        \"queries\": [],\n    }\n\n    for q in queries:\n        # Capture retrieval prompt (for debugging image-path extraction)\n        prompt = await rag.lightrag.aquery(\n            q, param=QueryParam(mode=args.mode, only_need_prompt=True)\n        )\n        image_path_count = prompt.count(\"Image Path:\") if isinstance(prompt, str) else 0\n\n        item: Dict[str, Any] = {\n            \"query\": q,\n            \"retrieval_prompt\": prompt,\n            \"retrieval_prompt_image_path_count\": image_path_count,\n            \"runs\": {},\n        }\n\n        for flag in [False, True]:\n            t0 = time.perf_counter()\n            ans = await rag.aquery(q, mode=args.mode, vlm_enhanced=flag)\n            elapsed = time.perf_counter() - t0\n            item[\"runs\"][str(flag)] = {\"vlm_enhanced\": flag, \"answer\": ans, \"elapsed_s\": elapsed}\n\n        results[\"queries\"].append(item)\n\n    out_path = run_dir / \"results.json\"\n    out_path.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n\n    print(f\"Saved: {out_path}\")\n    # Quick sanity: show prompt image-path counts\n    summary = [\n        {\n            \"query\": q[\"query\"],\n            \"image_path_count\": q[\"retrieval_prompt_image_path_count\"],\n            \"elapsed_vlm_off_s\": q[\"runs\"][\"False\"][\"elapsed_s\"],\n            \"elapsed_vlm_on_s\": q[\"runs\"][\"True\"][\"elapsed_s\"],\n        }\n        for q in results[\"queries\"]\n    ]\n    print(json.dumps({\"summary\": summary}, ensure_ascii=False, indent=2))\n    return 0\n\n\ndef build_arg_parser() -&gt; argparse.ArgumentParser:\n    p = argparse.ArgumentParser(description=\"VLM enhanced query evaluation\")",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 141,
                "column": 1
              },
              "endPosition": {
                "line": 142,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "async def main(args: argparse.Namespace) -&gt; int:",
            "lineNumber": 141,
            "isSignature": true
          },
          {
            "lineNumber": 187
          },
          {
            "lineNumber": 188
          },
          {
            "text": "    results: Dict[str, Any] = {",
            "lineNumber": 189
          },
          {
            "text": "        \"meta\": {",
            "lineNumber": 190
          },
          {
            "text": "            \"docs\": [str(p) for p in docs],",
            "lineNumber": 191
          },
          {
            "text": "            \"working_dir\": str(Path(cfg.working_dir).resolve()),",
            "lineNumber": 192
          },
          {
            "text": "            \"mode\": args.mode,",
            "lineNumber": 193
          },
          {
            "text": "            \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),",
            "lineNumber": 194
          },
          {
            "text": "            \"llm_model\": os.getenv(\"LLM_MODEL\", \"\"),",
            "lineNumber": 195
          },
          {
            "text": "            \"vlm_model\": os.getenv(\"VLM_MODEL\", \"\"),",
            "lineNumber": 196
          },
          {
            "text": "        },",
            "lineNumber": 197
          },
          {
            "text": "        \"queries\": [],",
            "lineNumber": 198
          },
          {
            "text": "    }",
            "lineNumber": 199
          },
          {
            "lineNumber": 200
          },
          {
            "text": "    for q in queries:",
            "lineNumber": 201
          },
          {
            "text": "        # Capture retrieval prompt (for debugging image-path extraction)",
            "lineNumber": 202
          },
          {
            "text": "        prompt = await rag.lightrag.aquery(",
            "lineNumber": 203
          },
          {
            "text": "            q, param=QueryParam(mode=args.mode, only_need_prompt=True)",
            "lineNumber": 204
          },
          {
            "text": "        )",
            "lineNumber": 205
          },
          {
            "text": "        image_path_count = prompt.count(\"Image Path:\") if isinstance(prompt, str) else 0",
            "lineNumber": 206
          },
          {
            "lineNumber": 207
          },
          {
            "text": "        item: Dict[str, Any] = {",
            "lineNumber": 208
          },
          {
            "text": "            \"query\": q,",
            "lineNumber": 209
          },
          {
            "text": "            \"retrieval_prompt\": prompt,",
            "lineNumber": 210
          },
          {
            "text": "            \"retrieval_prompt_image_path_count\": image_path_count,",
            "lineNumber": 211
          },
          {
            "text": "            \"runs\": {},",
            "lineNumber": 212
          },
          {
            "text": "        }",
            "lineNumber": 213
          },
          {
            "lineNumber": 214
          },
          {
            "text": "        for flag in [False, True]:",
            "lineNumber": 215
          },
          {
            "text": "            t0 = time.perf_counter()",
            "lineNumber": 216
          },
          {
            "text": "            ans = await rag.aquery(q, mode=args.mode, vlm_enhanced=flag)",
            "lineNumber": 217
          },
          {
            "text": "            elapsed = time.perf_counter() - t0",
            "lineNumber": 218
          },
          {
            "text": "            item[\"runs\"][str(flag)] = {\"vlm_enhanced\": flag, \"answer\": ans, \"elapsed_s\": elapsed}",
            "lineNumber": 219
          },
          {
            "lineNumber": 220
          },
          {
            "text": "        results[\"queries\"].append(item)",
            "lineNumber": 221
          },
          {
            "lineNumber": 222
          },
          {
            "text": "    out_path = run_dir / \"results.json\"",
            "lineNumber": 223
          },
          {
            "text": "    out_path.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding=\"utf-8\")",
            "lineNumber": 224
          },
          {
            "lineNumber": 225
          },
          {
            "text": "    print(f\"Saved: {out_path}\")",
            "lineNumber": 226
          },
          {
            "text": "    # Quick sanity: show prompt image-path counts",
            "lineNumber": 227
          },
          {
            "text": "    summary = [",
            "lineNumber": 228
          },
          {
            "text": "        {",
            "lineNumber": 229
          },
          {
            "text": "            \"query\": q[\"query\"],",
            "lineNumber": 230
          },
          {
            "text": "            \"image_path_count\": q[\"retrieval_prompt_image_path_count\"],",
            "lineNumber": 231
          },
          {
            "text": "            \"elapsed_vlm_off_s\": q[\"runs\"][\"False\"][\"elapsed_s\"],",
            "lineNumber": 232
          },
          {
            "text": "            \"elapsed_vlm_on_s\": q[\"runs\"][\"True\"][\"elapsed_s\"],",
            "lineNumber": 233
          },
          {
            "text": "        }",
            "lineNumber": 234
          },
          {
            "text": "        for q in results[\"queries\"]",
            "lineNumber": 235
          },
          {
            "text": "    ]",
            "lineNumber": 236
          },
          {
            "text": "    print(json.dumps({\"summary\": summary}, ensure_ascii=False, indent=2))",
            "lineNumber": 237
          },
          {
            "text": "    return 0",
            "lineNumber": 238,
            "isSignature": true
          },
          {
            "lineNumber": 239
          },
          {
            "lineNumber": 240
          },
          {
            "text": "def build_arg_parser() -&gt; argparse.ArgumentParser:",
            "lineNumber": 241,
            "isSignature": true
          },
          {
            "text": "    p = argparse.ArgumentParser(description=\"VLM enhanced query evaluation\")",
            "lineNumber": 242
          }
        ]
      },
      "score": 0.4254150092601776
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 53,
            "column": 51
          }
        },
        "contents": "\"\"\"VLM enhanced query evaluation (on/off comparison).\n\nGoal:\n- Verify that when retrieved context contains &#96;Image Path: ...&#96;, the VLM-enhanced path:\n  1) extracts image paths\n  2) encodes images as base64\n  3) calls vision_model_func with multimodal messages\n\nThis script runs the same queries twice:\n- vlm_enhanced=False (text-only)\n- vlm_enhanced=True  (multimodal, requires vision_model_func)\n\nUsage:\n  python eval/integration/vlm_enhanced_query_eval.py \\\n    --docs eval/docs/全球資訊服務暨軟體市場規模.png eval/docs/cat-01.jpg \\\n    --working-dir eval/integration/_rag_storage_vlm_eval \\\n    --out-dir eval/integration/_results\n\nEnv:\n- LLM_BINDING_API_KEY, LLM_BINDING_HOST\n- LLM_MODEL (text model, default gpt-4o-mini)\n- VLM_MODEL (vision model, default gpt-4o)\n- EMBEDDING_MODEL, EMBEDDING_DIM\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport asyncio\nimport json\nimport os\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nfrom lightrag import QueryParam\nfrom lightrag.llm.openai import openai_complete_if_cache, openai_embed\nfrom lightrag.utils import EmbeddingFunc\n\nfrom raganything import RAGAnything, RAGAnythingConfig\n\n\ndef _try_load_dotenv() -&gt; None:\n    try:\n        from dotenv import load_dotenv\n\n        load_dotenv(dotenv_path=\".env\", override=False)\n    except Exception:\n        return\n\n\ndef _now_tag() -&gt; str:\n    return datetime.now().strftime(\"%Y%m%d-%H%M%S\")",
        "signatures": {},
        "detailedLines": [
          {
            "text": "\"\"\"VLM enhanced query evaluation (on/off comparison).",
            "lineNumber": 1
          },
          {
            "lineNumber": 2
          },
          {
            "text": "Goal:",
            "lineNumber": 3
          },
          {
            "text": "- Verify that when retrieved context contains &#96;Image Path: ...&#96;, the VLM-enhanced path:",
            "lineNumber": 4
          },
          {
            "text": "  1) extracts image paths",
            "lineNumber": 5
          },
          {
            "text": "  2) encodes images as base64",
            "lineNumber": 6
          },
          {
            "text": "  3) calls vision_model_func with multimodal messages",
            "lineNumber": 7
          },
          {
            "lineNumber": 8
          },
          {
            "text": "This script runs the same queries twice:",
            "lineNumber": 9
          },
          {
            "text": "- vlm_enhanced=False (text-only)",
            "lineNumber": 10
          },
          {
            "text": "- vlm_enhanced=True  (multimodal, requires vision_model_func)",
            "lineNumber": 11
          },
          {
            "lineNumber": 12
          },
          {
            "text": "Usage:",
            "lineNumber": 13
          },
          {
            "text": "  python eval/integration/vlm_enhanced_query_eval.py \\",
            "lineNumber": 14
          },
          {
            "text": "    --docs eval/docs/全球資訊服務暨軟體市場規模.png eval/docs/cat-01.jpg \\",
            "lineNumber": 15
          },
          {
            "text": "    --working-dir eval/integration/_rag_storage_vlm_eval \\",
            "lineNumber": 16
          },
          {
            "text": "    --out-dir eval/integration/_results",
            "lineNumber": 17
          },
          {
            "lineNumber": 18
          },
          {
            "text": "Env:",
            "lineNumber": 19
          },
          {
            "text": "- LLM_BINDING_API_KEY, LLM_BINDING_HOST",
            "lineNumber": 20
          },
          {
            "text": "- LLM_MODEL (text model, default gpt-4o-mini)",
            "lineNumber": 21
          },
          {
            "text": "- VLM_MODEL (vision model, default gpt-4o)",
            "lineNumber": 22
          },
          {
            "text": "- EMBEDDING_MODEL, EMBEDDING_DIM",
            "lineNumber": 23
          },
          {
            "text": "\"\"\"",
            "lineNumber": 24
          },
          {
            "lineNumber": 25
          },
          {
            "text": "from __future__ import annotations",
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "import argparse",
            "lineNumber": 28
          },
          {
            "text": "import asyncio",
            "lineNumber": 29
          },
          {
            "text": "import json",
            "lineNumber": 30
          },
          {
            "text": "import os",
            "lineNumber": 31
          },
          {
            "text": "import time",
            "lineNumber": 32
          },
          {
            "text": "from datetime import datetime",
            "lineNumber": 33
          },
          {
            "text": "from pathlib import Path",
            "lineNumber": 34
          },
          {
            "text": "from typing import Any, Dict, List",
            "lineNumber": 35
          },
          {
            "lineNumber": 36
          },
          {
            "text": "from lightrag import QueryParam",
            "lineNumber": 37
          },
          {
            "text": "from lightrag.llm.openai import openai_complete_if_cache, openai_embed",
            "lineNumber": 38
          },
          {
            "text": "from lightrag.utils import EmbeddingFunc",
            "lineNumber": 39
          },
          {
            "lineNumber": 40
          },
          {
            "text": "from raganything import RAGAnything, RAGAnythingConfig",
            "lineNumber": 41
          },
          {
            "lineNumber": 42
          },
          {
            "lineNumber": 43
          },
          {
            "text": "def _try_load_dotenv() -&gt; None:",
            "lineNumber": 44,
            "isSignature": true
          },
          {
            "text": "    try:",
            "lineNumber": 45
          },
          {
            "text": "        from dotenv import load_dotenv",
            "lineNumber": 46
          },
          {
            "lineNumber": 47
          },
          {
            "text": "        load_dotenv(dotenv_path=\".env\", override=False)",
            "lineNumber": 48
          },
          {
            "text": "    except Exception:",
            "lineNumber": 49
          },
          {
            "text": "        return",
            "lineNumber": 50,
            "isSignature": true
          },
          {
            "lineNumber": 51
          },
          {
            "lineNumber": 52
          },
          {
            "text": "def _now_tag() -&gt; str:",
            "lineNumber": 53,
            "isSignature": true
          },
          {
            "text": "    return datetime.now().strftime(\"%Y%m%d-%H%M%S\")",
            "lineNumber": 54,
            "isSignature": true
          }
        ]
      },
      "score": 0.4173390865325928
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\ndef _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:\n\n        ]\n\n    async def _call_vlm_with_multimodal_content(self, messages: List[Dict]) -&gt; str:\n        \"\"\"\n        Call VLM to process multimodal content\n\n        Args:\n            messages: VLM message format\n\n        Returns:\n            str: VLM response result\n        \"\"\"\n        try:\n            user_message = messages[1]\n            content = user_message[\"content\"]\n            system_prompt = messages[0][\"content\"]\n\n            if isinstance(content, str):\n                # Pure text mode\n                result = await self.vision_model_func(content, system_prompt=system_prompt)\n            else:\n                # Multimodal mode - pass complete messages directly to VLM\n                result = await self.vision_model_func(\n                    \"\",  # Empty prompt since we're using messages format\n                    messages=messages,\n                )\n\n            return result\n\n        except Exception as e:\n            self.logger.error(f\"VLM call failed: {e}\")\n            raise\n\n    # Synchronous versions of query methods\n    def query(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:\n        \"\"\"\n        Synchronous version of pure text query\n\n        Args:\n            query: Query text\n            mode: Query mode (\"local\", \"global\", \"hybrid\", \"naive\", \"mix\", \"bypass\")\n            **kwargs: Other query parameters, will be passed to QueryParam\n                - vlm_enhanced: bool, default True when vision_model_func is available.\n                  If True, will parse image paths in retrieved context and replace them\n                  with base64 encoded images for VLM processing.\n\n        Returns:\n            str: Query result\n        \"\"\"\n        loop = always_get_an_event_loop()\n        return loop.run_until_complete(self.aquery(query, mode=mode, **kwargs))\n\n    def query_with_multimodal(\n        self,\n        query: str,\n        multimodal_content: List[Dict[str, Any]] = None,\n        mode: str = \"mix\",\n        **kwargs,\n    ) -&gt; str:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:",
            "lineNumber": 559
          },
          {
            "lineNumber": 627
          },
          {
            "text": "        ]",
            "lineNumber": 628
          },
          {
            "lineNumber": 629
          },
          {
            "text": "    async def _call_vlm_with_multimodal_content(self, messages: List[Dict]) -&gt; str:",
            "lineNumber": 630
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 631
          },
          {
            "text": "        Call VLM to process multimodal content",
            "lineNumber": 632
          },
          {
            "lineNumber": 633
          },
          {
            "text": "        Args:",
            "lineNumber": 634
          },
          {
            "text": "            messages: VLM message format",
            "lineNumber": 635
          },
          {
            "lineNumber": 636
          },
          {
            "text": "        Returns:",
            "lineNumber": 637
          },
          {
            "text": "            str: VLM response result",
            "lineNumber": 638
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 639
          },
          {
            "text": "        try:",
            "lineNumber": 640
          },
          {
            "text": "            user_message = messages[1]",
            "lineNumber": 641
          },
          {
            "text": "            content = user_message[\"content\"]",
            "lineNumber": 642
          },
          {
            "text": "            system_prompt = messages[0][\"content\"]",
            "lineNumber": 643
          },
          {
            "lineNumber": 644
          },
          {
            "text": "            if isinstance(content, str):",
            "lineNumber": 645
          },
          {
            "text": "                # Pure text mode",
            "lineNumber": 646
          },
          {
            "text": "                result = await self.vision_model_func(content, system_prompt=system_prompt)",
            "lineNumber": 647
          },
          {
            "text": "            else:",
            "lineNumber": 648
          },
          {
            "text": "                # Multimodal mode - pass complete messages directly to VLM",
            "lineNumber": 649
          },
          {
            "text": "                result = await self.vision_model_func(",
            "lineNumber": 650
          },
          {
            "text": "                    \"\",  # Empty prompt since we're using messages format",
            "lineNumber": 651
          },
          {
            "text": "                    messages=messages,",
            "lineNumber": 652
          },
          {
            "text": "                )",
            "lineNumber": 653
          },
          {
            "lineNumber": 654
          },
          {
            "text": "            return result",
            "lineNumber": 655
          },
          {
            "lineNumber": 656
          },
          {
            "text": "        except Exception as e:",
            "lineNumber": 657
          },
          {
            "text": "            self.logger.error(f\"VLM call failed: {e}\")",
            "lineNumber": 658
          },
          {
            "text": "            raise",
            "lineNumber": 659
          },
          {
            "lineNumber": 660
          },
          {
            "text": "    # Synchronous versions of query methods",
            "lineNumber": 661
          },
          {
            "text": "    def query(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:",
            "lineNumber": 662
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 663
          },
          {
            "text": "        Synchronous version of pure text query",
            "lineNumber": 664
          },
          {
            "lineNumber": 665
          },
          {
            "text": "        Args:",
            "lineNumber": 666
          },
          {
            "text": "            query: Query text",
            "lineNumber": 667
          },
          {
            "text": "            mode: Query mode (\"local\", \"global\", \"hybrid\", \"naive\", \"mix\", \"bypass\")",
            "lineNumber": 668
          },
          {
            "text": "            **kwargs: Other query parameters, will be passed to QueryParam",
            "lineNumber": 669
          },
          {
            "text": "                - vlm_enhanced: bool, default True when vision_model_func is available.",
            "lineNumber": 670
          },
          {
            "text": "                  If True, will parse image paths in retrieved context and replace them",
            "lineNumber": 671
          },
          {
            "text": "                  with base64 encoded images for VLM processing.",
            "lineNumber": 672
          },
          {
            "lineNumber": 673
          },
          {
            "text": "        Returns:",
            "lineNumber": 674
          },
          {
            "text": "            str: Query result",
            "lineNumber": 675
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 676
          },
          {
            "text": "        loop = always_get_an_event_loop()",
            "lineNumber": 677
          },
          {
            "text": "        return loop.run_until_complete(self.aquery(query, mode=mode, **kwargs))",
            "lineNumber": 678
          },
          {
            "lineNumber": 679
          },
          {
            "text": "    def query_with_multimodal(",
            "lineNumber": 680
          },
          {
            "text": "        self,",
            "lineNumber": 681
          },
          {
            "text": "        query: str,",
            "lineNumber": 682
          },
          {
            "text": "        multimodal_content: List[Dict[str, Any]] = None,",
            "lineNumber": 683
          },
          {
            "text": "        mode: str = \"mix\",",
            "lineNumber": 684
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 685
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 686
          },
          {
            "text": "        ",
            "lineNumber": 687
          }
        ]
      },
      "score": 0.41695618629455566
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {
            "line": 56
          },
          "endPosition": {
            "line": 237
          }
        },
        "contents": "def _build_llm_vision_embedding():\ndef vision_model_func(\n        prompt,\n        system_prompt=None,\n        history_messages=None,\n        image_data=None,\n        messages=None,\n        **kwargs,\n    ):\n        # VLM enhanced query will pass messages (multimodal format) when images are found.\n\n                **kwargs,\n            )\n        if image_data:\n            # Single image format (used by image processor)\n            return openai_complete_if_cache(\n                vlm_model,\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": prompt},\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                            },\n                        ],\n                    },\n                ],\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        return llm_model_func(prompt, system_prompt, history_messages or [], **kwargs)\n\n    emb_model = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n    emb_dim = int(os.getenv(\"EMBEDDING_DIM\", \"1536\"))\n\n    embedding_func = EmbeddingFunc(\n        embedding_dim=emb_dim,\n        max_token_size=8192,\n        func=lambda texts: openai_embed(\n            texts,\n            model=emb_model,\n            api_key=api_key,\n            base_url=base_url,\n        ),\n    )\n\n    return llm_model_func, vision_model_func, embedding_func\n\n\nasync def main(args: argparse.Namespace) -&gt; int:\n    _try_load_dotenv()",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 57,
                "column": 1
              },
              "endPosition": {
                "line": 58,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 78,
                "column": 5
              },
              "endPosition": {
                "line": 87,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def _build_llm_vision_embedding():",
            "lineNumber": 57,
            "isSignature": true
          },
          {
            "text": "def vision_model_func(",
            "lineNumber": 78
          },
          {
            "text": "        prompt,",
            "lineNumber": 79
          },
          {
            "text": "        system_prompt=None,",
            "lineNumber": 80
          },
          {
            "text": "        history_messages=None,",
            "lineNumber": 81
          },
          {
            "text": "        image_data=None,",
            "lineNumber": 82
          },
          {
            "text": "        messages=None,",
            "lineNumber": 83
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 84
          },
          {
            "text": "    ):",
            "lineNumber": 85
          },
          {
            "text": "        # VLM enhanced query will pass messages (multimodal format) when images are found.",
            "lineNumber": 86
          },
          {
            "lineNumber": 95
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 96
          },
          {
            "text": "            )",
            "lineNumber": 97
          },
          {
            "text": "        if image_data:",
            "lineNumber": 98
          },
          {
            "text": "            # Single image format (used by image processor)",
            "lineNumber": 99
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 100
          },
          {
            "text": "                vlm_model,",
            "lineNumber": 101
          },
          {
            "text": "                \"\",",
            "lineNumber": 102
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 103
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 104
          },
          {
            "text": "                messages=[",
            "lineNumber": 105
          },
          {
            "text": "                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,",
            "lineNumber": 106
          },
          {
            "text": "                    {",
            "lineNumber": 107
          },
          {
            "text": "                        \"role\": \"user\",",
            "lineNumber": 108
          },
          {
            "text": "                        \"content\": [",
            "lineNumber": 109
          },
          {
            "text": "                            {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 110
          },
          {
            "text": "                            {",
            "lineNumber": 111
          },
          {
            "text": "                                \"type\": \"image_url\",",
            "lineNumber": 112
          },
          {
            "text": "                                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},",
            "lineNumber": 113
          },
          {
            "text": "                            },",
            "lineNumber": 114
          },
          {
            "text": "                        ],",
            "lineNumber": 115
          },
          {
            "text": "                    },",
            "lineNumber": 116
          },
          {
            "text": "                ],",
            "lineNumber": 117
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 118
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 119
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 120
          },
          {
            "text": "            )",
            "lineNumber": 121
          },
          {
            "text": "        return llm_model_func(prompt, system_prompt, history_messages or [], **kwargs)",
            "lineNumber": 122
          },
          {
            "lineNumber": 123
          },
          {
            "text": "    emb_model = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")",
            "lineNumber": 124
          },
          {
            "text": "    emb_dim = int(os.getenv(\"EMBEDDING_DIM\", \"1536\"))",
            "lineNumber": 125
          },
          {
            "lineNumber": 126
          },
          {
            "text": "    embedding_func = EmbeddingFunc(",
            "lineNumber": 127
          },
          {
            "text": "        embedding_dim=emb_dim,",
            "lineNumber": 128
          },
          {
            "text": "        max_token_size=8192,",
            "lineNumber": 129
          },
          {
            "text": "        func=lambda texts: openai_embed(",
            "lineNumber": 130
          },
          {
            "text": "            texts,",
            "lineNumber": 131
          },
          {
            "text": "            model=emb_model,",
            "lineNumber": 132
          },
          {
            "text": "            api_key=api_key,",
            "lineNumber": 133
          },
          {
            "text": "            base_url=base_url,",
            "lineNumber": 134
          },
          {
            "text": "        ),",
            "lineNumber": 135
          },
          {
            "text": "    )",
            "lineNumber": 136
          },
          {
            "lineNumber": 137
          },
          {
            "text": "    return llm_model_func, vision_model_func, embedding_func",
            "lineNumber": 138,
            "isSignature": true
          },
          {
            "lineNumber": 139
          },
          {
            "lineNumber": 140
          },
          {
            "text": "async def main(args: argparse.Namespace) -&gt; int:",
            "lineNumber": 141,
            "isSignature": true
          },
          {
            "text": "    _try_load_dotenv()",
            "lineNumber": 142
          }
        ]
      },
      "score": 0.408290296792984
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\ndef _generate_multimodal_cache_key(\n        self, query: str, multimodal_content: List[Dict[str, Any]], mode: str, **kwargs\n    ) -&gt; str:\n\n\n        return f\"multimodal_query:{cache_hash}\"\n\n    async def aquery(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:\n        \"\"\"\n        Pure text query - directly calls LightRAG's query functionality\n\n        Args:\n            query: Query text\n            mode: Query mode (\"local\", \"global\", \"hybrid\", \"naive\", \"mix\", \"bypass\")\n            **kwargs: Other query parameters, will be passed to QueryParam\n                - vlm_enhanced: bool, default True when vision_model_func is available.\n                  If True, will parse image paths in retrieved context and replace them\n                  with base64 encoded images for VLM processing.\n\n        Returns:\n            str: Query result\n        \"\"\"\n        if self.lightrag is None:\n            raise ValueError(\n                \"No LightRAG instance available. Please process documents first or provide a pre-initialized LightRAG instance.\"\n            )\n\n        # Check if VLM enhanced query should be used\n        vlm_enhanced = kwargs.pop(\"vlm_enhanced\", None)\n\n        # Auto-determine VLM enhanced based on availability\n        if vlm_enhanced is None:\n            vlm_enhanced = hasattr(self, \"vision_model_func\") and self.vision_model_func is not None\n\n        # Use VLM enhanced query if enabled and available\n        if vlm_enhanced and hasattr(self, \"vision_model_func\") and self.vision_model_func:\n            return await self.aquery_vlm_enhanced(query, mode=mode, **kwargs)\n        elif vlm_enhanced and (\n            not hasattr(self, \"vision_model_func\") or not self.vision_model_func\n        ):\n            self.logger.warning(\n                \"VLM enhanced query requested but vision_model_func is not available, falling back to normal query\"\n            )\n\n        # Create query parameters\n        query_param = QueryParam(mode=mode, **kwargs)\n\n        self.logger.info(f\"Executing text query: {query[:100]}...\")\n        self.logger.info(f\"Query mode: {mode}\")\n\n        # Call LightRAG's query method\n        result = await self.lightrag.aquery(query, param=query_param)",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 25,
                "column": 5
              },
              "endPosition": {
                "line": 28,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "def _generate_multimodal_cache_key(",
            "lineNumber": 25
          },
          {
            "text": "        self, query: str, multimodal_content: List[Dict[str, Any]], mode: str, **kwargs",
            "lineNumber": 26
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 27
          },
          {
            "lineNumber": 94
          },
          {
            "lineNumber": 95
          },
          {
            "text": "        return f\"multimodal_query:{cache_hash}\"",
            "lineNumber": 96
          },
          {
            "lineNumber": 97
          },
          {
            "text": "    async def aquery(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:",
            "lineNumber": 98
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 99
          },
          {
            "text": "        Pure text query - directly calls LightRAG's query functionality",
            "lineNumber": 100
          },
          {
            "lineNumber": 101
          },
          {
            "text": "        Args:",
            "lineNumber": 102
          },
          {
            "text": "            query: Query text",
            "lineNumber": 103
          },
          {
            "text": "            mode: Query mode (\"local\", \"global\", \"hybrid\", \"naive\", \"mix\", \"bypass\")",
            "lineNumber": 104
          },
          {
            "text": "            **kwargs: Other query parameters, will be passed to QueryParam",
            "lineNumber": 105
          },
          {
            "text": "                - vlm_enhanced: bool, default True when vision_model_func is available.",
            "lineNumber": 106
          },
          {
            "text": "                  If True, will parse image paths in retrieved context and replace them",
            "lineNumber": 107
          },
          {
            "text": "                  with base64 encoded images for VLM processing.",
            "lineNumber": 108
          },
          {
            "lineNumber": 109
          },
          {
            "text": "        Returns:",
            "lineNumber": 110
          },
          {
            "text": "            str: Query result",
            "lineNumber": 111
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 112
          },
          {
            "text": "        if self.lightrag is None:",
            "lineNumber": 113
          },
          {
            "text": "            raise ValueError(",
            "lineNumber": 114
          },
          {
            "text": "                \"No LightRAG instance available. Please process documents first or provide a pre-initialized LightRAG instance.\"",
            "lineNumber": 115
          },
          {
            "text": "            )",
            "lineNumber": 116
          },
          {
            "lineNumber": 117
          },
          {
            "text": "        # Check if VLM enhanced query should be used",
            "lineNumber": 118
          },
          {
            "text": "        vlm_enhanced = kwargs.pop(\"vlm_enhanced\", None)",
            "lineNumber": 119
          },
          {
            "lineNumber": 120
          },
          {
            "text": "        # Auto-determine VLM enhanced based on availability",
            "lineNumber": 121
          },
          {
            "text": "        if vlm_enhanced is None:",
            "lineNumber": 122
          },
          {
            "text": "            vlm_enhanced = hasattr(self, \"vision_model_func\") and self.vision_model_func is not None",
            "lineNumber": 123
          },
          {
            "lineNumber": 124
          },
          {
            "text": "        # Use VLM enhanced query if enabled and available",
            "lineNumber": 125
          },
          {
            "text": "        if vlm_enhanced and hasattr(self, \"vision_model_func\") and self.vision_model_func:",
            "lineNumber": 126
          },
          {
            "text": "            return await self.aquery_vlm_enhanced(query, mode=mode, **kwargs)",
            "lineNumber": 127
          },
          {
            "text": "        elif vlm_enhanced and (",
            "lineNumber": 128
          },
          {
            "text": "            not hasattr(self, \"vision_model_func\") or not self.vision_model_func",
            "lineNumber": 129
          },
          {
            "text": "        ):",
            "lineNumber": 130
          },
          {
            "text": "            self.logger.warning(",
            "lineNumber": 131
          },
          {
            "text": "                \"VLM enhanced query requested but vision_model_func is not available, falling back to normal query\"",
            "lineNumber": 132
          },
          {
            "text": "            )",
            "lineNumber": 133
          },
          {
            "lineNumber": 134
          },
          {
            "text": "        # Create query parameters",
            "lineNumber": 135
          },
          {
            "text": "        query_param = QueryParam(mode=mode, **kwargs)",
            "lineNumber": 136
          },
          {
            "lineNumber": 137
          },
          {
            "text": "        self.logger.info(f\"Executing text query: {query[:100]}...\")",
            "lineNumber": 138
          },
          {
            "text": "        self.logger.info(f\"Query mode: {mode}\")",
            "lineNumber": 139
          },
          {
            "lineNumber": 140
          },
          {
            "text": "        # Call LightRAG's query method",
            "lineNumber": 141
          },
          {
            "text": "        result = await self.lightrag.aquery(query, param=query_param)",
            "lineNumber": 142
          }
        ]
      },
      "score": 0.4032084345817566
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def aquery_vlm_enhanced(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:\n\n\n        if not images_found:\n            self.logger.info(\"No valid images found, falling back to normal query\")\n            # Fallback to normal query\n            query_param = QueryParam(mode=mode, **kwargs)\n            return await self.lightrag.aquery(query, param=query_param)\n\n        self.logger.info(f\"Processed {images_found} images for VLM\")\n\n        # 3. Build VLM message format\n        messages = self._build_vlm_messages_with_images(enhanced_prompt, query)\n\n        # 4. Call VLM for question answering\n        result = await self._call_vlm_with_multimodal_content(messages)\n\n        self.logger.info(\"VLM enhanced query completed\")\n        return result\n\n    async def _process_multimodal_query_content(\n        self, base_query: str, multimodal_content: List[Dict[str, Any]]\n    ) -&gt; str:\n        \"\"\"\n        Process multimodal query content to generate enhanced query text\n\n        Args:\n            base_query: Base query text\n            multimodal_content: List of multimodal content\n\n        Returns:\n            str: Enhanced query text\n        \"\"\"\n        self.logger.info(\"Starting multimodal query content processing...\")\n\n        enhanced_parts = [f\"User query: {base_query}\"]\n\n        for i, content in enumerate(multimodal_content):",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 269,
                "column": 5
              },
              "endPosition": {
                "line": 270,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def aquery_vlm_enhanced(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:",
            "lineNumber": 269
          },
          {
            "lineNumber": 310
          },
          {
            "lineNumber": 311
          },
          {
            "text": "        if not images_found:",
            "lineNumber": 312
          },
          {
            "text": "            self.logger.info(\"No valid images found, falling back to normal query\")",
            "lineNumber": 313
          },
          {
            "text": "            # Fallback to normal query",
            "lineNumber": 314
          },
          {
            "text": "            query_param = QueryParam(mode=mode, **kwargs)",
            "lineNumber": 315
          },
          {
            "text": "            return await self.lightrag.aquery(query, param=query_param)",
            "lineNumber": 316
          },
          {
            "lineNumber": 317
          },
          {
            "text": "        self.logger.info(f\"Processed {images_found} images for VLM\")",
            "lineNumber": 318
          },
          {
            "lineNumber": 319
          },
          {
            "text": "        # 3. Build VLM message format",
            "lineNumber": 320
          },
          {
            "text": "        messages = self._build_vlm_messages_with_images(enhanced_prompt, query)",
            "lineNumber": 321
          },
          {
            "lineNumber": 322
          },
          {
            "text": "        # 4. Call VLM for question answering",
            "lineNumber": 323
          },
          {
            "text": "        result = await self._call_vlm_with_multimodal_content(messages)",
            "lineNumber": 324
          },
          {
            "lineNumber": 325
          },
          {
            "text": "        self.logger.info(\"VLM enhanced query completed\")",
            "lineNumber": 326
          },
          {
            "text": "        return result",
            "lineNumber": 327
          },
          {
            "lineNumber": 328
          },
          {
            "text": "    async def _process_multimodal_query_content(",
            "lineNumber": 329
          },
          {
            "text": "        self, base_query: str, multimodal_content: List[Dict[str, Any]]",
            "lineNumber": 330
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 331
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 332
          },
          {
            "text": "        Process multimodal query content to generate enhanced query text",
            "lineNumber": 333
          },
          {
            "lineNumber": 334
          },
          {
            "text": "        Args:",
            "lineNumber": 335
          },
          {
            "text": "            base_query: Base query text",
            "lineNumber": 336
          },
          {
            "text": "            multimodal_content: List of multimodal content",
            "lineNumber": 337
          },
          {
            "lineNumber": 338
          },
          {
            "text": "        Returns:",
            "lineNumber": 339
          },
          {
            "text": "            str: Enhanced query text",
            "lineNumber": 340
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 341
          },
          {
            "text": "        self.logger.info(\"Starting multimodal query content processing...\")",
            "lineNumber": 342
          },
          {
            "lineNumber": 343
          },
          {
            "text": "        enhanced_parts = [f\"User query: {base_query}\"]",
            "lineNumber": 344
          },
          {
            "lineNumber": 345
          },
          {
            "text": "        for i, content in enumerate(multimodal_content):",
            "lineNumber": 346
          }
        ]
      },
      "score": 0.40294650197029114
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README.md",
        "range": {
          "startPosition": {
            "line": 703
          },
          "endPosition": {
            "line": 756
          }
        },
        "contents": "    await initialize_pipeline_status()\n\n    # Define vision model function for image processing\n    def vision_model_func(\n        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs\n    ):\n        # If messages format is provided (for multimodal VLM enhanced query), use it directly\n        if messages:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=messages,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # Traditional single image format\n        elif image_data:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt}\n                    if system_prompt\n                    else None,\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": prompt},\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{image_data}\"\n                                },\n                            },\n                        ],\n                    }\n                    if image_data\n                    else {\"role\": \"user\", \"content\": prompt},\n                ],\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # Pure text format\n        else:\n            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)\n\n    # Now use existing LightRAG instance to initialize RAGAnything",
        "signatures": {},
        "detailedLines": [
          {
            "text": "    await initialize_pipeline_status()",
            "lineNumber": 704
          },
          {
            "lineNumber": 705
          },
          {
            "text": "    # Define vision model function for image processing",
            "lineNumber": 706
          },
          {
            "text": "    def vision_model_func(",
            "lineNumber": 707
          },
          {
            "text": "        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs",
            "lineNumber": 708
          },
          {
            "text": "    ):",
            "lineNumber": 709
          },
          {
            "text": "        # If messages format is provided (for multimodal VLM enhanced query), use it directly",
            "lineNumber": 710
          },
          {
            "text": "        if messages:",
            "lineNumber": 711
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 712
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 713
          },
          {
            "text": "                \"\",",
            "lineNumber": 714
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 715
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 716
          },
          {
            "text": "                messages=messages,",
            "lineNumber": 717
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 718
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 719
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 720
          },
          {
            "text": "            )",
            "lineNumber": 721
          },
          {
            "text": "        # Traditional single image format",
            "lineNumber": 722
          },
          {
            "text": "        elif image_data:",
            "lineNumber": 723
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 724
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 725
          },
          {
            "text": "                \"\",",
            "lineNumber": 726
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 727
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 728
          },
          {
            "text": "                messages=[",
            "lineNumber": 729
          },
          {
            "text": "                    {\"role\": \"system\", \"content\": system_prompt}",
            "lineNumber": 730
          },
          {
            "text": "                    if system_prompt",
            "lineNumber": 731
          },
          {
            "text": "                    else None,",
            "lineNumber": 732
          },
          {
            "text": "                    {",
            "lineNumber": 733
          },
          {
            "text": "                        \"role\": \"user\",",
            "lineNumber": 734
          },
          {
            "text": "                        \"content\": [",
            "lineNumber": 735
          },
          {
            "text": "                            {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 736
          },
          {
            "text": "                            {",
            "lineNumber": 737
          },
          {
            "text": "                                \"type\": \"image_url\",",
            "lineNumber": 738
          },
          {
            "text": "                                \"image_url\": {",
            "lineNumber": 739
          },
          {
            "text": "                                    \"url\": f\"data:image/jpeg;base64,{image_data}\"",
            "lineNumber": 740
          },
          {
            "text": "                                },",
            "lineNumber": 741
          },
          {
            "text": "                            },",
            "lineNumber": 742
          },
          {
            "text": "                        ],",
            "lineNumber": 743
          },
          {
            "text": "                    }",
            "lineNumber": 744
          },
          {
            "text": "                    if image_data",
            "lineNumber": 745
          },
          {
            "text": "                    else {\"role\": \"user\", \"content\": prompt},",
            "lineNumber": 746
          },
          {
            "text": "                ],",
            "lineNumber": 747
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 748
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 749
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 750
          },
          {
            "text": "            )",
            "lineNumber": 751
          },
          {
            "text": "        # Pure text format",
            "lineNumber": 752
          },
          {
            "text": "        else:",
            "lineNumber": 753
          },
          {
            "text": "            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)",
            "lineNumber": 754
          },
          {
            "lineNumber": 755
          },
          {
            "text": "    # Now use existing LightRAG instance to initialize RAGAnything",
            "lineNumber": 756
          }
        ]
      },
      "score": 0.3652941882610321
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\modalprocessors.py",
        "range": {
          "startPosition": {
            "line": 749
          },
          "endPosition": {
            "line": 1032
          }
        },
        "contents": "class ImageModalProcessor(BaseModalProcessor):\nasync def generate_description_only(\n        self,\n        modal_content,\n        content_type: str,\n        item_info: Dict[str, Any] = None,\n        entity_name: str = None,\n    ) -&gt; Tuple[str, Dict[str, Any]]:\n\n\n            # Build detailed visual analysis prompt with context\n            if context:\n                vision_prompt = PROMPTS.get(\n                    \"vision_prompt_with_context\", PROMPTS[\"vision_prompt\"]\n                ).format(\n                    context=context,\n                    entity_name=entity_name\n                    if entity_name\n                    else \"unique descriptive name for this image\",\n                    image_path=image_path,\n                    captions=captions if captions else \"None\",\n                    footnotes=footnotes if footnotes else \"None\",\n                )\n            else:\n                vision_prompt = PROMPTS[\"vision_prompt\"].format(\n                    entity_name=entity_name\n                    if entity_name\n                    else \"unique descriptive name for this image\",\n                    image_path=image_path,\n                    captions=captions if captions else \"None\",\n                    footnotes=footnotes if footnotes else \"None\",\n                )\n\n            # Encode image to base64\n            image_base64 = self._encode_image_to_base64(image_path)\n            if not image_base64:\n                raise RuntimeError(f\"Failed to encode image to base64: {image_path}\")\n\n            # Call vision model with encoded image (safely handle image_data parameter)\n            response = await self._call_vision_model_safe(\n                vision_prompt,\n                image_data=image_base64,\n                system_prompt=PROMPTS[\"IMAGE_ANALYSIS_SYSTEM\"],\n            )\n\n            # Parse response (reuse existing logic)\n            enhanced_caption, entity_info = self._parse_response(response, entity_name)\n\n            return enhanced_caption, entity_info\n\n        except Exception as e:\n            logger.error",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 750,
                "column": 1
              },
              "endPosition": {
                "line": 751,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 842,
                "column": 5
              },
              "endPosition": {
                "line": 849,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class ImageModalProcessor(BaseModalProcessor):",
            "lineNumber": 750,
            "isSignature": true
          },
          {
            "text": "async def generate_description_only(",
            "lineNumber": 842
          },
          {
            "text": "        self,",
            "lineNumber": 843
          },
          {
            "text": "        modal_content,",
            "lineNumber": 844
          },
          {
            "text": "        content_type: str,",
            "lineNumber": 845
          },
          {
            "text": "        item_info: Dict[str, Any] = None,",
            "lineNumber": 846
          },
          {
            "text": "        entity_name: str = None,",
            "lineNumber": 847
          },
          {
            "text": "    ) -&gt; Tuple[str, Dict[str, Any]]:",
            "lineNumber": 848
          },
          {
            "lineNumber": 888
          },
          {
            "lineNumber": 889
          },
          {
            "text": "            # Build detailed visual analysis prompt with context",
            "lineNumber": 890
          },
          {
            "text": "            if context:",
            "lineNumber": 891
          },
          {
            "text": "                vision_prompt = PROMPTS.get(",
            "lineNumber": 892
          },
          {
            "text": "                    \"vision_prompt_with_context\", PROMPTS[\"vision_prompt\"]",
            "lineNumber": 893
          },
          {
            "text": "                ).format(",
            "lineNumber": 894
          },
          {
            "text": "                    context=context,",
            "lineNumber": 895
          },
          {
            "text": "                    entity_name=entity_name",
            "lineNumber": 896
          },
          {
            "text": "                    if entity_name",
            "lineNumber": 897
          },
          {
            "text": "                    else \"unique descriptive name for this image\",",
            "lineNumber": 898
          },
          {
            "text": "                    image_path=image_path,",
            "lineNumber": 899
          },
          {
            "text": "                    captions=captions if captions else \"None\",",
            "lineNumber": 900
          },
          {
            "text": "                    footnotes=footnotes if footnotes else \"None\",",
            "lineNumber": 901
          },
          {
            "text": "                )",
            "lineNumber": 902
          },
          {
            "text": "            else:",
            "lineNumber": 903
          },
          {
            "text": "                vision_prompt = PROMPTS[\"vision_prompt\"].format(",
            "lineNumber": 904
          },
          {
            "text": "                    entity_name=entity_name",
            "lineNumber": 905
          },
          {
            "text": "                    if entity_name",
            "lineNumber": 906
          },
          {
            "text": "                    else \"unique descriptive name for this image\",",
            "lineNumber": 907
          },
          {
            "text": "                    image_path=image_path,",
            "lineNumber": 908
          },
          {
            "text": "                    captions=captions if captions else \"None\",",
            "lineNumber": 909
          },
          {
            "text": "                    footnotes=footnotes if footnotes else \"None\",",
            "lineNumber": 910
          },
          {
            "text": "                )",
            "lineNumber": 911
          },
          {
            "lineNumber": 912
          },
          {
            "text": "            # Encode image to base64",
            "lineNumber": 913
          },
          {
            "text": "            image_base64 = self._encode_image_to_base64(image_path)",
            "lineNumber": 914
          },
          {
            "text": "            if not image_base64:",
            "lineNumber": 915
          },
          {
            "text": "                raise RuntimeError(f\"Failed to encode image to base64: {image_path}\")",
            "lineNumber": 916
          },
          {
            "lineNumber": 917
          },
          {
            "text": "            # Call vision model with encoded image (safely handle image_data parameter)",
            "lineNumber": 918
          },
          {
            "text": "            response = await self._call_vision_model_safe(",
            "lineNumber": 919
          },
          {
            "text": "                vision_prompt,",
            "lineNumber": 920
          },
          {
            "text": "                image_data=image_base64,",
            "lineNumber": 921
          },
          {
            "text": "                system_prompt=PROMPTS[\"IMAGE_ANALYSIS_SYSTEM\"],",
            "lineNumber": 922
          },
          {
            "text": "            )",
            "lineNumber": 923
          },
          {
            "lineNumber": 924
          },
          {
            "text": "            # Parse response (reuse existing logic)",
            "lineNumber": 925
          },
          {
            "text": "            enhanced_caption, entity_info = self._parse_response(response, entity_name)",
            "lineNumber": 926
          },
          {
            "lineNumber": 927
          },
          {
            "text": "            return enhanced_caption, entity_info",
            "lineNumber": 928
          },
          {
            "lineNumber": 929
          },
          {
            "text": "        except Exception as e:",
            "lineNumber": 930
          },
          {
            "text": "            logger.error",
            "lineNumber": 931
          }
        ]
      },
      "score": 0.36461734771728516
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README.md",
        "range": {
          "startPosition": {
            "line": 800
          },
          "endPosition": {
            "line": 854
          }
        },
        "contents": "    )\n\n    # Define model functions\n    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):\n        return openai_complete_if_cache(\n            \"gpt-4o-mini\",\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            api_key=api_key,\n            base_url=base_url,\n            **kwargs,\n        )\n\n    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):\n        # If messages format is provided (for multimodal VLM enhanced query), use it directly\n        if messages:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=messages,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # Traditional single image format\n        elif image_data:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": prompt},\n                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}}\n                        ],\n                    } if image_data else {\"role\": \"user\", \"content\": prompt},\n                ],\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # Pure text format\n        else:\n            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)\n\n    embedding_func = EmbeddingFunc(\n        embedding_dim=3072,",
        "signatures": {},
        "detailedLines": [
          {
            "text": "    )",
            "lineNumber": 801
          },
          {
            "lineNumber": 802
          },
          {
            "text": "    # Define model functions",
            "lineNumber": 803
          },
          {
            "text": "    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):",
            "lineNumber": 804
          },
          {
            "text": "        return openai_complete_if_cache(",
            "lineNumber": 805
          },
          {
            "text": "            \"gpt-4o-mini\",",
            "lineNumber": 806
          },
          {
            "text": "            prompt,",
            "lineNumber": 807
          },
          {
            "text": "            system_prompt=system_prompt,",
            "lineNumber": 808
          },
          {
            "text": "            history_messages=history_messages,",
            "lineNumber": 809
          },
          {
            "text": "            api_key=api_key,",
            "lineNumber": 810
          },
          {
            "text": "            base_url=base_url,",
            "lineNumber": 811
          },
          {
            "text": "            **kwargs,",
            "lineNumber": 812
          },
          {
            "text": "        )",
            "lineNumber": 813
          },
          {
            "lineNumber": 814
          },
          {
            "text": "    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):",
            "lineNumber": 815
          },
          {
            "text": "        # If messages format is provided (for multimodal VLM enhanced query), use it directly",
            "lineNumber": 816
          },
          {
            "text": "        if messages:",
            "lineNumber": 817
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 818
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 819
          },
          {
            "text": "                \"\",",
            "lineNumber": 820
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 821
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 822
          },
          {
            "text": "                messages=messages,",
            "lineNumber": 823
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 824
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 825
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 826
          },
          {
            "text": "            )",
            "lineNumber": 827
          },
          {
            "text": "        # Traditional single image format",
            "lineNumber": 828
          },
          {
            "text": "        elif image_data:",
            "lineNumber": 829
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 830
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 831
          },
          {
            "text": "                \"\",",
            "lineNumber": 832
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 833
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 834
          },
          {
            "text": "                messages=[",
            "lineNumber": 835
          },
          {
            "text": "                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,",
            "lineNumber": 836
          },
          {
            "text": "                    {",
            "lineNumber": 837
          },
          {
            "text": "                        \"role\": \"user\",",
            "lineNumber": 838
          },
          {
            "text": "                        \"content\": [",
            "lineNumber": 839
          },
          {
            "text": "                            {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 840
          },
          {
            "text": "                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}}",
            "lineNumber": 841
          },
          {
            "text": "                        ],",
            "lineNumber": 842
          },
          {
            "text": "                    } if image_data else {\"role\": \"user\", \"content\": prompt},",
            "lineNumber": 843
          },
          {
            "text": "                ],",
            "lineNumber": 844
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 845
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 846
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 847
          },
          {
            "text": "            )",
            "lineNumber": 848
          },
          {
            "text": "        # Pure text format",
            "lineNumber": 849
          },
          {
            "text": "        else:",
            "lineNumber": 850
          },
          {
            "text": "            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)",
            "lineNumber": 851
          },
          {
            "lineNumber": 852
          },
          {
            "text": "    embedding_func = EmbeddingFunc(",
            "lineNumber": 853
          },
          {
            "text": "        embedding_dim=3072,",
            "lineNumber": 854
          }
        ]
      },
      "score": 0.3538401126861572
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\modalprocessors.py",
        "range": {
          "startPosition": {
            "line": 749
          },
          "endPosition": {
            "line": 1032
          }
        },
        "contents": "class ImageModalProcessor(BaseModalProcessor):\nasync def _call_vision_model_safe(\n        self, prompt: str, image_data: str = None, system_prompt: str = None, **kwargs\n    ):\n\n            # If messages doesn't work, log warning and fallback to text-only\n            logger.warning(\n                \"modal_caption_func doesn't support image_data or messages, falling back to text-only. \"\n                \"Consider providing a vision_model_func that supports image_data parameter.\"\n            )\n            # Remove image_data from kwargs to avoid passing it to underlying API\n            kwargs_clean = {k: v for k, v in kwargs.items() if k != \"image_data\"}\n            return await self.modal_caption_func(\n                prompt, system_prompt=system_prompt, **kwargs_clean\n            )\n        else:\n            # Function supports image_data, call directly\n            # Still remove image_data from kwargs to avoid duplicate parameter\n            kwargs_clean = {k: v for k, v in kwargs.items() if k != \"image_data\"}\n            return await self.modal_caption_func(\n                prompt, image_data=image_data, system_prompt=system_prompt, **kwargs_clean\n            )\n\n    def _encode_image_to_base64(self, image_path: str) -&gt; str:\n        \"\"\"Encode image to base64\"\"\"\n        try:\n            with open(image_path, \"rb\") as image_file:\n                encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n            return encoded_string\n        except Exception as e:\n            logger.error(f\"Failed to encode image {image_path}: {e}\")\n            return \"\"\n\n    async def generate_description_only(\n        self,\n        modal_content,\n        content_type: str,\n        item_info: Dict[str, Any] = None,\n        entity_name: str = None,\n    ) -&gt; Tuple[str, Dict[str, Any]]:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 750,
                "column": 1
              },
              "endPosition": {
                "line": 751,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 777,
                "column": 5
              },
              "endPosition": {
                "line": 780,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class ImageModalProcessor(BaseModalProcessor):",
            "lineNumber": 750,
            "isSignature": true
          },
          {
            "text": "async def _call_vision_model_safe(",
            "lineNumber": 777
          },
          {
            "text": "        self, prompt: str, image_data: str = None, system_prompt: str = None, **kwargs",
            "lineNumber": 778
          },
          {
            "text": "    ):",
            "lineNumber": 779
          },
          {
            "lineNumber": 813
          },
          {
            "text": "            # If messages doesn't work, log warning and fallback to text-only",
            "lineNumber": 814
          },
          {
            "text": "            logger.warning(",
            "lineNumber": 815
          },
          {
            "text": "                \"modal_caption_func doesn't support image_data or messages, falling back to text-only. \"",
            "lineNumber": 816
          },
          {
            "text": "                \"Consider providing a vision_model_func that supports image_data parameter.\"",
            "lineNumber": 817
          },
          {
            "text": "            )",
            "lineNumber": 818
          },
          {
            "text": "            # Remove image_data from kwargs to avoid passing it to underlying API",
            "lineNumber": 819
          },
          {
            "text": "            kwargs_clean = {k: v for k, v in kwargs.items() if k != \"image_data\"}",
            "lineNumber": 820
          },
          {
            "text": "            return await self.modal_caption_func(",
            "lineNumber": 821
          },
          {
            "text": "                prompt, system_prompt=system_prompt, **kwargs_clean",
            "lineNumber": 822
          },
          {
            "text": "            )",
            "lineNumber": 823
          },
          {
            "text": "        else:",
            "lineNumber": 824
          },
          {
            "text": "            # Function supports image_data, call directly",
            "lineNumber": 825
          },
          {
            "text": "            # Still remove image_data from kwargs to avoid duplicate parameter",
            "lineNumber": 826
          },
          {
            "text": "            kwargs_clean = {k: v for k, v in kwargs.items() if k != \"image_data\"}",
            "lineNumber": 827
          },
          {
            "text": "            return await self.modal_caption_func(",
            "lineNumber": 828
          },
          {
            "text": "                prompt, image_data=image_data, system_prompt=system_prompt, **kwargs_clean",
            "lineNumber": 829
          },
          {
            "text": "            )",
            "lineNumber": 830
          },
          {
            "lineNumber": 831
          },
          {
            "text": "    def _encode_image_to_base64(self, image_path: str) -&gt; str:",
            "lineNumber": 832
          },
          {
            "text": "        \"\"\"Encode image to base64\"\"\"",
            "lineNumber": 833
          },
          {
            "text": "        try:",
            "lineNumber": 834
          },
          {
            "text": "            with open(image_path, \"rb\") as image_file:",
            "lineNumber": 835
          },
          {
            "text": "                encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")",
            "lineNumber": 836
          },
          {
            "text": "            return encoded_string",
            "lineNumber": 837
          },
          {
            "text": "        except Exception as e:",
            "lineNumber": 838
          },
          {
            "text": "            logger.error(f\"Failed to encode image {image_path}: {e}\")",
            "lineNumber": 839
          },
          {
            "text": "            return \"\"",
            "lineNumber": 840
          },
          {
            "lineNumber": 841
          },
          {
            "text": "    async def generate_description_only(",
            "lineNumber": 842
          },
          {
            "text": "        self,",
            "lineNumber": 843
          },
          {
            "text": "        modal_content,",
            "lineNumber": 844
          },
          {
            "text": "        content_type: str,",
            "lineNumber": 845
          },
          {
            "text": "        item_info: Dict[str, Any] = None,",
            "lineNumber": 846
          },
          {
            "text": "        entity_name: str = None,",
            "lineNumber": 847
          },
          {
            "text": "    ) -&gt; Tuple[str, Dict[str, Any]]:",
            "lineNumber": 848
          },
          {
            "text": "        ",
            "lineNumber": 849
          }
        ]
      },
      "score": 0.3529500365257263
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {
            "line": 53,
            "column": 51
          },
          "endPosition": {
            "line": 137
          }
        },
        "contents": "def _build_llm_vision_embedding():\n    api_key = os.getenv(\"LLM_BINDING_API_KEY\")\n    base_url = os.getenv(\"LLM_BINDING_HOST\")\n\n    if not api_key:\n        raise RuntimeError(\"Missing env var: LLM_BINDING_API_KEY\")\n\n    llm_model = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n    vlm_model = os.getenv(\"VLM_MODEL\", \"gpt-4o\")\n\n    def llm_model_func(prompt, system_prompt=None, history_messages=None, **kwargs):\n        return openai_complete_if_cache(\n            llm_model,\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages or [],\n            api_key=api_key,\n            base_url=base_url,\n            **kwargs,\n        )\n\n    def vision_model_func(\n        prompt,\n        system_prompt=None,\n        history_messages=None,\n        image_data=None,\n        messages=None,\n        **kwargs,\n    ):\n        # VLM enhanced query will pass messages (multimodal format) when images are found.\n        if messages:\n            return openai_complete_if_cache(\n                vlm_model,\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=messages,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        if image_data:\n            # Single image format (used by image processor)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def _build_llm_vision_embedding():",
            "lineNumber": 57,
            "isSignature": true
          },
          {
            "text": "    api_key = os.getenv(\"LLM_BINDING_API_KEY\")",
            "lineNumber": 58
          },
          {
            "text": "    base_url = os.getenv(\"LLM_BINDING_HOST\")",
            "lineNumber": 59
          },
          {
            "lineNumber": 60
          },
          {
            "text": "    if not api_key:",
            "lineNumber": 61
          },
          {
            "text": "        raise RuntimeError(\"Missing env var: LLM_BINDING_API_KEY\")",
            "lineNumber": 62
          },
          {
            "lineNumber": 63
          },
          {
            "text": "    llm_model = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")",
            "lineNumber": 64
          },
          {
            "text": "    vlm_model = os.getenv(\"VLM_MODEL\", \"gpt-4o\")",
            "lineNumber": 65
          },
          {
            "lineNumber": 66
          },
          {
            "text": "    def llm_model_func(prompt, system_prompt=None, history_messages=None, **kwargs):",
            "lineNumber": 67
          },
          {
            "text": "        return openai_complete_if_cache(",
            "lineNumber": 68
          },
          {
            "text": "            llm_model,",
            "lineNumber": 69
          },
          {
            "text": "            prompt,",
            "lineNumber": 70
          },
          {
            "text": "            system_prompt=system_prompt,",
            "lineNumber": 71
          },
          {
            "text": "            history_messages=history_messages or [],",
            "lineNumber": 72
          },
          {
            "text": "            api_key=api_key,",
            "lineNumber": 73
          },
          {
            "text": "            base_url=base_url,",
            "lineNumber": 74
          },
          {
            "text": "            **kwargs,",
            "lineNumber": 75
          },
          {
            "text": "        )",
            "lineNumber": 76
          },
          {
            "lineNumber": 77
          },
          {
            "text": "    def vision_model_func(",
            "lineNumber": 78
          },
          {
            "text": "        prompt,",
            "lineNumber": 79
          },
          {
            "text": "        system_prompt=None,",
            "lineNumber": 80
          },
          {
            "text": "        history_messages=None,",
            "lineNumber": 81
          },
          {
            "text": "        image_data=None,",
            "lineNumber": 82
          },
          {
            "text": "        messages=None,",
            "lineNumber": 83
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 84
          },
          {
            "text": "    ):",
            "lineNumber": 85
          },
          {
            "text": "        # VLM enhanced query will pass messages (multimodal format) when images are found.",
            "lineNumber": 86
          },
          {
            "text": "        if messages:",
            "lineNumber": 87
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 88
          },
          {
            "text": "                vlm_model,",
            "lineNumber": 89
          },
          {
            "text": "                \"\",",
            "lineNumber": 90
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 91
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 92
          },
          {
            "text": "                messages=messages,",
            "lineNumber": 93
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 94
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 95
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 96
          },
          {
            "text": "            )",
            "lineNumber": 97
          },
          {
            "text": "        if image_data:",
            "lineNumber": 98
          },
          {
            "text": "            # Single image format (used by image processor)",
            "lineNumber": 99
          }
        ]
      },
      "score": 0.35181188583374023
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\RAG-Anything-試用報告-cursor.md",
        "range": {
          "startPosition": {
            "line": 295,
            "column": 5
          },
          "endPosition": {
            "line": 337,
            "column": 5
          }
        },
        "contents": "- 適合丟 Excel/BI 做圖表\n\n4. **如何判讀結果**：\n   - **平均分數（avg_score）**：越高越接近 ground truth\n   - **平均耗時（avg_elapsed_s）**：越低越好\n   - 建議再按 &#96;query_type&#96; 分組（事實/解釋/摘要），看 hybrid 在複雜題是否優勢更明顯\n\n**驗收標準：**\n- Hybrid 模式在複雜查詢上的相似度分數 ≥ 0.75\n- Hybrid 模式優於單一模式（naive/local/global）的查詢比例 ≥ 70%\n- 各模式的回應時間在可接受範圍內（&lt; 30 秒）\n- 檢索到的上下文相關性 ≥ 80%\n\n---\n\n#### 6.1.3 VLM 增強查詢功能測試\n\n**測試目標：**\n- 驗證 VLM（Vision Language Model）能正確分析檢索到的圖片\n- 確認圖片路徑能正確轉換為 base64 編碼並傳送給 VLM\n- 驗證 VLM 增強查詢的回應品質優於純文字查詢\n- 確認系統在無圖片時能正確降級為純文字查詢\n\n**測試方法（可重現、可驗證 VLM 流程是否真的被走到）：**\n\n核心判準：VLM enhanced query 需要在檢索 prompt 中出現 &#96;Image Path: ...&#96;，系統才會抓圖、轉 base64、以 multimodal messages 呼叫 &#96;vision_model_func&#96;。\n\n1. **準備含圖片的測試資料**：\n   - 可直接用 repo 現有圖檔：&#96;eval/docs/全球資訊服務暨軟體市場規模.png&#96;、&#96;eval/docs/cat-01.jpg&#96; 等\n\n2. **執行測試腳本（同題目 VLM on/off 對照）**：\n\n   &#96;&#96;&#96;powershell\n   uv run eval/integration/vlm_enhanced_query_eval.py &#96;\n     --docs eval/docs/全球資訊服務暨軟體市場規模.png eval/docs/cat-01.jpg &#96;\n     --working-dir eval/integration/rag_storage_vlm_eval &#96;\n     --out-dir eval/integration/results &#96;\n     --mode hybrid\n   &#96;&#96;&#96;\n\n3. **產出物與驗證點**：\n   - &#96;eval/integration/results/vlm_enhanced_query/&lt;timestamp&gt;/results.json&#96;\n     ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "- 適合丟 Excel/BI 做圖表",
            "lineNumber": 296
          },
          {
            "lineNumber": 297
          },
          {
            "text": "4. **如何判讀結果**：",
            "lineNumber": 298
          },
          {
            "text": "   - **平均分數（avg_score）**：越高越接近 ground truth",
            "lineNumber": 299
          },
          {
            "text": "   - **平均耗時（avg_elapsed_s）**：越低越好",
            "lineNumber": 300
          },
          {
            "text": "   - 建議再按 &#96;query_type&#96; 分組（事實/解釋/摘要），看 hybrid 在複雜題是否優勢更明顯",
            "lineNumber": 301
          },
          {
            "lineNumber": 302
          },
          {
            "text": "**驗收標準：**",
            "lineNumber": 303
          },
          {
            "text": "- Hybrid 模式在複雜查詢上的相似度分數 ≥ 0.75",
            "lineNumber": 304
          },
          {
            "text": "- Hybrid 模式優於單一模式（naive/local/global）的查詢比例 ≥ 70%",
            "lineNumber": 305
          },
          {
            "text": "- 各模式的回應時間在可接受範圍內（&lt; 30 秒）",
            "lineNumber": 306
          },
          {
            "text": "- 檢索到的上下文相關性 ≥ 80%",
            "lineNumber": 307
          },
          {
            "lineNumber": 308
          },
          {
            "text": "---",
            "lineNumber": 309
          },
          {
            "lineNumber": 310
          },
          {
            "text": "#### 6.1.3 VLM 增強查詢功能測試",
            "lineNumber": 311
          },
          {
            "lineNumber": 312
          },
          {
            "text": "**測試目標：**",
            "lineNumber": 313
          },
          {
            "text": "- 驗證 VLM（Vision Language Model）能正確分析檢索到的圖片",
            "lineNumber": 314
          },
          {
            "text": "- 確認圖片路徑能正確轉換為 base64 編碼並傳送給 VLM",
            "lineNumber": 315
          },
          {
            "text": "- 驗證 VLM 增強查詢的回應品質優於純文字查詢",
            "lineNumber": 316
          },
          {
            "text": "- 確認系統在無圖片時能正確降級為純文字查詢",
            "lineNumber": 317
          },
          {
            "lineNumber": 318
          },
          {
            "text": "**測試方法（可重現、可驗證 VLM 流程是否真的被走到）：**",
            "lineNumber": 319
          },
          {
            "lineNumber": 320
          },
          {
            "text": "核心判準：VLM enhanced query 需要在檢索 prompt 中出現 &#96;Image Path: ...&#96;，系統才會抓圖、轉 base64、以 multimodal messages 呼叫 &#96;vision_model_func&#96;。",
            "lineNumber": 321
          },
          {
            "lineNumber": 322
          },
          {
            "text": "1. **準備含圖片的測試資料**：",
            "lineNumber": 323
          },
          {
            "text": "   - 可直接用 repo 現有圖檔：&#96;eval/docs/全球資訊服務暨軟體市場規模.png&#96;、&#96;eval/docs/cat-01.jpg&#96; 等",
            "lineNumber": 324
          },
          {
            "lineNumber": 325
          },
          {
            "text": "2. **執行測試腳本（同題目 VLM on/off 對照）**：",
            "lineNumber": 326
          },
          {
            "lineNumber": 327
          },
          {
            "text": "   &#96;&#96;&#96;powershell",
            "lineNumber": 328
          },
          {
            "text": "   uv run eval/integration/vlm_enhanced_query_eval.py &#96;",
            "lineNumber": 329
          },
          {
            "text": "     --docs eval/docs/全球資訊服務暨軟體市場規模.png eval/docs/cat-01.jpg &#96;",
            "lineNumber": 330
          },
          {
            "text": "     --working-dir eval/integration/rag_storage_vlm_eval &#96;",
            "lineNumber": 331
          },
          {
            "text": "     --out-dir eval/integration/results &#96;",
            "lineNumber": 332
          },
          {
            "text": "     --mode hybrid",
            "lineNumber": 333
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 334
          },
          {
            "lineNumber": 335
          },
          {
            "text": "3. **產出物與驗證點**：",
            "lineNumber": 336
          },
          {
            "text": "   - &#96;eval/integration/results/vlm_enhanced_query/&lt;timestamp&gt;/results.json&#96;",
            "lineNumber": 337
          },
          {
            "text": "     ",
            "lineNumber": 338
          }
        ]
      },
      "score": 0.35051119327545166
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\modalprocessors.py",
        "range": {
          "startPosition": {
            "line": 749
          },
          "endPosition": {
            "line": 1032
          }
        },
        "contents": "class ImageModalProcessor(BaseModalProcessor):\ndef _check_func_supports_image_data(self, func) -&gt; bool:\n\n            return False\n\n    async def _call_vision_model_safe(\n        self, prompt: str, image_data: str = None, system_prompt: str = None, **kwargs\n    ):\n        \"\"\"Safely call vision model function, handling image_data parameter\"\"\"\n        if not image_data:\n            # No image data, call normally\n            return await self.modal_caption_func(prompt, system_prompt=system_prompt, **kwargs)\n\n        # Check if function supports image_data parameter\n        if not self._check_func_supports_image_data(self.modal_caption_func):\n            # Function doesn't support image_data, convert to messages format\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": prompt},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                        },\n                    ],\n                }\n            )\n            # Try calling with messages parameter\n            try:\n                sig = inspect.signature(self.modal_caption_func)\n                if \"messages\" in sig.parameters:\n                    # Remove image_data from kwargs to avoid passing it to underlying API\n                    kwargs_clean = {k: v for k, v in kwargs.items() if k != \"image_data\"}\n                    return await self.modal_caption_func(\n                        prompt=\"\", messages=messages, system_prompt=None, **kwargs_clean\n                    )\n            except (ValueError, TypeError):\n                pass\n            # If messages doesn't work, log warning and fallback to text-only\n            logger.warning(\n                \"",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 750,
                "column": 1
              },
              "endPosition": {
                "line": 751,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class ImageModalProcessor(BaseModalProcessor):",
            "lineNumber": 750,
            "isSignature": true
          },
          {
            "text": "def _check_func_supports_image_data(self, func) -&gt; bool:",
            "lineNumber": 768
          },
          {
            "lineNumber": 774
          },
          {
            "text": "            return False",
            "lineNumber": 775
          },
          {
            "lineNumber": 776
          },
          {
            "text": "    async def _call_vision_model_safe(",
            "lineNumber": 777
          },
          {
            "text": "        self, prompt: str, image_data: str = None, system_prompt: str = None, **kwargs",
            "lineNumber": 778
          },
          {
            "text": "    ):",
            "lineNumber": 779
          },
          {
            "text": "        \"\"\"Safely call vision model function, handling image_data parameter\"\"\"",
            "lineNumber": 780
          },
          {
            "text": "        if not image_data:",
            "lineNumber": 781
          },
          {
            "text": "            # No image data, call normally",
            "lineNumber": 782
          },
          {
            "text": "            return await self.modal_caption_func(prompt, system_prompt=system_prompt, **kwargs)",
            "lineNumber": 783
          },
          {
            "lineNumber": 784
          },
          {
            "text": "        # Check if function supports image_data parameter",
            "lineNumber": 785
          },
          {
            "text": "        if not self._check_func_supports_image_data(self.modal_caption_func):",
            "lineNumber": 786
          },
          {
            "text": "            # Function doesn't support image_data, convert to messages format",
            "lineNumber": 787
          },
          {
            "text": "            messages = []",
            "lineNumber": 788
          },
          {
            "text": "            if system_prompt:",
            "lineNumber": 789
          },
          {
            "text": "                messages.append({\"role\": \"system\", \"content\": system_prompt})",
            "lineNumber": 790
          },
          {
            "text": "            messages.append(",
            "lineNumber": 791
          },
          {
            "text": "                {",
            "lineNumber": 792
          },
          {
            "text": "                    \"role\": \"user\",",
            "lineNumber": 793
          },
          {
            "text": "                    \"content\": [",
            "lineNumber": 794
          },
          {
            "text": "                        {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 795
          },
          {
            "text": "                        {",
            "lineNumber": 796
          },
          {
            "text": "                            \"type\": \"image_url\",",
            "lineNumber": 797
          },
          {
            "text": "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},",
            "lineNumber": 798
          },
          {
            "text": "                        },",
            "lineNumber": 799
          },
          {
            "text": "                    ],",
            "lineNumber": 800
          },
          {
            "text": "                }",
            "lineNumber": 801
          },
          {
            "text": "            )",
            "lineNumber": 802
          },
          {
            "text": "            # Try calling with messages parameter",
            "lineNumber": 803
          },
          {
            "text": "            try:",
            "lineNumber": 804
          },
          {
            "text": "                sig = inspect.signature(self.modal_caption_func)",
            "lineNumber": 805
          },
          {
            "text": "                if \"messages\" in sig.parameters:",
            "lineNumber": 806
          },
          {
            "text": "                    # Remove image_data from kwargs to avoid passing it to underlying API",
            "lineNumber": 807
          },
          {
            "text": "                    kwargs_clean = {k: v for k, v in kwargs.items() if k != \"image_data\"}",
            "lineNumber": 808
          },
          {
            "text": "                    return await self.modal_caption_func(",
            "lineNumber": 809
          },
          {
            "text": "                        prompt=\"\", messages=messages, system_prompt=None, **kwargs_clean",
            "lineNumber": 810
          },
          {
            "text": "                    )",
            "lineNumber": 811
          },
          {
            "text": "            except (ValueError, TypeError):",
            "lineNumber": 812
          },
          {
            "text": "                pass",
            "lineNumber": 813
          },
          {
            "text": "            # If messages doesn't work, log warning and fallback to text-only",
            "lineNumber": 814
          },
          {
            "text": "            logger.warning(",
            "lineNumber": 815
          },
          {
            "text": "                \"",
            "lineNumber": 816
          }
        ]
      },
      "score": 0.3445056676864624
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README_zh.md",
        "range": {
          "startPosition": {
            "line": 791
          },
          "endPosition": {
            "line": 847
          }
        },
        "contents": "            base_url=base_url,\n            **kwargs,\n        )\n\n    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):\n        # 如果提供了messages格式（用于多模态VLM增强查询），直接使用\n        if messages:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=messages,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # 传统单图片格式\n        elif image_data:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": prompt},\n                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}}\n                        ],\n                    } if image_data else {\"role\": \"user\", \"content\": prompt},\n                ],\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # 纯文本格式\n        else:\n            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)\n\n    embedding_func = EmbeddingFunc(\n        embedding_dim=3072,\n        max_token_size=8192,\n        func=lambda texts: openai_embed(\n            texts,\n            model=\"text-embedding-3-large\",\n            api_key=api_key,\n            base_url=base_url,\n        ),\n    )\n\n    # 初始化 RAGAnything\n    rag = RAGAnything(\n        config=config,",
        "signatures": {},
        "detailedLines": [
          {
            "text": "            base_url=base_url,",
            "lineNumber": 792
          },
          {
            "text": "            **kwargs,",
            "lineNumber": 793
          },
          {
            "text": "        )",
            "lineNumber": 794
          },
          {
            "lineNumber": 795
          },
          {
            "text": "    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):",
            "lineNumber": 796
          },
          {
            "text": "        # 如果提供了messages格式（用于多模态VLM增强查询），直接使用",
            "lineNumber": 797
          },
          {
            "text": "        if messages:",
            "lineNumber": 798
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 799
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 800
          },
          {
            "text": "                \"\",",
            "lineNumber": 801
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 802
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 803
          },
          {
            "text": "                messages=messages,",
            "lineNumber": 804
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 805
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 806
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 807
          },
          {
            "text": "            )",
            "lineNumber": 808
          },
          {
            "text": "        # 传统单图片格式",
            "lineNumber": 809
          },
          {
            "text": "        elif image_data:",
            "lineNumber": 810
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 811
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 812
          },
          {
            "text": "                \"\",",
            "lineNumber": 813
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 814
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 815
          },
          {
            "text": "                messages=[",
            "lineNumber": 816
          },
          {
            "text": "                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,",
            "lineNumber": 817
          },
          {
            "text": "                    {",
            "lineNumber": 818
          },
          {
            "text": "                        \"role\": \"user\",",
            "lineNumber": 819
          },
          {
            "text": "                        \"content\": [",
            "lineNumber": 820
          },
          {
            "text": "                            {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 821
          },
          {
            "text": "                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}}",
            "lineNumber": 822
          },
          {
            "text": "                        ],",
            "lineNumber": 823
          },
          {
            "text": "                    } if image_data else {\"role\": \"user\", \"content\": prompt},",
            "lineNumber": 824
          },
          {
            "text": "                ],",
            "lineNumber": 825
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 826
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 827
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 828
          },
          {
            "text": "            )",
            "lineNumber": 829
          },
          {
            "text": "        # 纯文本格式",
            "lineNumber": 830
          },
          {
            "text": "        else:",
            "lineNumber": 831
          },
          {
            "text": "            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)",
            "lineNumber": 832
          },
          {
            "lineNumber": 833
          },
          {
            "text": "    embedding_func = EmbeddingFunc(",
            "lineNumber": 834
          },
          {
            "text": "        embedding_dim=3072,",
            "lineNumber": 835
          },
          {
            "text": "        max_token_size=8192,",
            "lineNumber": 836
          },
          {
            "text": "        func=lambda texts: openai_embed(",
            "lineNumber": 837
          },
          {
            "text": "            texts,",
            "lineNumber": 838
          },
          {
            "text": "            model=\"text-embedding-3-large\",",
            "lineNumber": 839
          },
          {
            "text": "            api_key=api_key,",
            "lineNumber": 840
          },
          {
            "text": "            base_url=base_url,",
            "lineNumber": 841
          },
          {
            "text": "        ),",
            "lineNumber": 842
          },
          {
            "text": "    )",
            "lineNumber": 843
          },
          {
            "lineNumber": 844
          },
          {
            "text": "    # 初始化 RAGAnything",
            "lineNumber": 845
          },
          {
            "text": "    rag = RAGAnything(",
            "lineNumber": 846
          },
          {
            "text": "        config=config,",
            "lineNumber": 847
          }
        ]
      },
      "score": 0.33761584758758545
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\n\n\n    async def _describe_image_for_query(self, processor, content: Dict[str, Any]) -&gt; str:\n        \"\"\"Generate image description for query\"\"\"\n        image_path = content.get(\"img_path\")\n        captions = content.get(\"image_caption\", content.get(\"img_caption\", []))\n        footnotes = content.get(\"image_footnote\", content.get(\"img_footnote\", []))\n\n        if image_path and Path(image_path).exists():\n            # If image exists, use vision model to generate description\n            image_base64 = processor._encode_image_to_base64(image_path)\n            if image_base64:\n                prompt = PROMPTS[\"QUERY_IMAGE_DESCRIPTION\"]\n                description = await processor.modal_caption_func(\n                    prompt,\n                    image_data=image_base64,\n                    system_prompt=PROMPTS[\"QUERY_IMAGE_ANALYST_SYSTEM\"],\n                )\n                return description\n\n        # If image doesn't exist or processing failed, use existing information\n        parts = []\n        if image_path:\n            parts.append(f\"Image path: {image_path}\")\n        if captions:\n            parts.append(f\"Image captions: {', '.join(captions)}\")\n        if footnotes:\n            parts.append(f\"Image footnotes: {', '.join(footnotes)}\")\n\n        return \"; \".join(parts) if parts else \"Image content information incomplete\"\n\n    async def _describe_table_for_query(self, processor, content: Dict[str, Any]) -&gt; str:\n        \"\"\"Generate table description for query\"\"\"\n        table_data = content.get(\"table_data\", \"\")\n        table_caption = content.get(\"table_caption\", \"\")\n\n        prompt = PROMPTS[\"QUERY_TABLE_ANALYSIS\"].format(\n            table_data=table_data, table_caption=table_caption\n        )\n\n        description = await processor.modal_caption_func(\n            prompt, system_prompt=PROMPTS[\"QUERY_TABLE_ANALYST_SYSTEM\"]\n        )\n\n        return description\n\n    async def _describe_equation_for_query(self, processor, content: Dict[str, Any]) -&gt; str:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "lineNumber": 404
          },
          {
            "lineNumber": 405
          },
          {
            "text": "    async def _describe_image_for_query(self, processor, content: Dict[str, Any]) -&gt; str:",
            "lineNumber": 406
          },
          {
            "text": "        \"\"\"Generate image description for query\"\"\"",
            "lineNumber": 407
          },
          {
            "text": "        image_path = content.get(\"img_path\")",
            "lineNumber": 408
          },
          {
            "text": "        captions = content.get(\"image_caption\", content.get(\"img_caption\", []))",
            "lineNumber": 409
          },
          {
            "text": "        footnotes = content.get(\"image_footnote\", content.get(\"img_footnote\", []))",
            "lineNumber": 410
          },
          {
            "lineNumber": 411
          },
          {
            "text": "        if image_path and Path(image_path).exists():",
            "lineNumber": 412
          },
          {
            "text": "            # If image exists, use vision model to generate description",
            "lineNumber": 413
          },
          {
            "text": "            image_base64 = processor._encode_image_to_base64(image_path)",
            "lineNumber": 414
          },
          {
            "text": "            if image_base64:",
            "lineNumber": 415
          },
          {
            "text": "                prompt = PROMPTS[\"QUERY_IMAGE_DESCRIPTION\"]",
            "lineNumber": 416
          },
          {
            "text": "                description = await processor.modal_caption_func(",
            "lineNumber": 417
          },
          {
            "text": "                    prompt,",
            "lineNumber": 418
          },
          {
            "text": "                    image_data=image_base64,",
            "lineNumber": 419
          },
          {
            "text": "                    system_prompt=PROMPTS[\"QUERY_IMAGE_ANALYST_SYSTEM\"],",
            "lineNumber": 420
          },
          {
            "text": "                )",
            "lineNumber": 421
          },
          {
            "text": "                return description",
            "lineNumber": 422
          },
          {
            "lineNumber": 423
          },
          {
            "text": "        # If image doesn't exist or processing failed, use existing information",
            "lineNumber": 424
          },
          {
            "text": "        parts = []",
            "lineNumber": 425
          },
          {
            "text": "        if image_path:",
            "lineNumber": 426
          },
          {
            "text": "            parts.append(f\"Image path: {image_path}\")",
            "lineNumber": 427
          },
          {
            "text": "        if captions:",
            "lineNumber": 428
          },
          {
            "text": "            parts.append(f\"Image captions: {', '.join(captions)}\")",
            "lineNumber": 429
          },
          {
            "text": "        if footnotes:",
            "lineNumber": 430
          },
          {
            "text": "            parts.append(f\"Image footnotes: {', '.join(footnotes)}\")",
            "lineNumber": 431
          },
          {
            "lineNumber": 432
          },
          {
            "text": "        return \"; \".join(parts) if parts else \"Image content information incomplete\"",
            "lineNumber": 433
          },
          {
            "lineNumber": 434
          },
          {
            "text": "    async def _describe_table_for_query(self, processor, content: Dict[str, Any]) -&gt; str:",
            "lineNumber": 435
          },
          {
            "text": "        \"\"\"Generate table description for query\"\"\"",
            "lineNumber": 436
          },
          {
            "text": "        table_data = content.get(\"table_data\", \"\")",
            "lineNumber": 437
          },
          {
            "text": "        table_caption = content.get(\"table_caption\", \"\")",
            "lineNumber": 438
          },
          {
            "lineNumber": 439
          },
          {
            "text": "        prompt = PROMPTS[\"QUERY_TABLE_ANALYSIS\"].format(",
            "lineNumber": 440
          },
          {
            "text": "            table_data=table_data, table_caption=table_caption",
            "lineNumber": 441
          },
          {
            "text": "        )",
            "lineNumber": 442
          },
          {
            "lineNumber": 443
          },
          {
            "text": "        description = await processor.modal_caption_func(",
            "lineNumber": 444
          },
          {
            "text": "            prompt, system_prompt=PROMPTS[\"QUERY_TABLE_ANALYST_SYSTEM\"]",
            "lineNumber": 445
          },
          {
            "text": "        )",
            "lineNumber": 446
          },
          {
            "lineNumber": 447
          },
          {
            "text": "        return description",
            "lineNumber": 448
          },
          {
            "lineNumber": 449
          },
          {
            "text": "    async def _describe_equation_for_query(self, processor, content: Dict[str, Any]) -&gt; str:",
            "lineNumber": 450
          },
          {
            "text": "        ",
            "lineNumber": 451
          }
        ]
      },
      "score": 0.33697575330734253
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README.md",
        "range": {
          "startPosition": {
            "line": 572
          },
          "endPosition": {
            "line": 626
          }
        },
        "contents": "class CustomModalProcessor(GenericModalProcessor):\n    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):\n        # Your custom processing logic\n        enhanced_description = await self.analyze_custom_content(modal_content)\n        entity_info = self.create_custom_entity(enhanced_description, entity_name)\n        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)\n&#96;&#96;&#96;\n\n#### 5. Query Options\n\nRAG-Anything provides three types of query methods:\n\n**Pure Text Queries** - Direct knowledge base search using LightRAG:\n&#96;&#96;&#96;python\n# Different query modes for text queries\ntext_result_hybrid = await rag.aquery(\"Your question\", mode=\"hybrid\")\ntext_result_local = await rag.aquery(\"Your question\", mode=\"local\")\ntext_result_global = await rag.aquery(\"Your question\", mode=\"global\")\ntext_result_naive = await rag.aquery(\"Your question\", mode=\"naive\")\n\n# Synchronous version\nsync_text_result = rag.query(\"Your question\", mode=\"hybrid\")\n&#96;&#96;&#96;\n\n**VLM Enhanced Queries** - Automatically analyze images in retrieved context using VLM:\n&#96;&#96;&#96;python\n# VLM enhanced query (automatically enabled when vision_model_func is provided)\nvlm_result = await rag.aquery(\n    \"Analyze the charts and figures in the document\",\n    mode=\"hybrid\"\n    # vlm_enhanced=True is automatically set when vision_model_func is available\n)\n\n# Manually control VLM enhancement\nvlm_enabled = await rag.aquery(\n    \"What do the images show in this document?\",\n    mode=\"hybrid\",\n    vlm_enhanced=True  # Force enable VLM enhancement\n)\n\nvlm_disabled = await rag.aquery(\n    \"What do the images show in this document?\",\n    mode=\"hybrid\",\n    vlm_enhanced=False  # Force disable VLM enhancement\n)\n\n# When documents contain images, VLM can see and analyze them directly\n# The system will automatically:\n# 1. Retrieve relevant context containing image paths\n# 2. Load and encode images as base64\n# 3. Send both text context and images to VLM for comprehensive analysis\n&#96;&#96;&#96;\n\n**Multimodal Queries** - Enhanced queries with specific multimodal content analysis:",
        "signatures": {},
        "detailedLines": [
          {
            "text": "class CustomModalProcessor(GenericModalProcessor):",
            "lineNumber": 573
          },
          {
            "text": "    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):",
            "lineNumber": 574
          },
          {
            "text": "        # Your custom processing logic",
            "lineNumber": 575
          },
          {
            "text": "        enhanced_description = await self.analyze_custom_content(modal_content)",
            "lineNumber": 576
          },
          {
            "text": "        entity_info = self.create_custom_entity(enhanced_description, entity_name)",
            "lineNumber": 577
          },
          {
            "text": "        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)",
            "lineNumber": 578
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 579
          },
          {
            "lineNumber": 580
          },
          {
            "text": "#### 5. Query Options",
            "lineNumber": 581
          },
          {
            "lineNumber": 582
          },
          {
            "text": "RAG-Anything provides three types of query methods:",
            "lineNumber": 583
          },
          {
            "lineNumber": 584
          },
          {
            "text": "**Pure Text Queries** - Direct knowledge base search using LightRAG:",
            "lineNumber": 585
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 586
          },
          {
            "text": "# Different query modes for text queries",
            "lineNumber": 587
          },
          {
            "text": "text_result_hybrid = await rag.aquery(\"Your question\", mode=\"hybrid\")",
            "lineNumber": 588
          },
          {
            "text": "text_result_local = await rag.aquery(\"Your question\", mode=\"local\")",
            "lineNumber": 589
          },
          {
            "text": "text_result_global = await rag.aquery(\"Your question\", mode=\"global\")",
            "lineNumber": 590
          },
          {
            "text": "text_result_naive = await rag.aquery(\"Your question\", mode=\"naive\")",
            "lineNumber": 591
          },
          {
            "lineNumber": 592
          },
          {
            "text": "# Synchronous version",
            "lineNumber": 593
          },
          {
            "text": "sync_text_result = rag.query(\"Your question\", mode=\"hybrid\")",
            "lineNumber": 594
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 595
          },
          {
            "lineNumber": 596
          },
          {
            "text": "**VLM Enhanced Queries** - Automatically analyze images in retrieved context using VLM:",
            "lineNumber": 597
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 598
          },
          {
            "text": "# VLM enhanced query (automatically enabled when vision_model_func is provided)",
            "lineNumber": 599
          },
          {
            "text": "vlm_result = await rag.aquery(",
            "lineNumber": 600
          },
          {
            "text": "    \"Analyze the charts and figures in the document\",",
            "lineNumber": 601
          },
          {
            "text": "    mode=\"hybrid\"",
            "lineNumber": 602
          },
          {
            "text": "    # vlm_enhanced=True is automatically set when vision_model_func is available",
            "lineNumber": 603
          },
          {
            "text": ")",
            "lineNumber": 604
          },
          {
            "lineNumber": 605
          },
          {
            "text": "# Manually control VLM enhancement",
            "lineNumber": 606
          },
          {
            "text": "vlm_enabled = await rag.aquery(",
            "lineNumber": 607
          },
          {
            "text": "    \"What do the images show in this document?\",",
            "lineNumber": 608
          },
          {
            "text": "    mode=\"hybrid\",",
            "lineNumber": 609
          },
          {
            "text": "    vlm_enhanced=True  # Force enable VLM enhancement",
            "lineNumber": 610
          },
          {
            "text": ")",
            "lineNumber": 611
          },
          {
            "lineNumber": 612
          },
          {
            "text": "vlm_disabled = await rag.aquery(",
            "lineNumber": 613
          },
          {
            "text": "    \"What do the images show in this document?\",",
            "lineNumber": 614
          },
          {
            "text": "    mode=\"hybrid\",",
            "lineNumber": 615
          },
          {
            "text": "    vlm_enhanced=False  # Force disable VLM enhancement",
            "lineNumber": 616
          },
          {
            "text": ")",
            "lineNumber": 617
          },
          {
            "lineNumber": 618
          },
          {
            "text": "# When documents contain images, VLM can see and analyze them directly",
            "lineNumber": 619
          },
          {
            "text": "# The system will automatically:",
            "lineNumber": 620
          },
          {
            "text": "# 1. Retrieve relevant context containing image paths",
            "lineNumber": 621
          },
          {
            "text": "# 2. Load and encode images as base64",
            "lineNumber": 622
          },
          {
            "text": "# 3. Send both text context and images to VLM for comprehensive analysis",
            "lineNumber": 623
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 624
          },
          {
            "lineNumber": 625
          },
          {
            "text": "**Multimodal Queries** - Enhanced queries with specific multimodal content analysis:",
            "lineNumber": 626
          }
        ]
      },
      "score": 0.33656880259513855
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README_zh.md",
        "range": {
          "startPosition": {
            "line": 684
          },
          "endPosition": {
            "line": 737
          }
        },
        "contents": "    await initialize_pipeline_status()\n\n    # 定义视觉模型函数用于图像处理\n    def vision_model_func(\n        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs\n    ):\n        # 如果提供了messages格式（用于多模态VLM增强查询），直接使用\n        if messages:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=messages,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # 传统单图片格式\n        elif image_data:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt}\n                    if system_prompt\n                    else None,\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": prompt},\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{image_data}\"\n                                },\n                            },\n                        ],\n                    }\n                    if image_data\n                    else {\"role\": \"user\", \"content\": prompt},\n                ],\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # 纯文本格式\n        else:\n            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)\n\n    # 现在使用已存在的 LightRAG 实例初始化 RAGAnything",
        "signatures": {},
        "detailedLines": [
          {
            "text": "    await initialize_pipeline_status()",
            "lineNumber": 685
          },
          {
            "lineNumber": 686
          },
          {
            "text": "    # 定义视觉模型函数用于图像处理",
            "lineNumber": 687
          },
          {
            "text": "    def vision_model_func(",
            "lineNumber": 688
          },
          {
            "text": "        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs",
            "lineNumber": 689
          },
          {
            "text": "    ):",
            "lineNumber": 690
          },
          {
            "text": "        # 如果提供了messages格式（用于多模态VLM增强查询），直接使用",
            "lineNumber": 691
          },
          {
            "text": "        if messages:",
            "lineNumber": 692
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 693
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 694
          },
          {
            "text": "                \"\",",
            "lineNumber": 695
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 696
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 697
          },
          {
            "text": "                messages=messages,",
            "lineNumber": 698
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 699
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 700
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 701
          },
          {
            "text": "            )",
            "lineNumber": 702
          },
          {
            "text": "        # 传统单图片格式",
            "lineNumber": 703
          },
          {
            "text": "        elif image_data:",
            "lineNumber": 704
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 705
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 706
          },
          {
            "text": "                \"\",",
            "lineNumber": 707
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 708
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 709
          },
          {
            "text": "                messages=[",
            "lineNumber": 710
          },
          {
            "text": "                    {\"role\": \"system\", \"content\": system_prompt}",
            "lineNumber": 711
          },
          {
            "text": "                    if system_prompt",
            "lineNumber": 712
          },
          {
            "text": "                    else None,",
            "lineNumber": 713
          },
          {
            "text": "                    {",
            "lineNumber": 714
          },
          {
            "text": "                        \"role\": \"user\",",
            "lineNumber": 715
          },
          {
            "text": "                        \"content\": [",
            "lineNumber": 716
          },
          {
            "text": "                            {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 717
          },
          {
            "text": "                            {",
            "lineNumber": 718
          },
          {
            "text": "                                \"type\": \"image_url\",",
            "lineNumber": 719
          },
          {
            "text": "                                \"image_url\": {",
            "lineNumber": 720
          },
          {
            "text": "                                    \"url\": f\"data:image/jpeg;base64,{image_data}\"",
            "lineNumber": 721
          },
          {
            "text": "                                },",
            "lineNumber": 722
          },
          {
            "text": "                            },",
            "lineNumber": 723
          },
          {
            "text": "                        ],",
            "lineNumber": 724
          },
          {
            "text": "                    }",
            "lineNumber": 725
          },
          {
            "text": "                    if image_data",
            "lineNumber": 726
          },
          {
            "text": "                    else {\"role\": \"user\", \"content\": prompt},",
            "lineNumber": 727
          },
          {
            "text": "                ],",
            "lineNumber": 728
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 729
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 730
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 731
          },
          {
            "text": "            )",
            "lineNumber": 732
          },
          {
            "text": "        # 纯文本格式",
            "lineNumber": 733
          },
          {
            "text": "        else:",
            "lineNumber": 734
          },
          {
            "text": "            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)",
            "lineNumber": 735
          },
          {
            "lineNumber": 736
          },
          {
            "text": "    # 现在使用已存在的 LightRAG 实例初始化 RAGAnything",
            "lineNumber": 737
          }
        ]
      },
      "score": 0.3354547619819641
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README_zh.md",
        "range": {
          "startPosition": {
            "line": 554
          },
          "endPosition": {
            "line": 611,
            "column": 4
          }
        },
        "contents": "class CustomModalProcessor(GenericModalProcessor):\n    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):\n        # 你的自定义处理逻辑\n        enhanced_description = await self.analyze_custom_content(modal_content)\n        entity_info = self.create_custom_entity(enhanced_description, entity_name)\n        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)\n&#96;&#96;&#96;\n\n#### 5. 查询选项\n\nRAG-Anything 提供三种类型的查询方法：\n\n**纯文本查询** - 使用LightRAG直接进行知识库搜索：\n&#96;&#96;&#96;python\n# 文本查询的不同模式\ntext_result_hybrid = await rag.aquery(\"你的问题\", mode=\"hybrid\")\ntext_result_local = await rag.aquery(\"你的问题\", mode=\"local\")\ntext_result_global = await rag.aquery(\"你的问题\", mode=\"global\")\ntext_result_naive = await rag.aquery(\"你的问题\", mode=\"naive\")\n\n# 同步版本\nsync_text_result = rag.query(\"你的问题\", mode=\"hybrid\")\n&#96;&#96;&#96;\n\n**VLM增强查询** - 使用VLM自动分析检索上下文中的图像：\n&#96;&#96;&#96;python\n# VLM增强查询（当提供vision_model_func时自动启用）\nvlm_result = await rag.aquery(\n    \"分析文档中的图表和数据\",\n    mode=\"hybrid\"\n    # vlm_enhanced=True 当vision_model_func可用时自动设置\n)\n\n# 手动控制VLM增强\nvlm_enabled = await rag.aquery(\n    \"这个文档中的图片显示了什么内容？\",\n    mode=\"hybrid\",\n    vlm_enhanced=True  # 强制启用VLM增强\n)\n\nvlm_disabled = await rag.aquery(\n    \"这个文档中的图片显示了什么内容？\",\n    mode=\"hybrid\",\n    vlm_enhanced=False  # 强制禁用VLM增强\n)\n\n# 当文档包含图片时，VLM可以直接查看和分析图片\n# 系统将自动：\n# 1. 检索包含图片路径的相关上下文\n# 2. 加载图片并编码为base64格式\n# 3. 将文本上下文和图片一起发送给VLM进行综合分析\n&#96;&#96;&#96;\n\n**多模态查询** - 包含特定多模态内容分析的增强查询：\n&#96;&#96;&#96;python\n# 包含表格数据的查询\ntable_result = await rag.aquery_with_multimodal(\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "class CustomModalProcessor(GenericModalProcessor):",
            "lineNumber": 555
          },
          {
            "text": "    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):",
            "lineNumber": 556
          },
          {
            "text": "        # 你的自定义处理逻辑",
            "lineNumber": 557
          },
          {
            "text": "        enhanced_description = await self.analyze_custom_content(modal_content)",
            "lineNumber": 558
          },
          {
            "text": "        entity_info = self.create_custom_entity(enhanced_description, entity_name)",
            "lineNumber": 559
          },
          {
            "text": "        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)",
            "lineNumber": 560
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 561
          },
          {
            "lineNumber": 562
          },
          {
            "text": "#### 5. 查询选项",
            "lineNumber": 563
          },
          {
            "lineNumber": 564
          },
          {
            "text": "RAG-Anything 提供三种类型的查询方法：",
            "lineNumber": 565
          },
          {
            "lineNumber": 566
          },
          {
            "text": "**纯文本查询** - 使用LightRAG直接进行知识库搜索：",
            "lineNumber": 567
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 568
          },
          {
            "text": "# 文本查询的不同模式",
            "lineNumber": 569
          },
          {
            "text": "text_result_hybrid = await rag.aquery(\"你的问题\", mode=\"hybrid\")",
            "lineNumber": 570
          },
          {
            "text": "text_result_local = await rag.aquery(\"你的问题\", mode=\"local\")",
            "lineNumber": 571
          },
          {
            "text": "text_result_global = await rag.aquery(\"你的问题\", mode=\"global\")",
            "lineNumber": 572
          },
          {
            "text": "text_result_naive = await rag.aquery(\"你的问题\", mode=\"naive\")",
            "lineNumber": 573
          },
          {
            "lineNumber": 574
          },
          {
            "text": "# 同步版本",
            "lineNumber": 575
          },
          {
            "text": "sync_text_result = rag.query(\"你的问题\", mode=\"hybrid\")",
            "lineNumber": 576
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 577
          },
          {
            "lineNumber": 578
          },
          {
            "text": "**VLM增强查询** - 使用VLM自动分析检索上下文中的图像：",
            "lineNumber": 579
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 580
          },
          {
            "text": "# VLM增强查询（当提供vision_model_func时自动启用）",
            "lineNumber": 581
          },
          {
            "text": "vlm_result = await rag.aquery(",
            "lineNumber": 582
          },
          {
            "text": "    \"分析文档中的图表和数据\",",
            "lineNumber": 583
          },
          {
            "text": "    mode=\"hybrid\"",
            "lineNumber": 584
          },
          {
            "text": "    # vlm_enhanced=True 当vision_model_func可用时自动设置",
            "lineNumber": 585
          },
          {
            "text": ")",
            "lineNumber": 586
          },
          {
            "lineNumber": 587
          },
          {
            "text": "# 手动控制VLM增强",
            "lineNumber": 588
          },
          {
            "text": "vlm_enabled = await rag.aquery(",
            "lineNumber": 589
          },
          {
            "text": "    \"这个文档中的图片显示了什么内容？\",",
            "lineNumber": 590
          },
          {
            "text": "    mode=\"hybrid\",",
            "lineNumber": 591
          },
          {
            "text": "    vlm_enhanced=True  # 强制启用VLM增强",
            "lineNumber": 592
          },
          {
            "text": ")",
            "lineNumber": 593
          },
          {
            "lineNumber": 594
          },
          {
            "text": "vlm_disabled = await rag.aquery(",
            "lineNumber": 595
          },
          {
            "text": "    \"这个文档中的图片显示了什么内容？\",",
            "lineNumber": 596
          },
          {
            "text": "    mode=\"hybrid\",",
            "lineNumber": 597
          },
          {
            "text": "    vlm_enhanced=False  # 强制禁用VLM增强",
            "lineNumber": 598
          },
          {
            "text": ")",
            "lineNumber": 599
          },
          {
            "lineNumber": 600
          },
          {
            "text": "# 当文档包含图片时，VLM可以直接查看和分析图片",
            "lineNumber": 601
          },
          {
            "text": "# 系统将自动：",
            "lineNumber": 602
          },
          {
            "text": "# 1. 检索包含图片路径的相关上下文",
            "lineNumber": 603
          },
          {
            "text": "# 2. 加载图片并编码为base64格式",
            "lineNumber": 604
          },
          {
            "text": "# 3. 将文本上下文和图片一起发送给VLM进行综合分析",
            "lineNumber": 605
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 606
          },
          {
            "lineNumber": 607
          },
          {
            "text": "**多模态查询** - 包含特定多模态内容分析的增强查询：",
            "lineNumber": 608
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 609
          },
          {
            "text": "# 包含表格数据的查询",
            "lineNumber": 610
          },
          {
            "text": "table_result = await rag.aquery_with_multimodal(",
            "lineNumber": 611
          },
          {
            "text": "    ",
            "lineNumber": 612
          }
        ]
      },
      "score": 0.3343772888183594
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {
            "line": 137,
            "column": 60
          },
          "endPosition": {
            "line": 237
          }
        },
        "contents": "async def main(args: argparse.Namespace) -&gt; int:\n    _try_load_dotenv()\n\n    docs = [Path(p).resolve() for p in args.docs]\n    for p in docs:\n        if not p.exists():\n            raise FileNotFoundError(str(p))\n\n    llm_model_func, vision_model_func, embedding_func = _build_llm_vision_embedding()\n\n    cfg = RAGAnythingConfig(\n        working_dir=str(Path(args.working_dir).resolve()),\n        parser=os.getenv(\"PARSER\", \"mineru\"),\n        parse_method=\"auto\",\n        enable_image_processing=True,\n        enable_table_processing=True,\n        enable_equation_processing=True,\n    )\n\n    rag = RAGAnything(\n        config=cfg,\n        llm_model_func=llm_model_func,\n        vision_model_func=vision_model_func,\n        embedding_func=embedding_func,\n    )\n\n    output_dir = Path(args.output_dir).resolve()\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Skip processing if storage exists\n    wd = Path(cfg.working_dir)\n    if not (wd.exists() and any(wd.iterdir())):\n        for doc in docs:\n            await rag.process_document_complete(\n                file_path=str(doc),\n                output_dir=str(output_dir),\n                parse_method=\"auto\",\n            )\n\n    queries = [\n        \"請描述文件/圖片中的主要內容重點。\",\n        \"如果是圖表，請說明它呈現的主題與可能的趨勢。\",\n        \"若畫面有標題或重要文字，請摘要出來。\",\n    ]\n\n    run_dir = Path(args.out_dir).resolve() / \"vlm_enhanced_query\" / _now_tag()\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    results: Dict[str, Any] = {\n        \"meta\":",
        "signatures": {},
        "detailedLines": [
          {
            "text": "async def main(args: argparse.Namespace) -&gt; int:",
            "lineNumber": 141,
            "isSignature": true
          },
          {
            "text": "    _try_load_dotenv()",
            "lineNumber": 142
          },
          {
            "lineNumber": 143
          },
          {
            "text": "    docs = [Path(p).resolve() for p in args.docs]",
            "lineNumber": 144
          },
          {
            "text": "    for p in docs:",
            "lineNumber": 145
          },
          {
            "text": "        if not p.exists():",
            "lineNumber": 146
          },
          {
            "text": "            raise FileNotFoundError(str(p))",
            "lineNumber": 147
          },
          {
            "lineNumber": 148
          },
          {
            "text": "    llm_model_func, vision_model_func, embedding_func = _build_llm_vision_embedding()",
            "lineNumber": 149
          },
          {
            "lineNumber": 150
          },
          {
            "text": "    cfg = RAGAnythingConfig(",
            "lineNumber": 151
          },
          {
            "text": "        working_dir=str(Path(args.working_dir).resolve()),",
            "lineNumber": 152
          },
          {
            "text": "        parser=os.getenv(\"PARSER\", \"mineru\"),",
            "lineNumber": 153
          },
          {
            "text": "        parse_method=\"auto\",",
            "lineNumber": 154
          },
          {
            "text": "        enable_image_processing=True,",
            "lineNumber": 155
          },
          {
            "text": "        enable_table_processing=True,",
            "lineNumber": 156
          },
          {
            "text": "        enable_equation_processing=True,",
            "lineNumber": 157
          },
          {
            "text": "    )",
            "lineNumber": 158
          },
          {
            "lineNumber": 159
          },
          {
            "text": "    rag = RAGAnything(",
            "lineNumber": 160
          },
          {
            "text": "        config=cfg,",
            "lineNumber": 161
          },
          {
            "text": "        llm_model_func=llm_model_func,",
            "lineNumber": 162
          },
          {
            "text": "        vision_model_func=vision_model_func,",
            "lineNumber": 163
          },
          {
            "text": "        embedding_func=embedding_func,",
            "lineNumber": 164
          },
          {
            "text": "    )",
            "lineNumber": 165
          },
          {
            "lineNumber": 166
          },
          {
            "text": "    output_dir = Path(args.output_dir).resolve()",
            "lineNumber": 167
          },
          {
            "text": "    output_dir.mkdir(parents=True, exist_ok=True)",
            "lineNumber": 168
          },
          {
            "lineNumber": 169
          },
          {
            "text": "    # Skip processing if storage exists",
            "lineNumber": 170
          },
          {
            "text": "    wd = Path(cfg.working_dir)",
            "lineNumber": 171
          },
          {
            "text": "    if not (wd.exists() and any(wd.iterdir())):",
            "lineNumber": 172
          },
          {
            "text": "        for doc in docs:",
            "lineNumber": 173
          },
          {
            "text": "            await rag.process_document_complete(",
            "lineNumber": 174
          },
          {
            "text": "                file_path=str(doc),",
            "lineNumber": 175
          },
          {
            "text": "                output_dir=str(output_dir),",
            "lineNumber": 176
          },
          {
            "text": "                parse_method=\"auto\",",
            "lineNumber": 177
          },
          {
            "text": "            )",
            "lineNumber": 178
          },
          {
            "lineNumber": 179
          },
          {
            "text": "    queries = [",
            "lineNumber": 180
          },
          {
            "text": "        \"請描述文件/圖片中的主要內容重點。\",",
            "lineNumber": 181
          },
          {
            "text": "        \"如果是圖表，請說明它呈現的主題與可能的趨勢。\",",
            "lineNumber": 182
          },
          {
            "text": "        \"若畫面有標題或重要文字，請摘要出來。\",",
            "lineNumber": 183
          },
          {
            "text": "    ]",
            "lineNumber": 184
          },
          {
            "lineNumber": 185
          },
          {
            "text": "    run_dir = Path(args.out_dir).resolve() / \"vlm_enhanced_query\" / _now_tag()",
            "lineNumber": 186
          },
          {
            "text": "    run_dir.mkdir(parents=True, exist_ok=True)",
            "lineNumber": 187
          },
          {
            "lineNumber": 188
          },
          {
            "text": "    results: Dict[str, Any] = {",
            "lineNumber": 189
          },
          {
            "text": "        \"meta\":",
            "lineNumber": 190
          }
        ]
      },
      "score": 0.33240941166877747
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="semantic_search_full">
<details>
<summary>Tool use: **semantic_search_full**</summary>


Parameters:

```json
{
  "query": "What happens when no images are found in VLM enhanced query fallback?",
  "topK": 10,
  "explanation": ""
}
```
Result:

```json
{
  "codeResults": [
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def aquery_vlm_enhanced(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:\n\n\n        if not images_found:\n            self.logger.info(\"No valid images found, falling back to normal query\")\n            # Fallback to normal query\n            query_param = QueryParam(mode=mode, **kwargs)\n            return await self.lightrag.aquery(query, param=query_param)\n\n        self.logger.info(f\"Processed {images_found} images for VLM\")\n\n        # 3. Build VLM message format\n        messages = self._build_vlm_messages_with_images(enhanced_prompt, query)\n\n        # 4. Call VLM for question answering\n        result = await self._call_vlm_with_multimodal_content(messages)\n\n        self.logger.info(\"VLM enhanced query completed\")\n        return result\n\n    async def _process_multimodal_query_content(\n        self, base_query: str, multimodal_content: List[Dict[str, Any]]\n    ) -&gt; str:\n        \"\"\"\n        Process multimodal query content to generate enhanced query text\n\n        Args:\n            base_query: Base query text\n            multimodal_content: List of multimodal content\n\n        Returns:\n            str: Enhanced query text\n        \"\"\"\n        self.logger.info(\"Starting multimodal query content processing...\")\n\n        enhanced_parts = [f\"User query: {base_query}\"]\n\n        for i, content in enumerate(multimodal_content):",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 269,
                "column": 5
              },
              "endPosition": {
                "line": 270,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def aquery_vlm_enhanced(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:",
            "lineNumber": 269
          },
          {
            "lineNumber": 310
          },
          {
            "lineNumber": 311
          },
          {
            "text": "        if not images_found:",
            "lineNumber": 312
          },
          {
            "text": "            self.logger.info(\"No valid images found, falling back to normal query\")",
            "lineNumber": 313
          },
          {
            "text": "            # Fallback to normal query",
            "lineNumber": 314
          },
          {
            "text": "            query_param = QueryParam(mode=mode, **kwargs)",
            "lineNumber": 315
          },
          {
            "text": "            return await self.lightrag.aquery(query, param=query_param)",
            "lineNumber": 316
          },
          {
            "lineNumber": 317
          },
          {
            "text": "        self.logger.info(f\"Processed {images_found} images for VLM\")",
            "lineNumber": 318
          },
          {
            "lineNumber": 319
          },
          {
            "text": "        # 3. Build VLM message format",
            "lineNumber": 320
          },
          {
            "text": "        messages = self._build_vlm_messages_with_images(enhanced_prompt, query)",
            "lineNumber": 321
          },
          {
            "lineNumber": 322
          },
          {
            "text": "        # 4. Call VLM for question answering",
            "lineNumber": 323
          },
          {
            "text": "        result = await self._call_vlm_with_multimodal_content(messages)",
            "lineNumber": 324
          },
          {
            "lineNumber": 325
          },
          {
            "text": "        self.logger.info(\"VLM enhanced query completed\")",
            "lineNumber": 326
          },
          {
            "text": "        return result",
            "lineNumber": 327
          },
          {
            "lineNumber": 328
          },
          {
            "text": "    async def _process_multimodal_query_content(",
            "lineNumber": 329
          },
          {
            "text": "        self, base_query: str, multimodal_content: List[Dict[str, Any]]",
            "lineNumber": 330
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 331
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 332
          },
          {
            "text": "        Process multimodal query content to generate enhanced query text",
            "lineNumber": 333
          },
          {
            "lineNumber": 334
          },
          {
            "text": "        Args:",
            "lineNumber": 335
          },
          {
            "text": "            base_query: Base query text",
            "lineNumber": 336
          },
          {
            "text": "            multimodal_content: List of multimodal content",
            "lineNumber": 337
          },
          {
            "lineNumber": 338
          },
          {
            "text": "        Returns:",
            "lineNumber": 339
          },
          {
            "text": "            str: Enhanced query text",
            "lineNumber": 340
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 341
          },
          {
            "text": "        self.logger.info(\"Starting multimodal query content processing...\")",
            "lineNumber": 342
          },
          {
            "lineNumber": 343
          },
          {
            "text": "        enhanced_parts = [f\"User query: {base_query}\"]",
            "lineNumber": 344
          },
          {
            "lineNumber": 345
          },
          {
            "text": "        for i, content in enumerate(multimodal_content):",
            "lineNumber": 346
          }
        ]
      },
      "score": 0.4679943323135376
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\n\n\n    async def aquery_vlm_enhanced(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:\n        \"\"\"\n        VLM enhanced query - replaces image paths in retrieved context with base64 encoded images for VLM processing\n\n        Args:\n            query: User query\n            mode: Underlying LightRAG query mode\n            **kwargs: Other query parameters\n\n        Returns:\n            str: VLM query result\n        \"\"\"\n        # Ensure VLM is available\n        if not hasattr(self, \"vision_model_func\") or not self.vision_model_func:\n            raise ValueError(\n                \"VLM enhanced query requires vision_model_func. \"\n                \"Please provide a vision model function when initializing RAGAnything.\"\n            )\n\n        # Ensure LightRAG is initialized\n        await self._ensure_lightrag_initialized()\n\n        self.logger.info(f\"Executing VLM enhanced query: {query[:100]}...\")\n\n        # Clear previous image cache\n        if hasattr(self, \"_current_images_base64\"):\n            delattr(self, \"_current_images_base64\")\n\n        # 1. Get original retrieval prompt (without generating final answer)\n        query_param = QueryParam(mode=mode, only_need_prompt=True, **kwargs)\n        raw_prompt = await self.lightrag.aquery(query, param=query_param)\n\n        self.logger.debug(\"Retrieved raw prompt from LightRAG\")\n\n        # Check if raw_prompt is None or empty\n        if not raw_prompt or not isinstance(raw_prompt, str):\n            self.logger.warning(\"Raw prompt is None or not a string, falling back to normal query\")\n            query_param = QueryParam(mode=mode, **kwargs)\n            return await self.lightrag.aquery(query, param=query_param)\n\n        # 2. Extract and process image paths\n        enhanced_prompt, images_found = await self._process_image_paths_for_vlm(raw_prompt)\n\n        if not images_found:\n            self.logger.info",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "lineNumber": 267
          },
          {
            "lineNumber": 268
          },
          {
            "text": "    async def aquery_vlm_enhanced(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:",
            "lineNumber": 269
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 270
          },
          {
            "text": "        VLM enhanced query - replaces image paths in retrieved context with base64 encoded images for VLM processing",
            "lineNumber": 271
          },
          {
            "lineNumber": 272
          },
          {
            "text": "        Args:",
            "lineNumber": 273
          },
          {
            "text": "            query: User query",
            "lineNumber": 274
          },
          {
            "text": "            mode: Underlying LightRAG query mode",
            "lineNumber": 275
          },
          {
            "text": "            **kwargs: Other query parameters",
            "lineNumber": 276
          },
          {
            "lineNumber": 277
          },
          {
            "text": "        Returns:",
            "lineNumber": 278
          },
          {
            "text": "            str: VLM query result",
            "lineNumber": 279
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 280
          },
          {
            "text": "        # Ensure VLM is available",
            "lineNumber": 281
          },
          {
            "text": "        if not hasattr(self, \"vision_model_func\") or not self.vision_model_func:",
            "lineNumber": 282
          },
          {
            "text": "            raise ValueError(",
            "lineNumber": 283
          },
          {
            "text": "                \"VLM enhanced query requires vision_model_func. \"",
            "lineNumber": 284
          },
          {
            "text": "                \"Please provide a vision model function when initializing RAGAnything.\"",
            "lineNumber": 285
          },
          {
            "text": "            )",
            "lineNumber": 286
          },
          {
            "lineNumber": 287
          },
          {
            "text": "        # Ensure LightRAG is initialized",
            "lineNumber": 288
          },
          {
            "text": "        await self._ensure_lightrag_initialized()",
            "lineNumber": 289
          },
          {
            "lineNumber": 290
          },
          {
            "text": "        self.logger.info(f\"Executing VLM enhanced query: {query[:100]}...\")",
            "lineNumber": 291
          },
          {
            "lineNumber": 292
          },
          {
            "text": "        # Clear previous image cache",
            "lineNumber": 293
          },
          {
            "text": "        if hasattr(self, \"_current_images_base64\"):",
            "lineNumber": 294
          },
          {
            "text": "            delattr(self, \"_current_images_base64\")",
            "lineNumber": 295
          },
          {
            "lineNumber": 296
          },
          {
            "text": "        # 1. Get original retrieval prompt (without generating final answer)",
            "lineNumber": 297
          },
          {
            "text": "        query_param = QueryParam(mode=mode, only_need_prompt=True, **kwargs)",
            "lineNumber": 298
          },
          {
            "text": "        raw_prompt = await self.lightrag.aquery(query, param=query_param)",
            "lineNumber": 299
          },
          {
            "lineNumber": 300
          },
          {
            "text": "        self.logger.debug(\"Retrieved raw prompt from LightRAG\")",
            "lineNumber": 301
          },
          {
            "lineNumber": 302
          },
          {
            "text": "        # Check if raw_prompt is None or empty",
            "lineNumber": 303
          },
          {
            "text": "        if not raw_prompt or not isinstance(raw_prompt, str):",
            "lineNumber": 304
          },
          {
            "text": "            self.logger.warning(\"Raw prompt is None or not a string, falling back to normal query\")",
            "lineNumber": 305
          },
          {
            "text": "            query_param = QueryParam(mode=mode, **kwargs)",
            "lineNumber": 306
          },
          {
            "text": "            return await self.lightrag.aquery(query, param=query_param)",
            "lineNumber": 307
          },
          {
            "lineNumber": 308
          },
          {
            "text": "        # 2. Extract and process image paths",
            "lineNumber": 309
          },
          {
            "text": "        enhanced_prompt, images_found = await self._process_image_paths_for_vlm(raw_prompt)",
            "lineNumber": 310
          },
          {
            "lineNumber": 311
          },
          {
            "text": "        if not images_found:",
            "lineNumber": 312
          },
          {
            "text": "            self.logger.info",
            "lineNumber": 313
          }
        ]
      },
      "score": 0.46476760506629944
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\ndef _generate_multimodal_cache_key(\n        self, query: str, multimodal_content: List[Dict[str, Any]], mode: str, **kwargs\n    ) -&gt; str:\n\n\n        return f\"multimodal_query:{cache_hash}\"\n\n    async def aquery(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:\n        \"\"\"\n        Pure text query - directly calls LightRAG's query functionality\n\n        Args:\n            query: Query text\n            mode: Query mode (\"local\", \"global\", \"hybrid\", \"naive\", \"mix\", \"bypass\")\n            **kwargs: Other query parameters, will be passed to QueryParam\n                - vlm_enhanced: bool, default True when vision_model_func is available.\n                  If True, will parse image paths in retrieved context and replace them\n                  with base64 encoded images for VLM processing.\n\n        Returns:\n            str: Query result\n        \"\"\"\n        if self.lightrag is None:\n            raise ValueError(\n                \"No LightRAG instance available. Please process documents first or provide a pre-initialized LightRAG instance.\"\n            )\n\n        # Check if VLM enhanced query should be used\n        vlm_enhanced = kwargs.pop(\"vlm_enhanced\", None)\n\n        # Auto-determine VLM enhanced based on availability\n        if vlm_enhanced is None:\n            vlm_enhanced = hasattr(self, \"vision_model_func\") and self.vision_model_func is not None\n\n        # Use VLM enhanced query if enabled and available\n        if vlm_enhanced and hasattr(self, \"vision_model_func\") and self.vision_model_func:\n            return await self.aquery_vlm_enhanced(query, mode=mode, **kwargs)\n        elif vlm_enhanced and (\n            not hasattr(self, \"vision_model_func\") or not self.vision_model_func\n        ):\n            self.logger.warning(\n                \"VLM enhanced query requested but vision_model_func is not available, falling back to normal query\"\n            )\n\n        # Create query parameters\n        query_param = QueryParam(mode=mode, **kwargs)\n\n        self.logger.info(f\"Executing text query: {query[:100]}...\")\n        self.logger.info(f\"Query mode: {mode}\")\n\n        # Call LightRAG's query method\n        result = await self.lightrag.aquery(query, param=query_param)",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 25,
                "column": 5
              },
              "endPosition": {
                "line": 28,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "def _generate_multimodal_cache_key(",
            "lineNumber": 25
          },
          {
            "text": "        self, query: str, multimodal_content: List[Dict[str, Any]], mode: str, **kwargs",
            "lineNumber": 26
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 27
          },
          {
            "lineNumber": 94
          },
          {
            "lineNumber": 95
          },
          {
            "text": "        return f\"multimodal_query:{cache_hash}\"",
            "lineNumber": 96
          },
          {
            "lineNumber": 97
          },
          {
            "text": "    async def aquery(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:",
            "lineNumber": 98
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 99
          },
          {
            "text": "        Pure text query - directly calls LightRAG's query functionality",
            "lineNumber": 100
          },
          {
            "lineNumber": 101
          },
          {
            "text": "        Args:",
            "lineNumber": 102
          },
          {
            "text": "            query: Query text",
            "lineNumber": 103
          },
          {
            "text": "            mode: Query mode (\"local\", \"global\", \"hybrid\", \"naive\", \"mix\", \"bypass\")",
            "lineNumber": 104
          },
          {
            "text": "            **kwargs: Other query parameters, will be passed to QueryParam",
            "lineNumber": 105
          },
          {
            "text": "                - vlm_enhanced: bool, default True when vision_model_func is available.",
            "lineNumber": 106
          },
          {
            "text": "                  If True, will parse image paths in retrieved context and replace them",
            "lineNumber": 107
          },
          {
            "text": "                  with base64 encoded images for VLM processing.",
            "lineNumber": 108
          },
          {
            "lineNumber": 109
          },
          {
            "text": "        Returns:",
            "lineNumber": 110
          },
          {
            "text": "            str: Query result",
            "lineNumber": 111
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 112
          },
          {
            "text": "        if self.lightrag is None:",
            "lineNumber": 113
          },
          {
            "text": "            raise ValueError(",
            "lineNumber": 114
          },
          {
            "text": "                \"No LightRAG instance available. Please process documents first or provide a pre-initialized LightRAG instance.\"",
            "lineNumber": 115
          },
          {
            "text": "            )",
            "lineNumber": 116
          },
          {
            "lineNumber": 117
          },
          {
            "text": "        # Check if VLM enhanced query should be used",
            "lineNumber": 118
          },
          {
            "text": "        vlm_enhanced = kwargs.pop(\"vlm_enhanced\", None)",
            "lineNumber": 119
          },
          {
            "lineNumber": 120
          },
          {
            "text": "        # Auto-determine VLM enhanced based on availability",
            "lineNumber": 121
          },
          {
            "text": "        if vlm_enhanced is None:",
            "lineNumber": 122
          },
          {
            "text": "            vlm_enhanced = hasattr(self, \"vision_model_func\") and self.vision_model_func is not None",
            "lineNumber": 123
          },
          {
            "lineNumber": 124
          },
          {
            "text": "        # Use VLM enhanced query if enabled and available",
            "lineNumber": 125
          },
          {
            "text": "        if vlm_enhanced and hasattr(self, \"vision_model_func\") and self.vision_model_func:",
            "lineNumber": 126
          },
          {
            "text": "            return await self.aquery_vlm_enhanced(query, mode=mode, **kwargs)",
            "lineNumber": 127
          },
          {
            "text": "        elif vlm_enhanced and (",
            "lineNumber": 128
          },
          {
            "text": "            not hasattr(self, \"vision_model_func\") or not self.vision_model_func",
            "lineNumber": 129
          },
          {
            "text": "        ):",
            "lineNumber": 130
          },
          {
            "text": "            self.logger.warning(",
            "lineNumber": 131
          },
          {
            "text": "                \"VLM enhanced query requested but vision_model_func is not available, falling back to normal query\"",
            "lineNumber": 132
          },
          {
            "text": "            )",
            "lineNumber": 133
          },
          {
            "lineNumber": 134
          },
          {
            "text": "        # Create query parameters",
            "lineNumber": 135
          },
          {
            "text": "        query_param = QueryParam(mode=mode, **kwargs)",
            "lineNumber": 136
          },
          {
            "lineNumber": 137
          },
          {
            "text": "        self.logger.info(f\"Executing text query: {query[:100]}...\")",
            "lineNumber": 138
          },
          {
            "text": "        self.logger.info(f\"Query mode: {mode}\")",
            "lineNumber": 139
          },
          {
            "lineNumber": 140
          },
          {
            "text": "        # Call LightRAG's query method",
            "lineNumber": 141
          },
          {
            "text": "        result = await self.lightrag.aquery(query, param=query_param)",
            "lineNumber": 142
          }
        ]
      },
      "score": 0.4057079553604126
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\ndef _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:\n\n        ]\n\n    async def _call_vlm_with_multimodal_content(self, messages: List[Dict]) -&gt; str:\n        \"\"\"\n        Call VLM to process multimodal content\n\n        Args:\n            messages: VLM message format\n\n        Returns:\n            str: VLM response result\n        \"\"\"\n        try:\n            user_message = messages[1]\n            content = user_message[\"content\"]\n            system_prompt = messages[0][\"content\"]\n\n            if isinstance(content, str):\n                # Pure text mode\n                result = await self.vision_model_func(content, system_prompt=system_prompt)\n            else:\n                # Multimodal mode - pass complete messages directly to VLM\n                result = await self.vision_model_func(\n                    \"\",  # Empty prompt since we're using messages format\n                    messages=messages,\n                )\n\n            return result\n\n        except Exception as e:\n            self.logger.error(f\"VLM call failed: {e}\")\n            raise\n\n    # Synchronous versions of query methods\n    def query(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:\n        \"\"\"\n        Synchronous version of pure text query\n\n        Args:\n            query: Query text\n            mode: Query mode (\"local\", \"global\", \"hybrid\", \"naive\", \"mix\", \"bypass\")\n            **kwargs: Other query parameters, will be passed to QueryParam\n                - vlm_enhanced: bool, default True when vision_model_func is available.\n                  If True, will parse image paths in retrieved context and replace them\n                  with base64 encoded images for VLM processing.\n\n        Returns:\n            str: Query result\n        \"\"\"\n        loop = always_get_an_event_loop()\n        return loop.run_until_complete(self.aquery(query, mode=mode, **kwargs))\n\n    def query_with_multimodal(\n        self,\n        query: str,\n        multimodal_content: List[Dict[str, Any]] = None,\n        mode: str = \"mix\",\n        **kwargs,\n    ) -&gt; str:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:",
            "lineNumber": 559
          },
          {
            "lineNumber": 627
          },
          {
            "text": "        ]",
            "lineNumber": 628
          },
          {
            "lineNumber": 629
          },
          {
            "text": "    async def _call_vlm_with_multimodal_content(self, messages: List[Dict]) -&gt; str:",
            "lineNumber": 630
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 631
          },
          {
            "text": "        Call VLM to process multimodal content",
            "lineNumber": 632
          },
          {
            "lineNumber": 633
          },
          {
            "text": "        Args:",
            "lineNumber": 634
          },
          {
            "text": "            messages: VLM message format",
            "lineNumber": 635
          },
          {
            "lineNumber": 636
          },
          {
            "text": "        Returns:",
            "lineNumber": 637
          },
          {
            "text": "            str: VLM response result",
            "lineNumber": 638
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 639
          },
          {
            "text": "        try:",
            "lineNumber": 640
          },
          {
            "text": "            user_message = messages[1]",
            "lineNumber": 641
          },
          {
            "text": "            content = user_message[\"content\"]",
            "lineNumber": 642
          },
          {
            "text": "            system_prompt = messages[0][\"content\"]",
            "lineNumber": 643
          },
          {
            "lineNumber": 644
          },
          {
            "text": "            if isinstance(content, str):",
            "lineNumber": 645
          },
          {
            "text": "                # Pure text mode",
            "lineNumber": 646
          },
          {
            "text": "                result = await self.vision_model_func(content, system_prompt=system_prompt)",
            "lineNumber": 647
          },
          {
            "text": "            else:",
            "lineNumber": 648
          },
          {
            "text": "                # Multimodal mode - pass complete messages directly to VLM",
            "lineNumber": 649
          },
          {
            "text": "                result = await self.vision_model_func(",
            "lineNumber": 650
          },
          {
            "text": "                    \"\",  # Empty prompt since we're using messages format",
            "lineNumber": 651
          },
          {
            "text": "                    messages=messages,",
            "lineNumber": 652
          },
          {
            "text": "                )",
            "lineNumber": 653
          },
          {
            "lineNumber": 654
          },
          {
            "text": "            return result",
            "lineNumber": 655
          },
          {
            "lineNumber": 656
          },
          {
            "text": "        except Exception as e:",
            "lineNumber": 657
          },
          {
            "text": "            self.logger.error(f\"VLM call failed: {e}\")",
            "lineNumber": 658
          },
          {
            "text": "            raise",
            "lineNumber": 659
          },
          {
            "lineNumber": 660
          },
          {
            "text": "    # Synchronous versions of query methods",
            "lineNumber": 661
          },
          {
            "text": "    def query(self, query: str, mode: str = \"mix\", **kwargs) -&gt; str:",
            "lineNumber": 662
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 663
          },
          {
            "text": "        Synchronous version of pure text query",
            "lineNumber": 664
          },
          {
            "lineNumber": 665
          },
          {
            "text": "        Args:",
            "lineNumber": 666
          },
          {
            "text": "            query: Query text",
            "lineNumber": 667
          },
          {
            "text": "            mode: Query mode (\"local\", \"global\", \"hybrid\", \"naive\", \"mix\", \"bypass\")",
            "lineNumber": 668
          },
          {
            "text": "            **kwargs: Other query parameters, will be passed to QueryParam",
            "lineNumber": 669
          },
          {
            "text": "                - vlm_enhanced: bool, default True when vision_model_func is available.",
            "lineNumber": 670
          },
          {
            "text": "                  If True, will parse image paths in retrieved context and replace them",
            "lineNumber": 671
          },
          {
            "text": "                  with base64 encoded images for VLM processing.",
            "lineNumber": 672
          },
          {
            "lineNumber": 673
          },
          {
            "text": "        Returns:",
            "lineNumber": 674
          },
          {
            "text": "            str: Query result",
            "lineNumber": 675
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 676
          },
          {
            "text": "        loop = always_get_an_event_loop()",
            "lineNumber": 677
          },
          {
            "text": "        return loop.run_until_complete(self.aquery(query, mode=mode, **kwargs))",
            "lineNumber": 678
          },
          {
            "lineNumber": 679
          },
          {
            "text": "    def query_with_multimodal(",
            "lineNumber": 680
          },
          {
            "text": "        self,",
            "lineNumber": 681
          },
          {
            "text": "        query: str,",
            "lineNumber": 682
          },
          {
            "text": "        multimodal_content: List[Dict[str, Any]] = None,",
            "lineNumber": 683
          },
          {
            "text": "        mode: str = \"mix\",",
            "lineNumber": 684
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 685
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 686
          },
          {
            "text": "        ",
            "lineNumber": 687
          }
        ]
      },
      "score": 0.3709245026111603
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\n\n\n    def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:\n        \"\"\"\n        Build VLM message format, using markers to correspond images with text positions\n\n        Args:\n            enhanced_prompt: Enhanced prompt with image markers\n            user_query: User query\n\n        Returns:\n            List[Dict]: VLM message format\n        \"\"\"\n        images_base64 = getattr(self, \"_current_images_base64\", [])\n\n        if not images_base64:\n            # Pure text mode\n            return [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Context:\\n{enhanced_prompt}\\n\\nUser Question: {user_query}\",\n                }\n            ]\n\n        # Build multimodal content\n        content_parts = []\n\n        # Split text at image markers and insert images\n        text_parts = enhanced_prompt.split(\"[VLM_IMAGE_\")\n\n        for i, text_part in enumerate(text_parts):",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "lineNumber": 557
          },
          {
            "lineNumber": 558
          },
          {
            "text": "    def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:",
            "lineNumber": 559
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 560
          },
          {
            "text": "        Build VLM message format, using markers to correspond images with text positions",
            "lineNumber": 561
          },
          {
            "lineNumber": 562
          },
          {
            "text": "        Args:",
            "lineNumber": 563
          },
          {
            "text": "            enhanced_prompt: Enhanced prompt with image markers",
            "lineNumber": 564
          },
          {
            "text": "            user_query: User query",
            "lineNumber": 565
          },
          {
            "lineNumber": 566
          },
          {
            "text": "        Returns:",
            "lineNumber": 567
          },
          {
            "text": "            List[Dict]: VLM message format",
            "lineNumber": 568
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 569
          },
          {
            "text": "        images_base64 = getattr(self, \"_current_images_base64\", [])",
            "lineNumber": 570
          },
          {
            "lineNumber": 571
          },
          {
            "text": "        if not images_base64:",
            "lineNumber": 572
          },
          {
            "text": "            # Pure text mode",
            "lineNumber": 573
          },
          {
            "text": "            return [",
            "lineNumber": 574
          },
          {
            "text": "                {",
            "lineNumber": 575
          },
          {
            "text": "                    \"role\": \"user\",",
            "lineNumber": 576
          },
          {
            "text": "                    \"content\": f\"Context:\\n{enhanced_prompt}\\n\\nUser Question: {user_query}\",",
            "lineNumber": 577
          },
          {
            "text": "                }",
            "lineNumber": 578
          },
          {
            "text": "            ]",
            "lineNumber": 579
          },
          {
            "lineNumber": 580
          },
          {
            "text": "        # Build multimodal content",
            "lineNumber": 581
          },
          {
            "text": "        content_parts = []",
            "lineNumber": 582
          },
          {
            "lineNumber": 583
          },
          {
            "text": "        # Split text at image markers and insert images",
            "lineNumber": 584
          },
          {
            "text": "        text_parts = enhanced_prompt.split(\"[VLM_IMAGE_\")",
            "lineNumber": 585
          },
          {
            "lineNumber": 586
          },
          {
            "text": "        for i, text_part in enumerate(text_parts):",
            "lineNumber": 587
          }
        ]
      },
      "score": 0.3589808940887451
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def _describe_generic_for_query(\n        self, processor, content: Dict[str, Any], content_type: str\n    ) -&gt; str:\n\n        )\n\n        return description\n\n    async def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:\n        \"\"\"\n        Process image paths in prompt, keeping original paths and adding VLM markers\n\n        Args:\n            prompt: Original prompt\n\n        Returns:\n            tuple: (processed prompt, image count)\n        \"\"\"\n        # Validate prompt\n        if not prompt or not isinstance(prompt, str):\n            self.logger.warning(\"Invalid prompt provided to _process_image_paths_for_vlm\")\n            return \"\", 0\n\n        enhanced_prompt = prompt\n        images_processed = 0\n\n        # Initialize image cache\n        self._current_images_base64 = []\n\n        # Enhanced regex pattern for matching image paths\n        # Matches only the path ending with image file extensions\n        image_path_pattern = r\"Image Path:\\s*([^\\r\\n]*?\\.(?:jpg|jpeg|png|gif|bmp|webp|tiff|tif))\"\n\n        # First, let's see what matches we find\n        matches = re.findall(image_path_pattern, prompt)\n        self.logger.info(f\"Found {len(matches)} image path matches in prompt\")\n\n        def replace_image_path(match):\n            nonlocal images_processed",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def _describe_generic_for_query(",
            "lineNumber": 465
          },
          {
            "text": "        self, processor, content: Dict[str, Any], content_type: str",
            "lineNumber": 466
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 467
          },
          {
            "lineNumber": 477
          },
          {
            "text": "        )",
            "lineNumber": 478
          },
          {
            "lineNumber": 479
          },
          {
            "text": "        return description",
            "lineNumber": 480
          },
          {
            "lineNumber": 481
          },
          {
            "text": "    async def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:",
            "lineNumber": 482
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 483
          },
          {
            "text": "        Process image paths in prompt, keeping original paths and adding VLM markers",
            "lineNumber": 484
          },
          {
            "lineNumber": 485
          },
          {
            "text": "        Args:",
            "lineNumber": 486
          },
          {
            "text": "            prompt: Original prompt",
            "lineNumber": 487
          },
          {
            "lineNumber": 488
          },
          {
            "text": "        Returns:",
            "lineNumber": 489
          },
          {
            "text": "            tuple: (processed prompt, image count)",
            "lineNumber": 490
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 491
          },
          {
            "text": "        # Validate prompt",
            "lineNumber": 492
          },
          {
            "text": "        if not prompt or not isinstance(prompt, str):",
            "lineNumber": 493
          },
          {
            "text": "            self.logger.warning(\"Invalid prompt provided to _process_image_paths_for_vlm\")",
            "lineNumber": 494
          },
          {
            "text": "            return \"\", 0",
            "lineNumber": 495
          },
          {
            "lineNumber": 496
          },
          {
            "text": "        enhanced_prompt = prompt",
            "lineNumber": 497
          },
          {
            "text": "        images_processed = 0",
            "lineNumber": 498
          },
          {
            "lineNumber": 499
          },
          {
            "text": "        # Initialize image cache",
            "lineNumber": 500
          },
          {
            "text": "        self._current_images_base64 = []",
            "lineNumber": 501
          },
          {
            "lineNumber": 502
          },
          {
            "text": "        # Enhanced regex pattern for matching image paths",
            "lineNumber": 503
          },
          {
            "text": "        # Matches only the path ending with image file extensions",
            "lineNumber": 504
          },
          {
            "text": "        image_path_pattern = r\"Image Path:\\s*([^\\r\\n]*?\\.(?:jpg|jpeg|png|gif|bmp|webp|tiff|tif))\"",
            "lineNumber": 505
          },
          {
            "lineNumber": 506
          },
          {
            "text": "        # First, let's see what matches we find",
            "lineNumber": 507
          },
          {
            "text": "        matches = re.findall(image_path_pattern, prompt)",
            "lineNumber": 508
          },
          {
            "text": "        self.logger.info(f\"Found {len(matches)} image path matches in prompt\")",
            "lineNumber": 509
          },
          {
            "lineNumber": 510
          },
          {
            "text": "        def replace_image_path(match):",
            "lineNumber": 511
          },
          {
            "text": "            nonlocal images_processed",
            "lineNumber": 512
          }
        ]
      },
      "score": 0.3361835479736328
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:\ndef replace_image_path(match):\n  # Keep original if validation fails\n\n            try:\n                # Encode image to base64 using utility function\n                self.logger.debug(f\"Attempting to encode image: {image_path}\")\n                image_base64 = encode_image_to_base64(image_path)\n                if image_base64:\n                    images_processed += 1\n                    # Save base64 to instance variable for later use\n                    self._current_images_base64.append(image_base64)\n\n                    # Keep original path info and add VLM marker\n                    result = f\"Image Path: {image_path}\\n[VLM_IMAGE_{images_processed}]\"\n                    self.logger.debug(\n                        f\"Successfully processed image {images_processed}: {image_path}\"\n                    )\n                    return result\n                else:\n                    self.logger.error(f\"Failed to encode image: {image_path}\")\n                    return match.group(0)  # Keep original if encoding failed\n\n            except Exception as e:\n                self.logger.error(f\"Failed to process image {image_path}: {e}\")\n                return match.group(0)  # Keep original\n\n        # Execute replacement\n        enhanced_prompt = re.sub(image_path_pattern, replace_image_path, enhanced_prompt)\n\n        return enhanced_prompt, images_processed\n\n    def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 482,
                "column": 5
              },
              "endPosition": {
                "line": 483,
                "column": 9
              }
            },
            {
              "startPosition": {
                "line": 511,
                "column": 9
              },
              "endPosition": {
                "line": 512,
                "column": 13
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:",
            "lineNumber": 482
          },
          {
            "text": "def replace_image_path(match):",
            "lineNumber": 511
          },
          {
            "text": "  # Keep original if validation fails",
            "lineNumber": 529
          },
          {
            "lineNumber": 530
          },
          {
            "text": "            try:",
            "lineNumber": 531
          },
          {
            "text": "                # Encode image to base64 using utility function",
            "lineNumber": 532
          },
          {
            "text": "                self.logger.debug(f\"Attempting to encode image: {image_path}\")",
            "lineNumber": 533
          },
          {
            "text": "                image_base64 = encode_image_to_base64(image_path)",
            "lineNumber": 534
          },
          {
            "text": "                if image_base64:",
            "lineNumber": 535
          },
          {
            "text": "                    images_processed += 1",
            "lineNumber": 536
          },
          {
            "text": "                    # Save base64 to instance variable for later use",
            "lineNumber": 537
          },
          {
            "text": "                    self._current_images_base64.append(image_base64)",
            "lineNumber": 538
          },
          {
            "lineNumber": 539
          },
          {
            "text": "                    # Keep original path info and add VLM marker",
            "lineNumber": 540
          },
          {
            "text": "                    result = f\"Image Path: {image_path}\\n[VLM_IMAGE_{images_processed}]\"",
            "lineNumber": 541
          },
          {
            "text": "                    self.logger.debug(",
            "lineNumber": 542
          },
          {
            "text": "                        f\"Successfully processed image {images_processed}: {image_path}\"",
            "lineNumber": 543
          },
          {
            "text": "                    )",
            "lineNumber": 544
          },
          {
            "text": "                    return result",
            "lineNumber": 545
          },
          {
            "text": "                else:",
            "lineNumber": 546
          },
          {
            "text": "                    self.logger.error(f\"Failed to encode image: {image_path}\")",
            "lineNumber": 547
          },
          {
            "text": "                    return match.group(0)  # Keep original if encoding failed",
            "lineNumber": 548
          },
          {
            "lineNumber": 549
          },
          {
            "text": "            except Exception as e:",
            "lineNumber": 550
          },
          {
            "text": "                self.logger.error(f\"Failed to process image {image_path}: {e}\")",
            "lineNumber": 551
          },
          {
            "text": "                return match.group(0)  # Keep original",
            "lineNumber": 552
          },
          {
            "lineNumber": 553
          },
          {
            "text": "        # Execute replacement",
            "lineNumber": 554
          },
          {
            "text": "        enhanced_prompt = re.sub(image_path_pattern, replace_image_path, enhanced_prompt)",
            "lineNumber": 555
          },
          {
            "lineNumber": 556
          },
          {
            "text": "        return enhanced_prompt, images_processed",
            "lineNumber": 557
          },
          {
            "lineNumber": 558
          },
          {
            "text": "    def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:",
            "lineNumber": 559
          },
          {
            "text": "        ",
            "lineNumber": 560
          }
        ]
      },
      "score": 0.33432286977767944
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\ndef _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:\n\n\n        for i, text_part in enumerate(text_parts):\n            if i == 0:\n                # First text part\n                if text_part.strip():\n                    content_parts.append({\"type\": \"text\", \"text\": text_part})\n            else:\n                # Find marker number and insert corresponding image\n                marker_match = re.match(r\"(\\d+)\\](.*)\", text_part, re.DOTALL)\n                if marker_match:\n                    image_num = int(marker_match.group(1)) - 1  # Convert to 0-based index\n                    remaining_text = marker_match.group(2)\n\n                    # Insert corresponding image\n                    if 0 &lt;= image_num &lt; len(images_base64):\n                        content_parts.append(\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{images_base64[image_num]}\"\n                                },\n                            }\n                        )\n\n                    # Insert remaining text\n                    if remaining_text.strip():\n                        content_parts.append({\"type\": \"text\", \"text\": remaining_text})\n\n        # Add user question\n        content_parts.append(\n            {\n                \"type\": \"text\",\n                \"text\": f\"\\n\\nUser Question: {user_query}\\n\\nPlease answer based on the context and images provided.\",\n            }\n        )\n\n        return [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that can analyze both text and image content to provide comprehensive answers.\",\n            },\n            {\"role\": \"user\", \"content\": content_parts},\n        ]\n\n    async def _call_vlm_with_multimodal_content(self, messages: List[Dict]) -&gt; str:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 559,
                "column": 5
              },
              "endPosition": {
                "line": 560,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "def _build_vlm_messages_with_images(self, enhanced_prompt: str, user_query: str) -&gt; List[Dict]:",
            "lineNumber": 559
          },
          {
            "lineNumber": 585
          },
          {
            "lineNumber": 586
          },
          {
            "text": "        for i, text_part in enumerate(text_parts):",
            "lineNumber": 587
          },
          {
            "text": "            if i == 0:",
            "lineNumber": 588
          },
          {
            "text": "                # First text part",
            "lineNumber": 589
          },
          {
            "text": "                if text_part.strip():",
            "lineNumber": 590
          },
          {
            "text": "                    content_parts.append({\"type\": \"text\", \"text\": text_part})",
            "lineNumber": 591
          },
          {
            "text": "            else:",
            "lineNumber": 592
          },
          {
            "text": "                # Find marker number and insert corresponding image",
            "lineNumber": 593
          },
          {
            "text": "                marker_match = re.match(r\"(\\d+)\\](.*)\", text_part, re.DOTALL)",
            "lineNumber": 594
          },
          {
            "text": "                if marker_match:",
            "lineNumber": 595
          },
          {
            "text": "                    image_num = int(marker_match.group(1)) - 1  # Convert to 0-based index",
            "lineNumber": 596
          },
          {
            "text": "                    remaining_text = marker_match.group(2)",
            "lineNumber": 597
          },
          {
            "lineNumber": 598
          },
          {
            "text": "                    # Insert corresponding image",
            "lineNumber": 599
          },
          {
            "text": "                    if 0 &lt;= image_num &lt; len(images_base64):",
            "lineNumber": 600
          },
          {
            "text": "                        content_parts.append(",
            "lineNumber": 601
          },
          {
            "text": "                            {",
            "lineNumber": 602
          },
          {
            "text": "                                \"type\": \"image_url\",",
            "lineNumber": 603
          },
          {
            "text": "                                \"image_url\": {",
            "lineNumber": 604
          },
          {
            "text": "                                    \"url\": f\"data:image/jpeg;base64,{images_base64[image_num]}\"",
            "lineNumber": 605
          },
          {
            "text": "                                },",
            "lineNumber": 606
          },
          {
            "text": "                            }",
            "lineNumber": 607
          },
          {
            "text": "                        )",
            "lineNumber": 608
          },
          {
            "lineNumber": 609
          },
          {
            "text": "                    # Insert remaining text",
            "lineNumber": 610
          },
          {
            "text": "                    if remaining_text.strip():",
            "lineNumber": 611
          },
          {
            "text": "                        content_parts.append({\"type\": \"text\", \"text\": remaining_text})",
            "lineNumber": 612
          },
          {
            "lineNumber": 613
          },
          {
            "text": "        # Add user question",
            "lineNumber": 614
          },
          {
            "text": "        content_parts.append(",
            "lineNumber": 615
          },
          {
            "text": "            {",
            "lineNumber": 616
          },
          {
            "text": "                \"type\": \"text\",",
            "lineNumber": 617
          },
          {
            "text": "                \"text\": f\"\\n\\nUser Question: {user_query}\\n\\nPlease answer based on the context and images provided.\",",
            "lineNumber": 618
          },
          {
            "text": "            }",
            "lineNumber": 619
          },
          {
            "text": "        )",
            "lineNumber": 620
          },
          {
            "lineNumber": 621
          },
          {
            "text": "        return [",
            "lineNumber": 622
          },
          {
            "text": "            {",
            "lineNumber": 623
          },
          {
            "text": "                \"role\": \"system\",",
            "lineNumber": 624
          },
          {
            "text": "                \"content\": \"You are a helpful assistant that can analyze both text and image content to provide comprehensive answers.\",",
            "lineNumber": 625
          },
          {
            "text": "            },",
            "lineNumber": 626
          },
          {
            "text": "            {\"role\": \"user\", \"content\": content_parts},",
            "lineNumber": 627
          },
          {
            "text": "        ]",
            "lineNumber": 628
          },
          {
            "lineNumber": 629
          },
          {
            "text": "    async def _call_vlm_with_multimodal_content(self, messages: List[Dict]) -&gt; str:",
            "lineNumber": 630
          },
          {
            "text": "        ",
            "lineNumber": 631
          }
        ]
      },
      "score": 0.32640135288238525
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def aquery_with_multimodal(\n        self,\n        query: str,\n        multimodal_content: List[Dict[str, Any]] = None,\n        mode: str = \"mix\",\n        **kwargs,\n    ) -&gt; str:\n\n\n        # If no multimodal content, fallback to pure text query\n        if not multimodal_content:\n            self.logger.info(\"No multimodal content provided, executing text query\")\n            return await self.aquery(query, mode=mode, **kwargs)\n\n        # Generate cache key for multimodal query\n        cache_key = self._generate_multimodal_cache_key(query, multimodal_content, mode, **kwargs)\n\n        # Check cache if available and enabled\n        cached_result = None\n        if (\n            hasattr(self, \"lightrag\")\n            and self.lightrag\n            and hasattr(self.lightrag, \"llm_response_cache\")\n            and self.lightrag.llm_response_cache\n        ):\n            if self.lightrag.llm_response_cache.global_config.get(\"enable_llm_cache\", True):\n                try:\n                    cached_result = await self.lightrag.llm_response_cache.get_by_id(cache_key)\n                    if cached_result and isinstance(cached_result, dict):\n                        result_content = cached_result.get(\"return\")\n                        if result_content:\n                            self.logger.info(f\"Multimodal query cache hit: {cache_key[:16]}...\")\n                            return result_content\n                except Exception as e:\n                    self.logger.debug(f\"Error accessing multimodal query cache: {e}\")\n\n        # Process multimodal content to generate enhanced query text\n        enhanced_query = await self._process_multimodal_query_content(query, multimodal_content)\n\n        self.logger.info(f\"Generated enhanced query length: {len(enhanced_query)} characters\")\n\n        # Execute enhanced query\n        result = await self.aquery(enhanced_query, mode=mode, **kwargs)\n\n        # Save to cache if available and enabled\n        if",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 25,
                "column": 5
              },
              "endPosition": {
                "line": 28,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def aquery_with_multimodal(",
            "lineNumber": 147
          },
          {
            "text": "        self,",
            "lineNumber": 148
          },
          {
            "text": "        query: str,",
            "lineNumber": 149
          },
          {
            "text": "        multimodal_content: List[Dict[str, Any]] = None,",
            "lineNumber": 150
          },
          {
            "text": "        mode: str = \"mix\",",
            "lineNumber": 151
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 152
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 153
          },
          {
            "lineNumber": 194
          },
          {
            "lineNumber": 195
          },
          {
            "text": "        # If no multimodal content, fallback to pure text query",
            "lineNumber": 196
          },
          {
            "text": "        if not multimodal_content:",
            "lineNumber": 197
          },
          {
            "text": "            self.logger.info(\"No multimodal content provided, executing text query\")",
            "lineNumber": 198
          },
          {
            "text": "            return await self.aquery(query, mode=mode, **kwargs)",
            "lineNumber": 199
          },
          {
            "lineNumber": 200
          },
          {
            "text": "        # Generate cache key for multimodal query",
            "lineNumber": 201
          },
          {
            "text": "        cache_key = self._generate_multimodal_cache_key(query, multimodal_content, mode, **kwargs)",
            "lineNumber": 202
          },
          {
            "lineNumber": 203
          },
          {
            "text": "        # Check cache if available and enabled",
            "lineNumber": 204
          },
          {
            "text": "        cached_result = None",
            "lineNumber": 205
          },
          {
            "text": "        if (",
            "lineNumber": 206
          },
          {
            "text": "            hasattr(self, \"lightrag\")",
            "lineNumber": 207
          },
          {
            "text": "            and self.lightrag",
            "lineNumber": 208
          },
          {
            "text": "            and hasattr(self.lightrag, \"llm_response_cache\")",
            "lineNumber": 209
          },
          {
            "text": "            and self.lightrag.llm_response_cache",
            "lineNumber": 210
          },
          {
            "text": "        ):",
            "lineNumber": 211
          },
          {
            "text": "            if self.lightrag.llm_response_cache.global_config.get(\"enable_llm_cache\", True):",
            "lineNumber": 212
          },
          {
            "text": "                try:",
            "lineNumber": 213
          },
          {
            "text": "                    cached_result = await self.lightrag.llm_response_cache.get_by_id(cache_key)",
            "lineNumber": 214
          },
          {
            "text": "                    if cached_result and isinstance(cached_result, dict):",
            "lineNumber": 215
          },
          {
            "text": "                        result_content = cached_result.get(\"return\")",
            "lineNumber": 216
          },
          {
            "text": "                        if result_content:",
            "lineNumber": 217
          },
          {
            "text": "                            self.logger.info(f\"Multimodal query cache hit: {cache_key[:16]}...\")",
            "lineNumber": 218
          },
          {
            "text": "                            return result_content",
            "lineNumber": 219
          },
          {
            "text": "                except Exception as e:",
            "lineNumber": 220
          },
          {
            "text": "                    self.logger.debug(f\"Error accessing multimodal query cache: {e}\")",
            "lineNumber": 221
          },
          {
            "lineNumber": 222
          },
          {
            "text": "        # Process multimodal content to generate enhanced query text",
            "lineNumber": 223
          },
          {
            "text": "        enhanced_query = await self._process_multimodal_query_content(query, multimodal_content)",
            "lineNumber": 224
          },
          {
            "lineNumber": 225
          },
          {
            "text": "        self.logger.info(f\"Generated enhanced query length: {len(enhanced_query)} characters\")",
            "lineNumber": 226
          },
          {
            "lineNumber": 227
          },
          {
            "text": "        # Execute enhanced query",
            "lineNumber": 228
          },
          {
            "text": "        result = await self.aquery(enhanced_query, mode=mode, **kwargs)",
            "lineNumber": 229
          },
          {
            "lineNumber": 230
          },
          {
            "text": "        # Save to cache if available and enabled",
            "lineNumber": 231
          },
          {
            "text": "        if",
            "lineNumber": 232
          }
        ]
      },
      "score": 0.31772536039352417
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README.md",
        "range": {
          "startPosition": {
            "line": 572
          },
          "endPosition": {
            "line": 626
          }
        },
        "contents": "class CustomModalProcessor(GenericModalProcessor):\n    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):\n        # Your custom processing logic\n        enhanced_description = await self.analyze_custom_content(modal_content)\n        entity_info = self.create_custom_entity(enhanced_description, entity_name)\n        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)\n&#96;&#96;&#96;\n\n#### 5. Query Options\n\nRAG-Anything provides three types of query methods:\n\n**Pure Text Queries** - Direct knowledge base search using LightRAG:\n&#96;&#96;&#96;python\n# Different query modes for text queries\ntext_result_hybrid = await rag.aquery(\"Your question\", mode=\"hybrid\")\ntext_result_local = await rag.aquery(\"Your question\", mode=\"local\")\ntext_result_global = await rag.aquery(\"Your question\", mode=\"global\")\ntext_result_naive = await rag.aquery(\"Your question\", mode=\"naive\")\n\n# Synchronous version\nsync_text_result = rag.query(\"Your question\", mode=\"hybrid\")\n&#96;&#96;&#96;\n\n**VLM Enhanced Queries** - Automatically analyze images in retrieved context using VLM:\n&#96;&#96;&#96;python\n# VLM enhanced query (automatically enabled when vision_model_func is provided)\nvlm_result = await rag.aquery(\n    \"Analyze the charts and figures in the document\",\n    mode=\"hybrid\"\n    # vlm_enhanced=True is automatically set when vision_model_func is available\n)\n\n# Manually control VLM enhancement\nvlm_enabled = await rag.aquery(\n    \"What do the images show in this document?\",\n    mode=\"hybrid\",\n    vlm_enhanced=True  # Force enable VLM enhancement\n)\n\nvlm_disabled = await rag.aquery(\n    \"What do the images show in this document?\",\n    mode=\"hybrid\",\n    vlm_enhanced=False  # Force disable VLM enhancement\n)\n\n# When documents contain images, VLM can see and analyze them directly\n# The system will automatically:\n# 1. Retrieve relevant context containing image paths\n# 2. Load and encode images as base64\n# 3. Send both text context and images to VLM for comprehensive analysis\n&#96;&#96;&#96;\n\n**Multimodal Queries** - Enhanced queries with specific multimodal content analysis:",
        "signatures": {},
        "detailedLines": [
          {
            "text": "class CustomModalProcessor(GenericModalProcessor):",
            "lineNumber": 573
          },
          {
            "text": "    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):",
            "lineNumber": 574
          },
          {
            "text": "        # Your custom processing logic",
            "lineNumber": 575
          },
          {
            "text": "        enhanced_description = await self.analyze_custom_content(modal_content)",
            "lineNumber": 576
          },
          {
            "text": "        entity_info = self.create_custom_entity(enhanced_description, entity_name)",
            "lineNumber": 577
          },
          {
            "text": "        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)",
            "lineNumber": 578
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 579
          },
          {
            "lineNumber": 580
          },
          {
            "text": "#### 5. Query Options",
            "lineNumber": 581
          },
          {
            "lineNumber": 582
          },
          {
            "text": "RAG-Anything provides three types of query methods:",
            "lineNumber": 583
          },
          {
            "lineNumber": 584
          },
          {
            "text": "**Pure Text Queries** - Direct knowledge base search using LightRAG:",
            "lineNumber": 585
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 586
          },
          {
            "text": "# Different query modes for text queries",
            "lineNumber": 587
          },
          {
            "text": "text_result_hybrid = await rag.aquery(\"Your question\", mode=\"hybrid\")",
            "lineNumber": 588
          },
          {
            "text": "text_result_local = await rag.aquery(\"Your question\", mode=\"local\")",
            "lineNumber": 589
          },
          {
            "text": "text_result_global = await rag.aquery(\"Your question\", mode=\"global\")",
            "lineNumber": 590
          },
          {
            "text": "text_result_naive = await rag.aquery(\"Your question\", mode=\"naive\")",
            "lineNumber": 591
          },
          {
            "lineNumber": 592
          },
          {
            "text": "# Synchronous version",
            "lineNumber": 593
          },
          {
            "text": "sync_text_result = rag.query(\"Your question\", mode=\"hybrid\")",
            "lineNumber": 594
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 595
          },
          {
            "lineNumber": 596
          },
          {
            "text": "**VLM Enhanced Queries** - Automatically analyze images in retrieved context using VLM:",
            "lineNumber": 597
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 598
          },
          {
            "text": "# VLM enhanced query (automatically enabled when vision_model_func is provided)",
            "lineNumber": 599
          },
          {
            "text": "vlm_result = await rag.aquery(",
            "lineNumber": 600
          },
          {
            "text": "    \"Analyze the charts and figures in the document\",",
            "lineNumber": 601
          },
          {
            "text": "    mode=\"hybrid\"",
            "lineNumber": 602
          },
          {
            "text": "    # vlm_enhanced=True is automatically set when vision_model_func is available",
            "lineNumber": 603
          },
          {
            "text": ")",
            "lineNumber": 604
          },
          {
            "lineNumber": 605
          },
          {
            "text": "# Manually control VLM enhancement",
            "lineNumber": 606
          },
          {
            "text": "vlm_enabled = await rag.aquery(",
            "lineNumber": 607
          },
          {
            "text": "    \"What do the images show in this document?\",",
            "lineNumber": 608
          },
          {
            "text": "    mode=\"hybrid\",",
            "lineNumber": 609
          },
          {
            "text": "    vlm_enhanced=True  # Force enable VLM enhancement",
            "lineNumber": 610
          },
          {
            "text": ")",
            "lineNumber": 611
          },
          {
            "lineNumber": 612
          },
          {
            "text": "vlm_disabled = await rag.aquery(",
            "lineNumber": 613
          },
          {
            "text": "    \"What do the images show in this document?\",",
            "lineNumber": 614
          },
          {
            "text": "    mode=\"hybrid\",",
            "lineNumber": 615
          },
          {
            "text": "    vlm_enhanced=False  # Force disable VLM enhancement",
            "lineNumber": 616
          },
          {
            "text": ")",
            "lineNumber": 617
          },
          {
            "lineNumber": 618
          },
          {
            "text": "# When documents contain images, VLM can see and analyze them directly",
            "lineNumber": 619
          },
          {
            "text": "# The system will automatically:",
            "lineNumber": 620
          },
          {
            "text": "# 1. Retrieve relevant context containing image paths",
            "lineNumber": 621
          },
          {
            "text": "# 2. Load and encode images as base64",
            "lineNumber": 622
          },
          {
            "text": "# 3. Send both text context and images to VLM for comprehensive analysis",
            "lineNumber": 623
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 624
          },
          {
            "lineNumber": 625
          },
          {
            "text": "**Multimodal Queries** - Enhanced queries with specific multimodal content analysis:",
            "lineNumber": 626
          }
        ]
      },
      "score": 0.3172405958175659
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {
            "line": 140
          },
          "endPosition": {
            "line": 247
          }
        },
        "contents": "async def main(args: argparse.Namespace) -&gt; int:\n\n\n    results: Dict[str, Any] = {\n        \"meta\": {\n            \"docs\": [str(p) for p in docs],\n            \"working_dir\": str(Path(cfg.working_dir).resolve()),\n            \"mode\": args.mode,\n            \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n            \"llm_model\": os.getenv(\"LLM_MODEL\", \"\"),\n            \"vlm_model\": os.getenv(\"VLM_MODEL\", \"\"),\n        },\n        \"queries\": [],\n    }\n\n    for q in queries:\n        # Capture retrieval prompt (for debugging image-path extraction)\n        prompt = await rag.lightrag.aquery(\n            q, param=QueryParam(mode=args.mode, only_need_prompt=True)\n        )\n        image_path_count = prompt.count(\"Image Path:\") if isinstance(prompt, str) else 0\n\n        item: Dict[str, Any] = {\n            \"query\": q,\n            \"retrieval_prompt\": prompt,\n            \"retrieval_prompt_image_path_count\": image_path_count,\n            \"runs\": {},\n        }\n\n        for flag in [False, True]:\n            t0 = time.perf_counter()\n            ans = await rag.aquery(q, mode=args.mode, vlm_enhanced=flag)\n            elapsed = time.perf_counter() - t0\n            item[\"runs\"][str(flag)] = {\"vlm_enhanced\": flag, \"answer\": ans, \"elapsed_s\": elapsed}\n\n        results[\"queries\"].append(item)\n\n    out_path = run_dir / \"results.json\"\n    out_path.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n\n    print(f\"Saved: {out_path}\")\n    # Quick sanity: show prompt image-path counts\n    summary = [\n        {\n            \"query\": q[\"query\"],\n            \"image_path_count\": q[\"retrieval_prompt_image_path_count\"],\n            \"elapsed_vlm_off_s\": q[\"runs\"][\"False\"][\"elapsed_s\"],\n            \"elapsed_vlm_on_s\": q[\"runs\"][\"True\"][\"elapsed_s\"],\n        }\n        for q in results[\"queries\"]\n    ]\n    print(json.dumps({\"summary\": summary}, ensure_ascii=False, indent=2))\n    return 0\n\n\ndef build_arg_parser() -&gt; argparse.ArgumentParser:\n    p = argparse.ArgumentParser(description=\"VLM enhanced query evaluation\")",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 141,
                "column": 1
              },
              "endPosition": {
                "line": 142,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "async def main(args: argparse.Namespace) -&gt; int:",
            "lineNumber": 141,
            "isSignature": true
          },
          {
            "lineNumber": 187
          },
          {
            "lineNumber": 188
          },
          {
            "text": "    results: Dict[str, Any] = {",
            "lineNumber": 189
          },
          {
            "text": "        \"meta\": {",
            "lineNumber": 190
          },
          {
            "text": "            \"docs\": [str(p) for p in docs],",
            "lineNumber": 191
          },
          {
            "text": "            \"working_dir\": str(Path(cfg.working_dir).resolve()),",
            "lineNumber": 192
          },
          {
            "text": "            \"mode\": args.mode,",
            "lineNumber": 193
          },
          {
            "text": "            \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),",
            "lineNumber": 194
          },
          {
            "text": "            \"llm_model\": os.getenv(\"LLM_MODEL\", \"\"),",
            "lineNumber": 195
          },
          {
            "text": "            \"vlm_model\": os.getenv(\"VLM_MODEL\", \"\"),",
            "lineNumber": 196
          },
          {
            "text": "        },",
            "lineNumber": 197
          },
          {
            "text": "        \"queries\": [],",
            "lineNumber": 198
          },
          {
            "text": "    }",
            "lineNumber": 199
          },
          {
            "lineNumber": 200
          },
          {
            "text": "    for q in queries:",
            "lineNumber": 201
          },
          {
            "text": "        # Capture retrieval prompt (for debugging image-path extraction)",
            "lineNumber": 202
          },
          {
            "text": "        prompt = await rag.lightrag.aquery(",
            "lineNumber": 203
          },
          {
            "text": "            q, param=QueryParam(mode=args.mode, only_need_prompt=True)",
            "lineNumber": 204
          },
          {
            "text": "        )",
            "lineNumber": 205
          },
          {
            "text": "        image_path_count = prompt.count(\"Image Path:\") if isinstance(prompt, str) else 0",
            "lineNumber": 206
          },
          {
            "lineNumber": 207
          },
          {
            "text": "        item: Dict[str, Any] = {",
            "lineNumber": 208
          },
          {
            "text": "            \"query\": q,",
            "lineNumber": 209
          },
          {
            "text": "            \"retrieval_prompt\": prompt,",
            "lineNumber": 210
          },
          {
            "text": "            \"retrieval_prompt_image_path_count\": image_path_count,",
            "lineNumber": 211
          },
          {
            "text": "            \"runs\": {},",
            "lineNumber": 212
          },
          {
            "text": "        }",
            "lineNumber": 213
          },
          {
            "lineNumber": 214
          },
          {
            "text": "        for flag in [False, True]:",
            "lineNumber": 215
          },
          {
            "text": "            t0 = time.perf_counter()",
            "lineNumber": 216
          },
          {
            "text": "            ans = await rag.aquery(q, mode=args.mode, vlm_enhanced=flag)",
            "lineNumber": 217
          },
          {
            "text": "            elapsed = time.perf_counter() - t0",
            "lineNumber": 218
          },
          {
            "text": "            item[\"runs\"][str(flag)] = {\"vlm_enhanced\": flag, \"answer\": ans, \"elapsed_s\": elapsed}",
            "lineNumber": 219
          },
          {
            "lineNumber": 220
          },
          {
            "text": "        results[\"queries\"].append(item)",
            "lineNumber": 221
          },
          {
            "lineNumber": 222
          },
          {
            "text": "    out_path = run_dir / \"results.json\"",
            "lineNumber": 223
          },
          {
            "text": "    out_path.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding=\"utf-8\")",
            "lineNumber": 224
          },
          {
            "lineNumber": 225
          },
          {
            "text": "    print(f\"Saved: {out_path}\")",
            "lineNumber": 226
          },
          {
            "text": "    # Quick sanity: show prompt image-path counts",
            "lineNumber": 227
          },
          {
            "text": "    summary = [",
            "lineNumber": 228
          },
          {
            "text": "        {",
            "lineNumber": 229
          },
          {
            "text": "            \"query\": q[\"query\"],",
            "lineNumber": 230
          },
          {
            "text": "            \"image_path_count\": q[\"retrieval_prompt_image_path_count\"],",
            "lineNumber": 231
          },
          {
            "text": "            \"elapsed_vlm_off_s\": q[\"runs\"][\"False\"][\"elapsed_s\"],",
            "lineNumber": 232
          },
          {
            "text": "            \"elapsed_vlm_on_s\": q[\"runs\"][\"True\"][\"elapsed_s\"],",
            "lineNumber": 233
          },
          {
            "text": "        }",
            "lineNumber": 234
          },
          {
            "text": "        for q in results[\"queries\"]",
            "lineNumber": 235
          },
          {
            "text": "    ]",
            "lineNumber": 236
          },
          {
            "text": "    print(json.dumps({\"summary\": summary}, ensure_ascii=False, indent=2))",
            "lineNumber": 237
          },
          {
            "text": "    return 0",
            "lineNumber": 238,
            "isSignature": true
          },
          {
            "lineNumber": 239
          },
          {
            "lineNumber": 240
          },
          {
            "text": "def build_arg_parser() -&gt; argparse.ArgumentParser:",
            "lineNumber": 241,
            "isSignature": true
          },
          {
            "text": "    p = argparse.ArgumentParser(description=\"VLM enhanced query evaluation\")",
            "lineNumber": 242
          }
        ]
      },
      "score": 0.31723758578300476
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README_zh.md",
        "range": {
          "startPosition": {
            "line": 554
          },
          "endPosition": {
            "line": 611,
            "column": 4
          }
        },
        "contents": "class CustomModalProcessor(GenericModalProcessor):\n    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):\n        # 你的自定义处理逻辑\n        enhanced_description = await self.analyze_custom_content(modal_content)\n        entity_info = self.create_custom_entity(enhanced_description, entity_name)\n        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)\n&#96;&#96;&#96;\n\n#### 5. 查询选项\n\nRAG-Anything 提供三种类型的查询方法：\n\n**纯文本查询** - 使用LightRAG直接进行知识库搜索：\n&#96;&#96;&#96;python\n# 文本查询的不同模式\ntext_result_hybrid = await rag.aquery(\"你的问题\", mode=\"hybrid\")\ntext_result_local = await rag.aquery(\"你的问题\", mode=\"local\")\ntext_result_global = await rag.aquery(\"你的问题\", mode=\"global\")\ntext_result_naive = await rag.aquery(\"你的问题\", mode=\"naive\")\n\n# 同步版本\nsync_text_result = rag.query(\"你的问题\", mode=\"hybrid\")\n&#96;&#96;&#96;\n\n**VLM增强查询** - 使用VLM自动分析检索上下文中的图像：\n&#96;&#96;&#96;python\n# VLM增强查询（当提供vision_model_func时自动启用）\nvlm_result = await rag.aquery(\n    \"分析文档中的图表和数据\",\n    mode=\"hybrid\"\n    # vlm_enhanced=True 当vision_model_func可用时自动设置\n)\n\n# 手动控制VLM增强\nvlm_enabled = await rag.aquery(\n    \"这个文档中的图片显示了什么内容？\",\n    mode=\"hybrid\",\n    vlm_enhanced=True  # 强制启用VLM增强\n)\n\nvlm_disabled = await rag.aquery(\n    \"这个文档中的图片显示了什么内容？\",\n    mode=\"hybrid\",\n    vlm_enhanced=False  # 强制禁用VLM增强\n)\n\n# 当文档包含图片时，VLM可以直接查看和分析图片\n# 系统将自动：\n# 1. 检索包含图片路径的相关上下文\n# 2. 加载图片并编码为base64格式\n# 3. 将文本上下文和图片一起发送给VLM进行综合分析\n&#96;&#96;&#96;\n\n**多模态查询** - 包含特定多模态内容分析的增强查询：\n&#96;&#96;&#96;python\n# 包含表格数据的查询\ntable_result = await rag.aquery_with_multimodal(\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "class CustomModalProcessor(GenericModalProcessor):",
            "lineNumber": 555
          },
          {
            "text": "    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):",
            "lineNumber": 556
          },
          {
            "text": "        # 你的自定义处理逻辑",
            "lineNumber": 557
          },
          {
            "text": "        enhanced_description = await self.analyze_custom_content(modal_content)",
            "lineNumber": 558
          },
          {
            "text": "        entity_info = self.create_custom_entity(enhanced_description, entity_name)",
            "lineNumber": 559
          },
          {
            "text": "        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)",
            "lineNumber": 560
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 561
          },
          {
            "lineNumber": 562
          },
          {
            "text": "#### 5. 查询选项",
            "lineNumber": 563
          },
          {
            "lineNumber": 564
          },
          {
            "text": "RAG-Anything 提供三种类型的查询方法：",
            "lineNumber": 565
          },
          {
            "lineNumber": 566
          },
          {
            "text": "**纯文本查询** - 使用LightRAG直接进行知识库搜索：",
            "lineNumber": 567
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 568
          },
          {
            "text": "# 文本查询的不同模式",
            "lineNumber": 569
          },
          {
            "text": "text_result_hybrid = await rag.aquery(\"你的问题\", mode=\"hybrid\")",
            "lineNumber": 570
          },
          {
            "text": "text_result_local = await rag.aquery(\"你的问题\", mode=\"local\")",
            "lineNumber": 571
          },
          {
            "text": "text_result_global = await rag.aquery(\"你的问题\", mode=\"global\")",
            "lineNumber": 572
          },
          {
            "text": "text_result_naive = await rag.aquery(\"你的问题\", mode=\"naive\")",
            "lineNumber": 573
          },
          {
            "lineNumber": 574
          },
          {
            "text": "# 同步版本",
            "lineNumber": 575
          },
          {
            "text": "sync_text_result = rag.query(\"你的问题\", mode=\"hybrid\")",
            "lineNumber": 576
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 577
          },
          {
            "lineNumber": 578
          },
          {
            "text": "**VLM增强查询** - 使用VLM自动分析检索上下文中的图像：",
            "lineNumber": 579
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 580
          },
          {
            "text": "# VLM增强查询（当提供vision_model_func时自动启用）",
            "lineNumber": 581
          },
          {
            "text": "vlm_result = await rag.aquery(",
            "lineNumber": 582
          },
          {
            "text": "    \"分析文档中的图表和数据\",",
            "lineNumber": 583
          },
          {
            "text": "    mode=\"hybrid\"",
            "lineNumber": 584
          },
          {
            "text": "    # vlm_enhanced=True 当vision_model_func可用时自动设置",
            "lineNumber": 585
          },
          {
            "text": ")",
            "lineNumber": 586
          },
          {
            "lineNumber": 587
          },
          {
            "text": "# 手动控制VLM增强",
            "lineNumber": 588
          },
          {
            "text": "vlm_enabled = await rag.aquery(",
            "lineNumber": 589
          },
          {
            "text": "    \"这个文档中的图片显示了什么内容？\",",
            "lineNumber": 590
          },
          {
            "text": "    mode=\"hybrid\",",
            "lineNumber": 591
          },
          {
            "text": "    vlm_enhanced=True  # 强制启用VLM增强",
            "lineNumber": 592
          },
          {
            "text": ")",
            "lineNumber": 593
          },
          {
            "lineNumber": 594
          },
          {
            "text": "vlm_disabled = await rag.aquery(",
            "lineNumber": 595
          },
          {
            "text": "    \"这个文档中的图片显示了什么内容？\",",
            "lineNumber": 596
          },
          {
            "text": "    mode=\"hybrid\",",
            "lineNumber": 597
          },
          {
            "text": "    vlm_enhanced=False  # 强制禁用VLM增强",
            "lineNumber": 598
          },
          {
            "text": ")",
            "lineNumber": 599
          },
          {
            "lineNumber": 600
          },
          {
            "text": "# 当文档包含图片时，VLM可以直接查看和分析图片",
            "lineNumber": 601
          },
          {
            "text": "# 系统将自动：",
            "lineNumber": 602
          },
          {
            "text": "# 1. 检索包含图片路径的相关上下文",
            "lineNumber": 603
          },
          {
            "text": "# 2. 加载图片并编码为base64格式",
            "lineNumber": 604
          },
          {
            "text": "# 3. 将文本上下文和图片一起发送给VLM进行综合分析",
            "lineNumber": 605
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 606
          },
          {
            "lineNumber": 607
          },
          {
            "text": "**多模态查询** - 包含特定多模态内容分析的增强查询：",
            "lineNumber": 608
          },
          {
            "text": "&#96;&#96;&#96;python",
            "lineNumber": 609
          },
          {
            "text": "# 包含表格数据的查询",
            "lineNumber": 610
          },
          {
            "text": "table_result = await rag.aquery_with_multimodal(",
            "lineNumber": 611
          },
          {
            "text": "    ",
            "lineNumber": 612
          }
        ]
      },
      "score": 0.2987242341041565
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 53,
            "column": 51
          }
        },
        "contents": "\"\"\"VLM enhanced query evaluation (on/off comparison).\n\nGoal:\n- Verify that when retrieved context contains &#96;Image Path: ...&#96;, the VLM-enhanced path:\n  1) extracts image paths\n  2) encodes images as base64\n  3) calls vision_model_func with multimodal messages\n\nThis script runs the same queries twice:\n- vlm_enhanced=False (text-only)\n- vlm_enhanced=True  (multimodal, requires vision_model_func)\n\nUsage:\n  python eval/integration/vlm_enhanced_query_eval.py \\\n    --docs eval/docs/全球資訊服務暨軟體市場規模.png eval/docs/cat-01.jpg \\\n    --working-dir eval/integration/_rag_storage_vlm_eval \\\n    --out-dir eval/integration/_results\n\nEnv:\n- LLM_BINDING_API_KEY, LLM_BINDING_HOST\n- LLM_MODEL (text model, default gpt-4o-mini)\n- VLM_MODEL (vision model, default gpt-4o)\n- EMBEDDING_MODEL, EMBEDDING_DIM\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport asyncio\nimport json\nimport os\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nfrom lightrag import QueryParam\nfrom lightrag.llm.openai import openai_complete_if_cache, openai_embed\nfrom lightrag.utils import EmbeddingFunc\n\nfrom raganything import RAGAnything, RAGAnythingConfig\n\n\ndef _try_load_dotenv() -&gt; None:\n    try:\n        from dotenv import load_dotenv\n\n        load_dotenv(dotenv_path=\".env\", override=False)\n    except Exception:\n        return\n\n\ndef _now_tag() -&gt; str:\n    return datetime.now().strftime(\"%Y%m%d-%H%M%S\")",
        "signatures": {},
        "detailedLines": [
          {
            "text": "\"\"\"VLM enhanced query evaluation (on/off comparison).",
            "lineNumber": 1
          },
          {
            "lineNumber": 2
          },
          {
            "text": "Goal:",
            "lineNumber": 3
          },
          {
            "text": "- Verify that when retrieved context contains &#96;Image Path: ...&#96;, the VLM-enhanced path:",
            "lineNumber": 4
          },
          {
            "text": "  1) extracts image paths",
            "lineNumber": 5
          },
          {
            "text": "  2) encodes images as base64",
            "lineNumber": 6
          },
          {
            "text": "  3) calls vision_model_func with multimodal messages",
            "lineNumber": 7
          },
          {
            "lineNumber": 8
          },
          {
            "text": "This script runs the same queries twice:",
            "lineNumber": 9
          },
          {
            "text": "- vlm_enhanced=False (text-only)",
            "lineNumber": 10
          },
          {
            "text": "- vlm_enhanced=True  (multimodal, requires vision_model_func)",
            "lineNumber": 11
          },
          {
            "lineNumber": 12
          },
          {
            "text": "Usage:",
            "lineNumber": 13
          },
          {
            "text": "  python eval/integration/vlm_enhanced_query_eval.py \\",
            "lineNumber": 14
          },
          {
            "text": "    --docs eval/docs/全球資訊服務暨軟體市場規模.png eval/docs/cat-01.jpg \\",
            "lineNumber": 15
          },
          {
            "text": "    --working-dir eval/integration/_rag_storage_vlm_eval \\",
            "lineNumber": 16
          },
          {
            "text": "    --out-dir eval/integration/_results",
            "lineNumber": 17
          },
          {
            "lineNumber": 18
          },
          {
            "text": "Env:",
            "lineNumber": 19
          },
          {
            "text": "- LLM_BINDING_API_KEY, LLM_BINDING_HOST",
            "lineNumber": 20
          },
          {
            "text": "- LLM_MODEL (text model, default gpt-4o-mini)",
            "lineNumber": 21
          },
          {
            "text": "- VLM_MODEL (vision model, default gpt-4o)",
            "lineNumber": 22
          },
          {
            "text": "- EMBEDDING_MODEL, EMBEDDING_DIM",
            "lineNumber": 23
          },
          {
            "text": "\"\"\"",
            "lineNumber": 24
          },
          {
            "lineNumber": 25
          },
          {
            "text": "from __future__ import annotations",
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "import argparse",
            "lineNumber": 28
          },
          {
            "text": "import asyncio",
            "lineNumber": 29
          },
          {
            "text": "import json",
            "lineNumber": 30
          },
          {
            "text": "import os",
            "lineNumber": 31
          },
          {
            "text": "import time",
            "lineNumber": 32
          },
          {
            "text": "from datetime import datetime",
            "lineNumber": 33
          },
          {
            "text": "from pathlib import Path",
            "lineNumber": 34
          },
          {
            "text": "from typing import Any, Dict, List",
            "lineNumber": 35
          },
          {
            "lineNumber": 36
          },
          {
            "text": "from lightrag import QueryParam",
            "lineNumber": 37
          },
          {
            "text": "from lightrag.llm.openai import openai_complete_if_cache, openai_embed",
            "lineNumber": 38
          },
          {
            "text": "from lightrag.utils import EmbeddingFunc",
            "lineNumber": 39
          },
          {
            "lineNumber": 40
          },
          {
            "text": "from raganything import RAGAnything, RAGAnythingConfig",
            "lineNumber": 41
          },
          {
            "lineNumber": 42
          },
          {
            "lineNumber": 43
          },
          {
            "text": "def _try_load_dotenv() -&gt; None:",
            "lineNumber": 44,
            "isSignature": true
          },
          {
            "text": "    try:",
            "lineNumber": 45
          },
          {
            "text": "        from dotenv import load_dotenv",
            "lineNumber": 46
          },
          {
            "lineNumber": 47
          },
          {
            "text": "        load_dotenv(dotenv_path=\".env\", override=False)",
            "lineNumber": 48
          },
          {
            "text": "    except Exception:",
            "lineNumber": 49
          },
          {
            "text": "        return",
            "lineNumber": 50,
            "isSignature": true
          },
          {
            "lineNumber": 51
          },
          {
            "lineNumber": 52
          },
          {
            "text": "def _now_tag() -&gt; str:",
            "lineNumber": 53,
            "isSignature": true
          },
          {
            "text": "    return datetime.now().strftime(\"%Y%m%d-%H%M%S\")",
            "lineNumber": 54,
            "isSignature": true
          }
        ]
      },
      "score": 0.2675372362136841
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:\n\n\n        def replace_image_path(match):\n            nonlocal images_processed\n\n            image_path = match.group(1).strip()\n            self.logger.debug(f\"Processing image path: '{image_path}'\")\n\n            # Validate path format (basic check)\n            if not image_path or len(image_path) &lt; 3:\n                self.logger.warning(f\"Invalid image path format: {image_path}\")\n                return match.group(0)  # Keep original\n\n            # Use utility function to validate image file\n            self.logger.debug(f\"Calling validate_image_file for: {image_path}\")\n            is_valid = validate_image_file(image_path)\n            self.logger.debug(f\"Validation result for {image_path}: {is_valid}\")\n\n            if not is_valid:\n                self.logger.warning(f\"Image validation failed for: {image_path}\")\n                return match.group(0)  # Keep original if validation fails\n\n            try:",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 482,
                "column": 5
              },
              "endPosition": {
                "line": 483,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def _process_image_paths_for_vlm(self, prompt: str) -&gt; tuple[str, int]:",
            "lineNumber": 482
          },
          {
            "lineNumber": 509
          },
          {
            "lineNumber": 510
          },
          {
            "text": "        def replace_image_path(match):",
            "lineNumber": 511
          },
          {
            "text": "            nonlocal images_processed",
            "lineNumber": 512
          },
          {
            "lineNumber": 513
          },
          {
            "text": "            image_path = match.group(1).strip()",
            "lineNumber": 514
          },
          {
            "text": "            self.logger.debug(f\"Processing image path: '{image_path}'\")",
            "lineNumber": 515
          },
          {
            "lineNumber": 516
          },
          {
            "text": "            # Validate path format (basic check)",
            "lineNumber": 517
          },
          {
            "text": "            if not image_path or len(image_path) &lt; 3:",
            "lineNumber": 518
          },
          {
            "text": "                self.logger.warning(f\"Invalid image path format: {image_path}\")",
            "lineNumber": 519
          },
          {
            "text": "                return match.group(0)  # Keep original",
            "lineNumber": 520
          },
          {
            "lineNumber": 521
          },
          {
            "text": "            # Use utility function to validate image file",
            "lineNumber": 522
          },
          {
            "text": "            self.logger.debug(f\"Calling validate_image_file for: {image_path}\")",
            "lineNumber": 523
          },
          {
            "text": "            is_valid = validate_image_file(image_path)",
            "lineNumber": 524
          },
          {
            "text": "            self.logger.debug(f\"Validation result for {image_path}: {is_valid}\")",
            "lineNumber": 525
          },
          {
            "lineNumber": 526
          },
          {
            "text": "            if not is_valid:",
            "lineNumber": 527
          },
          {
            "text": "                self.logger.warning(f\"Image validation failed for: {image_path}\")",
            "lineNumber": 528
          },
          {
            "text": "                return match.group(0)  # Keep original if validation fails",
            "lineNumber": 529
          },
          {
            "lineNumber": 530
          },
          {
            "text": "            try:",
            "lineNumber": 531
          }
        ]
      },
      "score": 0.2667100131511688
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\query.py",
        "range": {
          "startPosition": {
            "line": 21
          },
          "endPosition": {
            "line": 703
          }
        },
        "contents": "class QueryMixin:\nasync def _process_multimodal_query_content(\n        self, base_query: str, multimodal_content: List[Dict[str, Any]]\n    ) -&gt; str:\n\n\n        for i, content in enumerate(multimodal_content):\n            content_type = content.get(\"type\", \"unknown\")\n            self.logger.info(\n                f\"Processing {i + 1}/{len(multimodal_content)} multimodal content: {content_type}\"\n            )\n\n            try:\n                # Get appropriate processor\n                processor = get_processor_for_type(self.modal_processors, content_type)\n\n                if processor:\n                    # Generate content description\n                    description = await self._generate_query_content_description(\n                        processor, content, content_type\n                    )\n                    enhanced_parts.append(f\"\\nRelated {content_type} content: {description}\")\n                else:\n                    # If no appropriate processor, use basic description\n                    basic_desc = str(content)[:200]\n                    enhanced_parts.append(f\"\\nRelated {content_type} content: {basic_desc}\")\n\n            except Exception as e:\n                self.logger.error(f\"Error processing multimodal content: {str(e)}\")\n                # Continue processing other content\n                continue\n\n        enhanced_query = \"\\n\".join(enhanced_parts)\n        enhanced_query += PROMPTS[\"QUERY_ENHANCEMENT_SUFFIX\"]\n\n        self.logger.info(\"Multimodal query content processing completed\")\n        return enhanced_query\n\n    async def _generate_query_content_description(\n        self, processor, content: Dict[str, Any], content_type: str\n    ) -&gt; str:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 22,
                "column": 1
              },
              "endPosition": {
                "line": 23,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 269,
                "column": 5
              },
              "endPosition": {
                "line": 270,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class QueryMixin:",
            "lineNumber": 22,
            "isSignature": true
          },
          {
            "text": "async def _process_multimodal_query_content(",
            "lineNumber": 329
          },
          {
            "text": "        self, base_query: str, multimodal_content: List[Dict[str, Any]]",
            "lineNumber": 330
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 331
          },
          {
            "lineNumber": 344
          },
          {
            "lineNumber": 345
          },
          {
            "text": "        for i, content in enumerate(multimodal_content):",
            "lineNumber": 346
          },
          {
            "text": "            content_type = content.get(\"type\", \"unknown\")",
            "lineNumber": 347
          },
          {
            "text": "            self.logger.info(",
            "lineNumber": 348
          },
          {
            "text": "                f\"Processing {i + 1}/{len(multimodal_content)} multimodal content: {content_type}\"",
            "lineNumber": 349
          },
          {
            "text": "            )",
            "lineNumber": 350
          },
          {
            "lineNumber": 351
          },
          {
            "text": "            try:",
            "lineNumber": 352
          },
          {
            "text": "                # Get appropriate processor",
            "lineNumber": 353
          },
          {
            "text": "                processor = get_processor_for_type(self.modal_processors, content_type)",
            "lineNumber": 354
          },
          {
            "lineNumber": 355
          },
          {
            "text": "                if processor:",
            "lineNumber": 356
          },
          {
            "text": "                    # Generate content description",
            "lineNumber": 357
          },
          {
            "text": "                    description = await self._generate_query_content_description(",
            "lineNumber": 358
          },
          {
            "text": "                        processor, content, content_type",
            "lineNumber": 359
          },
          {
            "text": "                    )",
            "lineNumber": 360
          },
          {
            "text": "                    enhanced_parts.append(f\"\\nRelated {content_type} content: {description}\")",
            "lineNumber": 361
          },
          {
            "text": "                else:",
            "lineNumber": 362
          },
          {
            "text": "                    # If no appropriate processor, use basic description",
            "lineNumber": 363
          },
          {
            "text": "                    basic_desc = str(content)[:200]",
            "lineNumber": 364
          },
          {
            "text": "                    enhanced_parts.append(f\"\\nRelated {content_type} content: {basic_desc}\")",
            "lineNumber": 365
          },
          {
            "lineNumber": 366
          },
          {
            "text": "            except Exception as e:",
            "lineNumber": 367
          },
          {
            "text": "                self.logger.error(f\"Error processing multimodal content: {str(e)}\")",
            "lineNumber": 368
          },
          {
            "text": "                # Continue processing other content",
            "lineNumber": 369
          },
          {
            "text": "                continue",
            "lineNumber": 370
          },
          {
            "lineNumber": 371
          },
          {
            "text": "        enhanced_query = \"\\n\".join(enhanced_parts)",
            "lineNumber": 372
          },
          {
            "text": "        enhanced_query += PROMPTS[\"QUERY_ENHANCEMENT_SUFFIX\"]",
            "lineNumber": 373
          },
          {
            "lineNumber": 374
          },
          {
            "text": "        self.logger.info(\"Multimodal query content processing completed\")",
            "lineNumber": 375
          },
          {
            "text": "        return enhanced_query",
            "lineNumber": 376
          },
          {
            "lineNumber": 377
          },
          {
            "text": "    async def _generate_query_content_description(",
            "lineNumber": 378
          },
          {
            "text": "        self, processor, content: Dict[str, Any], content_type: str",
            "lineNumber": 379
          },
          {
            "text": "    ) -&gt; str:",
            "lineNumber": 380
          },
          {
            "text": "        ",
            "lineNumber": 381
          }
        ]
      },
      "score": 0.26627254486083984
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {
            "line": 53,
            "column": 51
          },
          "endPosition": {
            "line": 137
          }
        },
        "contents": "def _build_llm_vision_embedding():\n    api_key = os.getenv(\"LLM_BINDING_API_KEY\")\n    base_url = os.getenv(\"LLM_BINDING_HOST\")\n\n    if not api_key:\n        raise RuntimeError(\"Missing env var: LLM_BINDING_API_KEY\")\n\n    llm_model = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n    vlm_model = os.getenv(\"VLM_MODEL\", \"gpt-4o\")\n\n    def llm_model_func(prompt, system_prompt=None, history_messages=None, **kwargs):\n        return openai_complete_if_cache(\n            llm_model,\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages or [],\n            api_key=api_key,\n            base_url=base_url,\n            **kwargs,\n        )\n\n    def vision_model_func(\n        prompt,\n        system_prompt=None,\n        history_messages=None,\n        image_data=None,\n        messages=None,\n        **kwargs,\n    ):\n        # VLM enhanced query will pass messages (multimodal format) when images are found.\n        if messages:\n            return openai_complete_if_cache(\n                vlm_model,\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=messages,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        if image_data:\n            # Single image format (used by image processor)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def _build_llm_vision_embedding():",
            "lineNumber": 57,
            "isSignature": true
          },
          {
            "text": "    api_key = os.getenv(\"LLM_BINDING_API_KEY\")",
            "lineNumber": 58
          },
          {
            "text": "    base_url = os.getenv(\"LLM_BINDING_HOST\")",
            "lineNumber": 59
          },
          {
            "lineNumber": 60
          },
          {
            "text": "    if not api_key:",
            "lineNumber": 61
          },
          {
            "text": "        raise RuntimeError(\"Missing env var: LLM_BINDING_API_KEY\")",
            "lineNumber": 62
          },
          {
            "lineNumber": 63
          },
          {
            "text": "    llm_model = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")",
            "lineNumber": 64
          },
          {
            "text": "    vlm_model = os.getenv(\"VLM_MODEL\", \"gpt-4o\")",
            "lineNumber": 65
          },
          {
            "lineNumber": 66
          },
          {
            "text": "    def llm_model_func(prompt, system_prompt=None, history_messages=None, **kwargs):",
            "lineNumber": 67
          },
          {
            "text": "        return openai_complete_if_cache(",
            "lineNumber": 68
          },
          {
            "text": "            llm_model,",
            "lineNumber": 69
          },
          {
            "text": "            prompt,",
            "lineNumber": 70
          },
          {
            "text": "            system_prompt=system_prompt,",
            "lineNumber": 71
          },
          {
            "text": "            history_messages=history_messages or [],",
            "lineNumber": 72
          },
          {
            "text": "            api_key=api_key,",
            "lineNumber": 73
          },
          {
            "text": "            base_url=base_url,",
            "lineNumber": 74
          },
          {
            "text": "            **kwargs,",
            "lineNumber": 75
          },
          {
            "text": "        )",
            "lineNumber": 76
          },
          {
            "lineNumber": 77
          },
          {
            "text": "    def vision_model_func(",
            "lineNumber": 78
          },
          {
            "text": "        prompt,",
            "lineNumber": 79
          },
          {
            "text": "        system_prompt=None,",
            "lineNumber": 80
          },
          {
            "text": "        history_messages=None,",
            "lineNumber": 81
          },
          {
            "text": "        image_data=None,",
            "lineNumber": 82
          },
          {
            "text": "        messages=None,",
            "lineNumber": 83
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 84
          },
          {
            "text": "    ):",
            "lineNumber": 85
          },
          {
            "text": "        # VLM enhanced query will pass messages (multimodal format) when images are found.",
            "lineNumber": 86
          },
          {
            "text": "        if messages:",
            "lineNumber": 87
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 88
          },
          {
            "text": "                vlm_model,",
            "lineNumber": 89
          },
          {
            "text": "                \"\",",
            "lineNumber": 90
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 91
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 92
          },
          {
            "text": "                messages=messages,",
            "lineNumber": 93
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 94
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 95
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 96
          },
          {
            "text": "            )",
            "lineNumber": 97
          },
          {
            "text": "        if image_data:",
            "lineNumber": 98
          },
          {
            "text": "            # Single image format (used by image processor)",
            "lineNumber": 99
          }
        ]
      },
      "score": 0.26598119735717773
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\results\\vlm_enhanced_query\\20251212-162407\\results.json",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 20
          }
        },
        "contents": "{\n  \"meta\": {\n    \"docs\": [\n      \"D:\\\\_Code\\\\_GitHub\\\\RAG-Anything-1022\\\\eval\\\\docs\\\\全球資訊服務暨軟體市場規模.png\",\n      \"D:\\\\_Code\\\\_GitHub\\\\RAG-Anything-1022\\\\eval\\\\docs\\\\cat-01.jpg\"\n    ],\n    \"working_dir\": \"D:\\\\_Code\\\\_GitHub\\\\RAG-Anything-1022\\\\eval\\\\integration\\\\rag_storage_vlm_eval\",\n    \"mode\": \"hybrid\",\n    \"timestamp\": \"2025-12-12T16:24:07\",\n    \"llm_model\": \"gpt-4o-mini\",\n    \"vlm_model\": \"\"\n  },\n  \"queries\": [\n    {\n      \"query\": \"請描述文件/圖片中的主要內容重點。\",\n      \"retrieval_prompt\": \"---Role---\\n\\nYou are an expert AI assistant specializing in synthesizing information from a provided knowledge base. Your primary function is to answer user queries accurately by ONLY using the information within the provided **Context**.\\n\\n---Goal---\\n\\nGenerate a comprehensive, well-structured answer to the user query.\\nThe answer must integrate relevant facts from the Knowledge Graph and Document Chunks found in the **Context**.\\nConsider the conversation history if provided to maintain conversational flow and avoid repeating information.\\n\\n---Instructions---\\n\\n1. Step-by-Step Instruction:\\n  - Carefully determine the user's query intent in the context of the conversation history to fully understand the user's information need.\\n  - Scrutinize both &#96;Knowledge Graph Data&#96; and &#96;Document Chunks&#96; in the **Context**. Identify and extract all pieces of information that are directly relevant to answering the user query.\\n  - Weave the extracted facts into a coherent and logical response. Your own knowledge must ONLY be used to formulate fluent sentences and connect ideas, NOT to introduce any external information.\\n  - Track the reference_id of the document chunk which directly support the facts presented in the response. Correlate reference_id with the entries in the &#96;Reference Document List&#96; to generate the appropriate citations.\\n  - Generate a references section at the end of the response. Each reference document must directly support the facts presented in the response.\\n  - Do not generate anything after the reference section.\\n\\n2. Content &amp; Grounding:\\n  - Strictly adhere to the provided context from the **Context**; DO NOT invent, assume, or infer any information not explicitly stated.\\n  - If the answer cannot be found in the **Context**, state that you do not have enough information to answer. Do not attempt to guess.\\n\\n3. Formatting &amp; Language:\\n  - The response MUST be in the same language as the user query.\\n  - The response MUST utilize Markdown formatting for enhanced clarity and structure (e.g., headings, bold text, bullet points).\\n  - The response should be presented in Multiple Paragraphs.\\n\\n4. References Section Format:\\n  - The References section should be under heading: &#96;### References&#96;\\n  - Reference list entries should adhere to the format: &#96;* [n] Document Title&#96;. Do not include a caret (&#96;^&#96;) after opening square bracket (&#96;[&#96;).\\n  - The Document Title in the citation must retain its original language.\\n  - Output each citation on an individual line\\n  - Provide maximum of 5 most relevant citations.\\n  - Do not generate footnotes section or any comment, summary, or explanation after the references.\\n\\n5. Reference Section Example:\\n&#96;&#96;&#96;\\n### References\\n\\n- [1] Document Title One\\n- [2] Document Title Two\\n- [3] Document Title Three\\n&#96;&#96;&#96;\\n\\n6. Additional Instructions: n/a\\n\\n\\n---Context---\\n\\n\\nKnowledge Graph Data (Entity):\\n\\n&#96;&#96;&#96;json\\n{\\\"entity\\\": \\\"MIC Economic Research Team\\\", \\\"type\\\": \\\"organization\\\", \\\"description\\\": \\\"The MIC Economic Research Team is the source of the data, providing analysis and market insights.\\\"}\\n{\\\"entity\\\": \\\"Bar Chart\\\", \\\"type\\\": \\\"artifact\\\", \\\"description\\\": \\\"The Bar Chart visually represents the global information service and software market size data from 2020 to 2024.\\\"}\\n{\\\"entity\\\": \\\"Cozy Atmosphere\\\", \\\"type\\\": \\\"concept\\\", \\\"description\\\": \\\"The scene is characterized by a soft and warm tonal palette, creating a cozy and serene atmosphere.\\\"}\\n{\\\"entity\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"type\\\": \\\"image\\\", \\\"description\\\": \\\"This image presents a bar chart displaying the projected growth and market size of IT Software and IT Services from 2020 to 2024, highlighting growth rates and total values for each year, as provided by 資策會MIC經濟部ITIS研究團隊整理·2021年9月.\\\"}\\n{\\\"entity\\\": \\\"IT Software\\\", \\\"type\\\": \\\"category\\\", \\\"description\\\": \\\"IT Software refers to a segment of the information service and software market, represented in the bar chart.\\\"}\\n{\\\"entity\\\": \\\"2021\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2021 marks the year showing a projected growth rate of 5.6% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"2022\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2022 marks the year showing a projected growth rate of 8.7% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"IT Services\\\", \\\"type\\\": \\\"category\\\", \\\"description\\\": \\\"IT Services refers to another segment of the information service and software market, represented in the bar chart.\\\"}\\n{\\\"entity\\\": \\\"2023\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2023 marks the year showing a projected growth rate of 5.6% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"IT Software and IT Services\\\", \\\"type\\\": \\\"category\\\", \\\"description\\\": \\\"IT Software and IT Services are the two segments represented in the bar chart for market size evaluation.\\\"}\\n{\\\"entity\\\": \\\"2020\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2020 marks the year showing a growth rate of 1.5% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"2024\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2024 marks the year showing a projected growth rate of 6.0% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"Global Information Service and Software Market Size\\\", \\\"type\\\": \\\"concept\\\", \\\"description\\\": \\\"The Global Information Service and Software Market Size is represented in a bar chart indicating market growth from 2020 to 2024.\\\"}\\n{\\\"entity\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"type\\\": \\\"image\\\", \\\"description\\\": \\\"A serene scene of a white cat with orange and grey patches, sitting on a sunlit windowsill, characterized by a warm and cozy atmosphere.\\\"}\\n{\\\"entity\\\": \\\"Sunlight\\\", \\\"type\\\": \\\"naturalobject\\\", \\\"description\\\": \\\"Sunlight brightens the scene and casts a warm glow over the cat's fur and the windowsill.\\\"}\\n{\\\"entity\\\": \\\"Growth Rate\\\", \\\"type\\\": \\\"concept\\\", \\\"description\\\": \\\"The Growth Rate refers to the annual percentage increases in the global information service and software market over the specified years.\\\"}\\n{\\\"entity\\\": \\\"White Cat\\\", \\\"type\\\": \\\"creature\\\", \\\"description\\\": \\\"The white cat features orange and grey patches, lounging on a windowsill with folded ears and attentive amber eyes.\\\"}\\n{\\\"entity\\\": \\\"Windowsill\\\", \\\"type\\\": \\\"artifact\\\", \\\"description\\\": \\\"The windowsill is a surface where the cat is resting, providing a cozy spot for relaxation and observation.\\\"}\\n{\\\"entity\\\": \\\"Green Houseplant\\\", \\\"type\\\": \\\"naturalobject\\\", \\\"description\\\": \\\"The leafy green houseplant is located in the background, adding a touch of nature and depth to the scene.\\\"}\\n&#96;&#96;&#96;\\n\\nKnowledge Graph Data (Relationship):\\n\\n&#96;&#96;&#96;json\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"MIC Economic Research Team\\\", \\\"description\\\": \\\"Entity MIC Economic Research Team belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Bar Chart\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity Bar Chart belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"IT Software\\\", \\\"description\\\": \\\"Entity IT Software belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"2021\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2021 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"2022\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2022 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"IT Services\\\", \\\"description\\\": \\\"Entity IT Services belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"2023\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2023 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"IT Software and IT Services\\\", \\\"description\\\": \\\"Entity IT Software and IT Services belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"2020\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2020 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"MIC Economic Research Team\\\", \\\"description\\\": \\\"The MIC Economic Research Team is identified as the source of the data for the Global Information Service and Software Market Size analysis.\\\"}\\n{\\\"entity1\\\": \\\"2024\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2024 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"IT Software\\\", \\\"description\\\": \\\"IT Software is depicted as a segment of the Global Information Service and Software Market Size in the bar chart.\\\"}\\n{\\\"entity1\\\": \\\"Bar Chart\\\", \\\"entity2\\\": \\\"Global Information Service and Software Market Size\\\", \\\"description\\\": \\\"The Bar Chart represents the Global Information Service and Software Market Size data visually from 2020 to 2024.\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"IT Services\\\", \\\"description\\\": \\\"IT Services is depicted as a segment of the Global Information Service and Software Market Size in the bar chart.\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity Global Information Service and Software Market Size belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"IT Software and IT Services\\\", \\\"description\\\": \\\"IT Software and IT Services collectively represent the two sectors of the market size being analyzed.\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"Sunlight\\\", \\\"description\\\": \\\"Entity Sunlight belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"Cozy Atmosphere\\\", \\\"description\\\": \\\"Entity Cozy Atmosphere belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"Growth Rate\\\", \\\"description\\\": \\\"Entity Growth Rate belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"White Cat\\\", \\\"description\\\": \\\"Entity White Cat belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"Windowsill\\\", \\\"description\\\": \\\"Entity Windowsill belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"Green Houseplant\\\", \\\"description\\\": \\\"Entity Green Houseplant belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Green Houseplant\\\", \\\"entity2\\\": \\\"White Cat\\\", \\\"description\\\": \\\"The presence of the green houseplant enhances the interior setting and adds to the visual aesthetic of the scene.\\\"}\\n&#96;&#96;&#96;\\n\\nDocument Chunks (Each entry has a reference_id refer to the &#96;Reference Document List&#96;):\\n\\n&#96;&#96;&#96;json\\n{\\\"reference_id\\\": \\\"1\\\", \\\"content\\\": \\\"\\\\nImage Content Analysis:\\\\nImage Path: D:\\\\\\\\_Code\\\\\\\\_GitHub\\\\\\\\RAG-Anything-1022\\\\\\\\eval\\\\\\\\integration\\\\\\\\output\\\\\\\\全球資訊服務暨軟體市場規模\\\\\\\\ocr\\\\\\\\images\\\\\\\\03a9fb8d391d79e6cad5b9e2e2a330c2bfecf5cf34a9a895a89365b80e1f55ad.jpg\\\\nCaptions: 圖1全球資訊服務暨軟體市場規模, 資料來源：資策會MIC經濟部ITIS研究團隊整理·2021年9月\\\\nFootnotes: None\\\\n\\\\nVisual Analysis: The image is a bar chart depicting the global information service and software market size from 2020 to 2024, with projected figures for 2021 to 2024. The chart consists of vertical stacked bars representing values for IT Software and IT Services across five years. Each bar is divided into a light blue segment for IT Software and a darker blue segment for IT Services. The values for IT Software and IT Services are annotated at the base of each bar. A gray line with percentage values appears above the bars, indicating the growth rate from year to year: 1.5% for 2020, 5.6% for 2021, 8.7% for 2022, 5.6% for 2023, and 6.0% for 2024. The legend at the bottom clarifies the colors used for IT Software and IT Services. Text at the bottom provides the total market size for each year. The data source is cited as 資策會MIC經濟部ITIS研究團隊整理·2021年9月. The overall visual style is modern, with clean lines and contrasting colors for clarity.\\\"}\\n{\\\"reference_id\\\": \\\"2\\\", \\\"content\\\": \\\"\\\\nImage Content Analysis:\\\\nImage Path: D:\\\\\\\\_Code\\\\\\\\_GitHub\\\\\\\\RAG-Anything-1022\\\\\\\\eval\\\\\\\\integration\\\\\\\\output\\\\\\\\cat-01\\\\\\\\ocr\\\\\\\\images\\\\\\\\dfbd21cf1cd9b9febb670cc77e73d9497c0df0a2591b55f3643db901527bec20.jpg\\\\nCaptions: None\\\\nFootnotes: None\\\\n\\\\nVisual Analysis: The image features a white cat with orange and grey patches lounging on a white windowsill. The cat's ears are folded, and it has large, attentive amber eyes staring outside. Sunlight brightens the scene, casting a warm glow across the cat's fur and the surface of the windowsill. In the background, out of focus, is a leafy green houseplant, adding depth and a touch of nature to the interior setting. The overall tonal palette is soft and warm, creating a cozy and serene atmosphere. The cat's body is relaxed, with its front paws neatly tucked under its chin, suggesting a calm disposition as it gazes out the window, likely observing something of interest.\\\"}\\n&#96;&#96;&#96;\\n\\nReference Document List (Each entry starts with a [reference_id] that corresponds to entries in the Document Chunks):\\n\\n&#96;&#96;&#96;\\n[1] 全球資訊服務暨軟體市場規模.png\\n[2] cat-01.jpg\\n&#96;&#96;&#96;\\n\\n\\n\\n\\n---User Query---\\n\\n請描述文件/圖片中的主要內容重點。\",\n      \"retrieval_prompt_image_path_count\": 2,\n      \"runs\": {\n        \"False\": {\n          \"vlm_enhanced\": false,",
        "signatures": {},
        "detailedLines": [
          {
            "text": "{",
            "lineNumber": 1
          },
          {
            "text": "  \"meta\": {",
            "lineNumber": 2
          },
          {
            "text": "    \"docs\": [",
            "lineNumber": 3
          },
          {
            "text": "      \"D:\\\\_Code\\\\_GitHub\\\\RAG-Anything-1022\\\\eval\\\\docs\\\\全球資訊服務暨軟體市場規模.png\",",
            "lineNumber": 4
          },
          {
            "text": "      \"D:\\\\_Code\\\\_GitHub\\\\RAG-Anything-1022\\\\eval\\\\docs\\\\cat-01.jpg\"",
            "lineNumber": 5
          },
          {
            "text": "    ],",
            "lineNumber": 6
          },
          {
            "text": "    \"working_dir\": \"D:\\\\_Code\\\\_GitHub\\\\RAG-Anything-1022\\\\eval\\\\integration\\\\rag_storage_vlm_eval\",",
            "lineNumber": 7
          },
          {
            "text": "    \"mode\": \"hybrid\",",
            "lineNumber": 8
          },
          {
            "text": "    \"timestamp\": \"2025-12-12T16:24:07\",",
            "lineNumber": 9
          },
          {
            "text": "    \"llm_model\": \"gpt-4o-mini\",",
            "lineNumber": 10
          },
          {
            "text": "    \"vlm_model\": \"\"",
            "lineNumber": 11
          },
          {
            "text": "  },",
            "lineNumber": 12
          },
          {
            "text": "  \"queries\": [",
            "lineNumber": 13
          },
          {
            "text": "    {",
            "lineNumber": 14
          },
          {
            "text": "      \"query\": \"請描述文件/圖片中的主要內容重點。\",",
            "lineNumber": 15
          },
          {
            "text": "      \"retrieval_prompt\": \"---Role---\\n\\nYou are an expert AI assistant specializing in synthesizing information from a provided knowledge base. Your primary function is to answer user queries accurately by ONLY using the information within the provided **Context**.\\n\\n---Goal---\\n\\nGenerate a comprehensive, well-structured answer to the user query.\\nThe answer must integrate relevant facts from the Knowledge Graph and Document Chunks found in the **Context**.\\nConsider the conversation history if provided to maintain conversational flow and avoid repeating information.\\n\\n---Instructions---\\n\\n1. Step-by-Step Instruction:\\n  - Carefully determine the user's query intent in the context of the conversation history to fully understand the user's information need.\\n  - Scrutinize both &#96;Knowledge Graph Data&#96; and &#96;Document Chunks&#96; in the **Context**. Identify and extract all pieces of information that are directly relevant to answering the user query.\\n  - Weave the extracted facts into a coherent and logical response. Your own knowledge must ONLY be used to formulate fluent sentences and connect ideas, NOT to introduce any external information.\\n  - Track the reference_id of the document chunk which directly support the facts presented in the response. Correlate reference_id with the entries in the &#96;Reference Document List&#96; to generate the appropriate citations.\\n  - Generate a references section at the end of the response. Each reference document must directly support the facts presented in the response.\\n  - Do not generate anything after the reference section.\\n\\n2. Content &amp; Grounding:\\n  - Strictly adhere to the provided context from the **Context**; DO NOT invent, assume, or infer any information not explicitly stated.\\n  - If the answer cannot be found in the **Context**, state that you do not have enough information to answer. Do not attempt to guess.\\n\\n3. Formatting &amp; Language:\\n  - The response MUST be in the same language as the user query.\\n  - The response MUST utilize Markdown formatting for enhanced clarity and structure (e.g., headings, bold text, bullet points).\\n  - The response should be presented in Multiple Paragraphs.\\n\\n4. References Section Format:\\n  - The References section should be under heading: &#96;### References&#96;\\n  - Reference list entries should adhere to the format: &#96;* [n] Document Title&#96;. Do not include a caret (&#96;^&#96;) after opening square bracket (&#96;[&#96;).\\n  - The Document Title in the citation must retain its original language.\\n  - Output each citation on an individual line\\n  - Provide maximum of 5 most relevant citations.\\n  - Do not generate footnotes section or any comment, summary, or explanation after the references.\\n\\n5. Reference Section Example:\\n&#96;&#96;&#96;\\n### References\\n\\n- [1] Document Title One\\n- [2] Document Title Two\\n- [3] Document Title Three\\n&#96;&#96;&#96;\\n\\n6. Additional Instructions: n/a\\n\\n\\n---Context---\\n\\n\\nKnowledge Graph Data (Entity):\\n\\n&#96;&#96;&#96;json\\n{\\\"entity\\\": \\\"MIC Economic Research Team\\\", \\\"type\\\": \\\"organization\\\", \\\"description\\\": \\\"The MIC Economic Research Team is the source of the data, providing analysis and market insights.\\\"}\\n{\\\"entity\\\": \\\"Bar Chart\\\", \\\"type\\\": \\\"artifact\\\", \\\"description\\\": \\\"The Bar Chart visually represents the global information service and software market size data from 2020 to 2024.\\\"}\\n{\\\"entity\\\": \\\"Cozy Atmosphere\\\", \\\"type\\\": \\\"concept\\\", \\\"description\\\": \\\"The scene is characterized by a soft and warm tonal palette, creating a cozy and serene atmosphere.\\\"}\\n{\\\"entity\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"type\\\": \\\"image\\\", \\\"description\\\": \\\"This image presents a bar chart displaying the projected growth and market size of IT Software and IT Services from 2020 to 2024, highlighting growth rates and total values for each year, as provided by 資策會MIC經濟部ITIS研究團隊整理·2021年9月.\\\"}\\n{\\\"entity\\\": \\\"IT Software\\\", \\\"type\\\": \\\"category\\\", \\\"description\\\": \\\"IT Software refers to a segment of the information service and software market, represented in the bar chart.\\\"}\\n{\\\"entity\\\": \\\"2021\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2021 marks the year showing a projected growth rate of 5.6% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"2022\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2022 marks the year showing a projected growth rate of 8.7% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"IT Services\\\", \\\"type\\\": \\\"category\\\", \\\"description\\\": \\\"IT Services refers to another segment of the information service and software market, represented in the bar chart.\\\"}\\n{\\\"entity\\\": \\\"2023\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2023 marks the year showing a projected growth rate of 5.6% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"IT Software and IT Services\\\", \\\"type\\\": \\\"category\\\", \\\"description\\\": \\\"IT Software and IT Services are the two segments represented in the bar chart for market size evaluation.\\\"}\\n{\\\"entity\\\": \\\"2020\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2020 marks the year showing a growth rate of 1.5% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"2024\\\", \\\"type\\\": \\\"data\\\", \\\"description\\\": \\\"2024 marks the year showing a projected growth rate of 6.0% for the information service and software market.\\\"}\\n{\\\"entity\\\": \\\"Global Information Service and Software Market Size\\\", \\\"type\\\": \\\"concept\\\", \\\"description\\\": \\\"The Global Information Service and Software Market Size is represented in a bar chart indicating market growth from 2020 to 2024.\\\"}\\n{\\\"entity\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"type\\\": \\\"image\\\", \\\"description\\\": \\\"A serene scene of a white cat with orange and grey patches, sitting on a sunlit windowsill, characterized by a warm and cozy atmosphere.\\\"}\\n{\\\"entity\\\": \\\"Sunlight\\\", \\\"type\\\": \\\"naturalobject\\\", \\\"description\\\": \\\"Sunlight brightens the scene and casts a warm glow over the cat's fur and the windowsill.\\\"}\\n{\\\"entity\\\": \\\"Growth Rate\\\", \\\"type\\\": \\\"concept\\\", \\\"description\\\": \\\"The Growth Rate refers to the annual percentage increases in the global information service and software market over the specified years.\\\"}\\n{\\\"entity\\\": \\\"White Cat\\\", \\\"type\\\": \\\"creature\\\", \\\"description\\\": \\\"The white cat features orange and grey patches, lounging on a windowsill with folded ears and attentive amber eyes.\\\"}\\n{\\\"entity\\\": \\\"Windowsill\\\", \\\"type\\\": \\\"artifact\\\", \\\"description\\\": \\\"The windowsill is a surface where the cat is resting, providing a cozy spot for relaxation and observation.\\\"}\\n{\\\"entity\\\": \\\"Green Houseplant\\\", \\\"type\\\": \\\"naturalobject\\\", \\\"description\\\": \\\"The leafy green houseplant is located in the background, adding a touch of nature and depth to the scene.\\\"}\\n&#96;&#96;&#96;\\n\\nKnowledge Graph Data (Relationship):\\n\\n&#96;&#96;&#96;json\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"MIC Economic Research Team\\\", \\\"description\\\": \\\"Entity MIC Economic Research Team belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Bar Chart\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity Bar Chart belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"IT Software\\\", \\\"description\\\": \\\"Entity IT Software belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"2021\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2021 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"2022\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2022 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"IT Services\\\", \\\"description\\\": \\\"Entity IT Services belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"2023\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2023 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"IT Software and IT Services\\\", \\\"description\\\": \\\"Entity IT Software and IT Services belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"2020\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2020 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"MIC Economic Research Team\\\", \\\"description\\\": \\\"The MIC Economic Research Team is identified as the source of the data for the Global Information Service and Software Market Size analysis.\\\"}\\n{\\\"entity1\\\": \\\"2024\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity 2024 belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"IT Software\\\", \\\"description\\\": \\\"IT Software is depicted as a segment of the Global Information Service and Software Market Size in the bar chart.\\\"}\\n{\\\"entity1\\\": \\\"Bar Chart\\\", \\\"entity2\\\": \\\"Global Information Service and Software Market Size\\\", \\\"description\\\": \\\"The Bar Chart represents the Global Information Service and Software Market Size data visually from 2020 to 2024.\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"IT Services\\\", \\\"description\\\": \\\"IT Services is depicted as a segment of the Global Information Service and Software Market Size in the bar chart.\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"description\\\": \\\"Entity Global Information Service and Software Market Size belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Service and Software Market Size\\\", \\\"entity2\\\": \\\"IT Software and IT Services\\\", \\\"description\\\": \\\"IT Software and IT Services collectively represent the two sectors of the market size being analyzed.\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"Sunlight\\\", \\\"description\\\": \\\"Entity Sunlight belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"Cozy Atmosphere\\\", \\\"description\\\": \\\"Entity Cozy Atmosphere belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Global Information Services and Software Market Size Chart (image)\\\", \\\"entity2\\\": \\\"Growth Rate\\\", \\\"description\\\": \\\"Entity Growth Rate belongs to Global Information Services and Software Market Size Chart (image)\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"White Cat\\\", \\\"description\\\": \\\"Entity White Cat belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"Windowsill\\\", \\\"description\\\": \\\"Entity Windowsill belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Cat Lounging on Windowsill (image)\\\", \\\"entity2\\\": \\\"Green Houseplant\\\", \\\"description\\\": \\\"Entity Green Houseplant belongs to Cat Lounging on Windowsill (image)\\\"}\\n{\\\"entity1\\\": \\\"Green Houseplant\\\", \\\"entity2\\\": \\\"White Cat\\\", \\\"description\\\": \\\"The presence of the green houseplant enhances the interior setting and adds to the visual aesthetic of the scene.\\\"}\\n&#96;&#96;&#96;\\n\\nDocument Chunks (Each entry has a reference_id refer to the &#96;Reference Document List&#96;):\\n\\n&#96;&#96;&#96;json\\n{\\\"reference_id\\\": \\\"1\\\", \\\"content\\\": \\\"\\\\nImage Content Analysis:\\\\nImage Path: D:\\\\\\\\_Code\\\\\\\\_GitHub\\\\\\\\RAG-Anything-1022\\\\\\\\eval\\\\\\\\integration\\\\\\\\output\\\\\\\\全球資訊服務暨軟體市場規模\\\\\\\\ocr\\\\\\\\images\\\\\\\\03a9fb8d391d79e6cad5b9e2e2a330c2bfecf5cf34a9a895a89365b80e1f55ad.jpg\\\\nCaptions: 圖1全球資訊服務暨軟體市場規模, 資料來源：資策會MIC經濟部ITIS研究團隊整理·2021年9月\\\\nFootnotes: None\\\\n\\\\nVisual Analysis: The image is a bar chart depicting the global information service and software market size from 2020 to 2024, with projected figures for 2021 to 2024. The chart consists of vertical stacked bars representing values for IT Software and IT Services across five years. Each bar is divided into a light blue segment for IT Software and a darker blue segment for IT Services. The values for IT Software and IT Services are annotated at the base of each bar. A gray line with percentage values appears above the bars, indicating the growth rate from year to year: 1.5% for 2020, 5.6% for 2021, 8.7% for 2022, 5.6% for 2023, and 6.0% for 2024. The legend at the bottom clarifies the colors used for IT Software and IT Services. Text at the bottom provides the total market size for each year. The data source is cited as 資策會MIC經濟部ITIS研究團隊整理·2021年9月. The overall visual style is modern, with clean lines and contrasting colors for clarity.\\\"}\\n{\\\"reference_id\\\": \\\"2\\\", \\\"content\\\": \\\"\\\\nImage Content Analysis:\\\\nImage Path: D:\\\\\\\\_Code\\\\\\\\_GitHub\\\\\\\\RAG-Anything-1022\\\\\\\\eval\\\\\\\\integration\\\\\\\\output\\\\\\\\cat-01\\\\\\\\ocr\\\\\\\\images\\\\\\\\dfbd21cf1cd9b9febb670cc77e73d9497c0df0a2591b55f3643db901527bec20.jpg\\\\nCaptions: None\\\\nFootnotes: None\\\\n\\\\nVisual Analysis: The image features a white cat with orange and grey patches lounging on a white windowsill. The cat's ears are folded, and it has large, attentive amber eyes staring outside. Sunlight brightens the scene, casting a warm glow across the cat's fur and the surface of the windowsill. In the background, out of focus, is a leafy green houseplant, adding depth and a touch of nature to the interior setting. The overall tonal palette is soft and warm, creating a cozy and serene atmosphere. The cat's body is relaxed, with its front paws neatly tucked under its chin, suggesting a calm disposition as it gazes out the window, likely observing something of interest.\\\"}\\n&#96;&#96;&#96;\\n\\nReference Document List (Each entry starts with a [reference_id] that corresponds to entries in the Document Chunks):\\n\\n&#96;&#96;&#96;\\n[1] 全球資訊服務暨軟體市場規模.png\\n[2] cat-01.jpg\\n&#96;&#96;&#96;\\n\\n\\n\\n\\n---User Query---\\n\\n請描述文件/圖片中的主要內容重點。\",",
            "lineNumber": 16
          },
          {
            "text": "      \"retrieval_prompt_image_path_count\": 2,",
            "lineNumber": 17
          },
          {
            "text": "      \"runs\": {",
            "lineNumber": 18
          },
          {
            "text": "        \"False\": {",
            "lineNumber": 19
          },
          {
            "text": "          \"vlm_enhanced\": false,",
            "lineNumber": 20
          }
        ]
      },
      "score": 0.2633570432662964
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\RAG-Anything-試用報告-cursor.md",
        "range": {
          "startPosition": {
            "line": 295,
            "column": 5
          },
          "endPosition": {
            "line": 337,
            "column": 5
          }
        },
        "contents": "- 適合丟 Excel/BI 做圖表\n\n4. **如何判讀結果**：\n   - **平均分數（avg_score）**：越高越接近 ground truth\n   - **平均耗時（avg_elapsed_s）**：越低越好\n   - 建議再按 &#96;query_type&#96; 分組（事實/解釋/摘要），看 hybrid 在複雜題是否優勢更明顯\n\n**驗收標準：**\n- Hybrid 模式在複雜查詢上的相似度分數 ≥ 0.75\n- Hybrid 模式優於單一模式（naive/local/global）的查詢比例 ≥ 70%\n- 各模式的回應時間在可接受範圍內（&lt; 30 秒）\n- 檢索到的上下文相關性 ≥ 80%\n\n---\n\n#### 6.1.3 VLM 增強查詢功能測試\n\n**測試目標：**\n- 驗證 VLM（Vision Language Model）能正確分析檢索到的圖片\n- 確認圖片路徑能正確轉換為 base64 編碼並傳送給 VLM\n- 驗證 VLM 增強查詢的回應品質優於純文字查詢\n- 確認系統在無圖片時能正確降級為純文字查詢\n\n**測試方法（可重現、可驗證 VLM 流程是否真的被走到）：**\n\n核心判準：VLM enhanced query 需要在檢索 prompt 中出現 &#96;Image Path: ...&#96;，系統才會抓圖、轉 base64、以 multimodal messages 呼叫 &#96;vision_model_func&#96;。\n\n1. **準備含圖片的測試資料**：\n   - 可直接用 repo 現有圖檔：&#96;eval/docs/全球資訊服務暨軟體市場規模.png&#96;、&#96;eval/docs/cat-01.jpg&#96; 等\n\n2. **執行測試腳本（同題目 VLM on/off 對照）**：\n\n   &#96;&#96;&#96;powershell\n   uv run eval/integration/vlm_enhanced_query_eval.py &#96;\n     --docs eval/docs/全球資訊服務暨軟體市場規模.png eval/docs/cat-01.jpg &#96;\n     --working-dir eval/integration/rag_storage_vlm_eval &#96;\n     --out-dir eval/integration/results &#96;\n     --mode hybrid\n   &#96;&#96;&#96;\n\n3. **產出物與驗證點**：\n   - &#96;eval/integration/results/vlm_enhanced_query/&lt;timestamp&gt;/results.json&#96;\n     ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "- 適合丟 Excel/BI 做圖表",
            "lineNumber": 296
          },
          {
            "lineNumber": 297
          },
          {
            "text": "4. **如何判讀結果**：",
            "lineNumber": 298
          },
          {
            "text": "   - **平均分數（avg_score）**：越高越接近 ground truth",
            "lineNumber": 299
          },
          {
            "text": "   - **平均耗時（avg_elapsed_s）**：越低越好",
            "lineNumber": 300
          },
          {
            "text": "   - 建議再按 &#96;query_type&#96; 分組（事實/解釋/摘要），看 hybrid 在複雜題是否優勢更明顯",
            "lineNumber": 301
          },
          {
            "lineNumber": 302
          },
          {
            "text": "**驗收標準：**",
            "lineNumber": 303
          },
          {
            "text": "- Hybrid 模式在複雜查詢上的相似度分數 ≥ 0.75",
            "lineNumber": 304
          },
          {
            "text": "- Hybrid 模式優於單一模式（naive/local/global）的查詢比例 ≥ 70%",
            "lineNumber": 305
          },
          {
            "text": "- 各模式的回應時間在可接受範圍內（&lt; 30 秒）",
            "lineNumber": 306
          },
          {
            "text": "- 檢索到的上下文相關性 ≥ 80%",
            "lineNumber": 307
          },
          {
            "lineNumber": 308
          },
          {
            "text": "---",
            "lineNumber": 309
          },
          {
            "lineNumber": 310
          },
          {
            "text": "#### 6.1.3 VLM 增強查詢功能測試",
            "lineNumber": 311
          },
          {
            "lineNumber": 312
          },
          {
            "text": "**測試目標：**",
            "lineNumber": 313
          },
          {
            "text": "- 驗證 VLM（Vision Language Model）能正確分析檢索到的圖片",
            "lineNumber": 314
          },
          {
            "text": "- 確認圖片路徑能正確轉換為 base64 編碼並傳送給 VLM",
            "lineNumber": 315
          },
          {
            "text": "- 驗證 VLM 增強查詢的回應品質優於純文字查詢",
            "lineNumber": 316
          },
          {
            "text": "- 確認系統在無圖片時能正確降級為純文字查詢",
            "lineNumber": 317
          },
          {
            "lineNumber": 318
          },
          {
            "text": "**測試方法（可重現、可驗證 VLM 流程是否真的被走到）：**",
            "lineNumber": 319
          },
          {
            "lineNumber": 320
          },
          {
            "text": "核心判準：VLM enhanced query 需要在檢索 prompt 中出現 &#96;Image Path: ...&#96;，系統才會抓圖、轉 base64、以 multimodal messages 呼叫 &#96;vision_model_func&#96;。",
            "lineNumber": 321
          },
          {
            "lineNumber": 322
          },
          {
            "text": "1. **準備含圖片的測試資料**：",
            "lineNumber": 323
          },
          {
            "text": "   - 可直接用 repo 現有圖檔：&#96;eval/docs/全球資訊服務暨軟體市場規模.png&#96;、&#96;eval/docs/cat-01.jpg&#96; 等",
            "lineNumber": 324
          },
          {
            "lineNumber": 325
          },
          {
            "text": "2. **執行測試腳本（同題目 VLM on/off 對照）**：",
            "lineNumber": 326
          },
          {
            "lineNumber": 327
          },
          {
            "text": "   &#96;&#96;&#96;powershell",
            "lineNumber": 328
          },
          {
            "text": "   uv run eval/integration/vlm_enhanced_query_eval.py &#96;",
            "lineNumber": 329
          },
          {
            "text": "     --docs eval/docs/全球資訊服務暨軟體市場規模.png eval/docs/cat-01.jpg &#96;",
            "lineNumber": 330
          },
          {
            "text": "     --working-dir eval/integration/rag_storage_vlm_eval &#96;",
            "lineNumber": 331
          },
          {
            "text": "     --out-dir eval/integration/results &#96;",
            "lineNumber": 332
          },
          {
            "text": "     --mode hybrid",
            "lineNumber": 333
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 334
          },
          {
            "lineNumber": 335
          },
          {
            "text": "3. **產出物與驗證點**：",
            "lineNumber": 336
          },
          {
            "text": "   - &#96;eval/integration/results/vlm_enhanced_query/&lt;timestamp&gt;/results.json&#96;",
            "lineNumber": 337
          },
          {
            "text": "     ",
            "lineNumber": 338
          }
        ]
      },
      "score": 0.2545863389968872
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {
            "line": 137,
            "column": 60
          },
          "endPosition": {
            "line": 237
          }
        },
        "contents": "async def main(args: argparse.Namespace) -&gt; int:\n    _try_load_dotenv()\n\n    docs = [Path(p).resolve() for p in args.docs]\n    for p in docs:\n        if not p.exists():\n            raise FileNotFoundError(str(p))\n\n    llm_model_func, vision_model_func, embedding_func = _build_llm_vision_embedding()\n\n    cfg = RAGAnythingConfig(\n        working_dir=str(Path(args.working_dir).resolve()),\n        parser=os.getenv(\"PARSER\", \"mineru\"),\n        parse_method=\"auto\",\n        enable_image_processing=True,\n        enable_table_processing=True,\n        enable_equation_processing=True,\n    )\n\n    rag = RAGAnything(\n        config=cfg,\n        llm_model_func=llm_model_func,\n        vision_model_func=vision_model_func,\n        embedding_func=embedding_func,\n    )\n\n    output_dir = Path(args.output_dir).resolve()\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Skip processing if storage exists\n    wd = Path(cfg.working_dir)\n    if not (wd.exists() and any(wd.iterdir())):\n        for doc in docs:\n            await rag.process_document_complete(\n                file_path=str(doc),\n                output_dir=str(output_dir),\n                parse_method=\"auto\",\n            )\n\n    queries = [\n        \"請描述文件/圖片中的主要內容重點。\",\n        \"如果是圖表，請說明它呈現的主題與可能的趨勢。\",\n        \"若畫面有標題或重要文字，請摘要出來。\",\n    ]\n\n    run_dir = Path(args.out_dir).resolve() / \"vlm_enhanced_query\" / _now_tag()\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    results: Dict[str, Any] = {\n        \"meta\":",
        "signatures": {},
        "detailedLines": [
          {
            "text": "async def main(args: argparse.Namespace) -&gt; int:",
            "lineNumber": 141,
            "isSignature": true
          },
          {
            "text": "    _try_load_dotenv()",
            "lineNumber": 142
          },
          {
            "lineNumber": 143
          },
          {
            "text": "    docs = [Path(p).resolve() for p in args.docs]",
            "lineNumber": 144
          },
          {
            "text": "    for p in docs:",
            "lineNumber": 145
          },
          {
            "text": "        if not p.exists():",
            "lineNumber": 146
          },
          {
            "text": "            raise FileNotFoundError(str(p))",
            "lineNumber": 147
          },
          {
            "lineNumber": 148
          },
          {
            "text": "    llm_model_func, vision_model_func, embedding_func = _build_llm_vision_embedding()",
            "lineNumber": 149
          },
          {
            "lineNumber": 150
          },
          {
            "text": "    cfg = RAGAnythingConfig(",
            "lineNumber": 151
          },
          {
            "text": "        working_dir=str(Path(args.working_dir).resolve()),",
            "lineNumber": 152
          },
          {
            "text": "        parser=os.getenv(\"PARSER\", \"mineru\"),",
            "lineNumber": 153
          },
          {
            "text": "        parse_method=\"auto\",",
            "lineNumber": 154
          },
          {
            "text": "        enable_image_processing=True,",
            "lineNumber": 155
          },
          {
            "text": "        enable_table_processing=True,",
            "lineNumber": 156
          },
          {
            "text": "        enable_equation_processing=True,",
            "lineNumber": 157
          },
          {
            "text": "    )",
            "lineNumber": 158
          },
          {
            "lineNumber": 159
          },
          {
            "text": "    rag = RAGAnything(",
            "lineNumber": 160
          },
          {
            "text": "        config=cfg,",
            "lineNumber": 161
          },
          {
            "text": "        llm_model_func=llm_model_func,",
            "lineNumber": 162
          },
          {
            "text": "        vision_model_func=vision_model_func,",
            "lineNumber": 163
          },
          {
            "text": "        embedding_func=embedding_func,",
            "lineNumber": 164
          },
          {
            "text": "    )",
            "lineNumber": 165
          },
          {
            "lineNumber": 166
          },
          {
            "text": "    output_dir = Path(args.output_dir).resolve()",
            "lineNumber": 167
          },
          {
            "text": "    output_dir.mkdir(parents=True, exist_ok=True)",
            "lineNumber": 168
          },
          {
            "lineNumber": 169
          },
          {
            "text": "    # Skip processing if storage exists",
            "lineNumber": 170
          },
          {
            "text": "    wd = Path(cfg.working_dir)",
            "lineNumber": 171
          },
          {
            "text": "    if not (wd.exists() and any(wd.iterdir())):",
            "lineNumber": 172
          },
          {
            "text": "        for doc in docs:",
            "lineNumber": 173
          },
          {
            "text": "            await rag.process_document_complete(",
            "lineNumber": 174
          },
          {
            "text": "                file_path=str(doc),",
            "lineNumber": 175
          },
          {
            "text": "                output_dir=str(output_dir),",
            "lineNumber": 176
          },
          {
            "text": "                parse_method=\"auto\",",
            "lineNumber": 177
          },
          {
            "text": "            )",
            "lineNumber": 178
          },
          {
            "lineNumber": 179
          },
          {
            "text": "    queries = [",
            "lineNumber": 180
          },
          {
            "text": "        \"請描述文件/圖片中的主要內容重點。\",",
            "lineNumber": 181
          },
          {
            "text": "        \"如果是圖表，請說明它呈現的主題與可能的趨勢。\",",
            "lineNumber": 182
          },
          {
            "text": "        \"若畫面有標題或重要文字，請摘要出來。\",",
            "lineNumber": 183
          },
          {
            "text": "    ]",
            "lineNumber": 184
          },
          {
            "lineNumber": 185
          },
          {
            "text": "    run_dir = Path(args.out_dir).resolve() / \"vlm_enhanced_query\" / _now_tag()",
            "lineNumber": 186
          },
          {
            "text": "    run_dir.mkdir(parents=True, exist_ok=True)",
            "lineNumber": 187
          },
          {
            "lineNumber": 188
          },
          {
            "text": "    results: Dict[str, Any] = {",
            "lineNumber": 189
          },
          {
            "text": "        \"meta\":",
            "lineNumber": 190
          }
        ]
      },
      "score": 0.2540605068206787
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\eval\\integration\\vlm_enhanced_query_eval.py",
        "range": {
          "startPosition": {
            "line": 56
          },
          "endPosition": {
            "line": 237
          }
        },
        "contents": "def _build_llm_vision_embedding():\ndef vision_model_func(\n        prompt,\n        system_prompt=None,\n        history_messages=None,\n        image_data=None,\n        messages=None,\n        **kwargs,\n    ):\n        # VLM enhanced query will pass messages (multimodal format) when images are found.\n\n                **kwargs,\n            )\n        if image_data:\n            # Single image format (used by image processor)\n            return openai_complete_if_cache(\n                vlm_model,\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": prompt},\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                            },\n                        ],\n                    },\n                ],\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        return llm_model_func(prompt, system_prompt, history_messages or [], **kwargs)\n\n    emb_model = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n    emb_dim = int(os.getenv(\"EMBEDDING_DIM\", \"1536\"))\n\n    embedding_func = EmbeddingFunc(\n        embedding_dim=emb_dim,\n        max_token_size=8192,\n        func=lambda texts: openai_embed(\n            texts,\n            model=emb_model,\n            api_key=api_key,\n            base_url=base_url,\n        ),\n    )\n\n    return llm_model_func, vision_model_func, embedding_func\n\n\nasync def main(args: argparse.Namespace) -&gt; int:\n    _try_load_dotenv()",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 57,
                "column": 1
              },
              "endPosition": {
                "line": 58,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 78,
                "column": 5
              },
              "endPosition": {
                "line": 87,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def _build_llm_vision_embedding():",
            "lineNumber": 57,
            "isSignature": true
          },
          {
            "text": "def vision_model_func(",
            "lineNumber": 78
          },
          {
            "text": "        prompt,",
            "lineNumber": 79
          },
          {
            "text": "        system_prompt=None,",
            "lineNumber": 80
          },
          {
            "text": "        history_messages=None,",
            "lineNumber": 81
          },
          {
            "text": "        image_data=None,",
            "lineNumber": 82
          },
          {
            "text": "        messages=None,",
            "lineNumber": 83
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 84
          },
          {
            "text": "    ):",
            "lineNumber": 85
          },
          {
            "text": "        # VLM enhanced query will pass messages (multimodal format) when images are found.",
            "lineNumber": 86
          },
          {
            "lineNumber": 95
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 96
          },
          {
            "text": "            )",
            "lineNumber": 97
          },
          {
            "text": "        if image_data:",
            "lineNumber": 98
          },
          {
            "text": "            # Single image format (used by image processor)",
            "lineNumber": 99
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 100
          },
          {
            "text": "                vlm_model,",
            "lineNumber": 101
          },
          {
            "text": "                \"\",",
            "lineNumber": 102
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 103
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 104
          },
          {
            "text": "                messages=[",
            "lineNumber": 105
          },
          {
            "text": "                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,",
            "lineNumber": 106
          },
          {
            "text": "                    {",
            "lineNumber": 107
          },
          {
            "text": "                        \"role\": \"user\",",
            "lineNumber": 108
          },
          {
            "text": "                        \"content\": [",
            "lineNumber": 109
          },
          {
            "text": "                            {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 110
          },
          {
            "text": "                            {",
            "lineNumber": 111
          },
          {
            "text": "                                \"type\": \"image_url\",",
            "lineNumber": 112
          },
          {
            "text": "                                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},",
            "lineNumber": 113
          },
          {
            "text": "                            },",
            "lineNumber": 114
          },
          {
            "text": "                        ],",
            "lineNumber": 115
          },
          {
            "text": "                    },",
            "lineNumber": 116
          },
          {
            "text": "                ],",
            "lineNumber": 117
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 118
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 119
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 120
          },
          {
            "text": "            )",
            "lineNumber": 121
          },
          {
            "text": "        return llm_model_func(prompt, system_prompt, history_messages or [], **kwargs)",
            "lineNumber": 122
          },
          {
            "lineNumber": 123
          },
          {
            "text": "    emb_model = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")",
            "lineNumber": 124
          },
          {
            "text": "    emb_dim = int(os.getenv(\"EMBEDDING_DIM\", \"1536\"))",
            "lineNumber": 125
          },
          {
            "lineNumber": 126
          },
          {
            "text": "    embedding_func = EmbeddingFunc(",
            "lineNumber": 127
          },
          {
            "text": "        embedding_dim=emb_dim,",
            "lineNumber": 128
          },
          {
            "text": "        max_token_size=8192,",
            "lineNumber": 129
          },
          {
            "text": "        func=lambda texts: openai_embed(",
            "lineNumber": 130
          },
          {
            "text": "            texts,",
            "lineNumber": 131
          },
          {
            "text": "            model=emb_model,",
            "lineNumber": 132
          },
          {
            "text": "            api_key=api_key,",
            "lineNumber": 133
          },
          {
            "text": "            base_url=base_url,",
            "lineNumber": 134
          },
          {
            "text": "        ),",
            "lineNumber": 135
          },
          {
            "text": "    )",
            "lineNumber": 136
          },
          {
            "lineNumber": 137
          },
          {
            "text": "    return llm_model_func, vision_model_func, embedding_func",
            "lineNumber": 138,
            "isSignature": true
          },
          {
            "lineNumber": 139
          },
          {
            "lineNumber": 140
          },
          {
            "text": "async def main(args: argparse.Namespace) -&gt; int:",
            "lineNumber": 141,
            "isSignature": true
          },
          {
            "text": "    _try_load_dotenv()",
            "lineNumber": 142
          }
        ]
      },
      "score": 0.24390053749084473
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README.md",
        "range": {
          "startPosition": {
            "line": 800
          },
          "endPosition": {
            "line": 854
          }
        },
        "contents": "    )\n\n    # Define model functions\n    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):\n        return openai_complete_if_cache(\n            \"gpt-4o-mini\",\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            api_key=api_key,\n            base_url=base_url,\n            **kwargs,\n        )\n\n    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):\n        # If messages format is provided (for multimodal VLM enhanced query), use it directly\n        if messages:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=messages,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # Traditional single image format\n        elif image_data:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": prompt},\n                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}}\n                        ],\n                    } if image_data else {\"role\": \"user\", \"content\": prompt},\n                ],\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # Pure text format\n        else:\n            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)\n\n    embedding_func = EmbeddingFunc(\n        embedding_dim=3072,",
        "signatures": {},
        "detailedLines": [
          {
            "text": "    )",
            "lineNumber": 801
          },
          {
            "lineNumber": 802
          },
          {
            "text": "    # Define model functions",
            "lineNumber": 803
          },
          {
            "text": "    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):",
            "lineNumber": 804
          },
          {
            "text": "        return openai_complete_if_cache(",
            "lineNumber": 805
          },
          {
            "text": "            \"gpt-4o-mini\",",
            "lineNumber": 806
          },
          {
            "text": "            prompt,",
            "lineNumber": 807
          },
          {
            "text": "            system_prompt=system_prompt,",
            "lineNumber": 808
          },
          {
            "text": "            history_messages=history_messages,",
            "lineNumber": 809
          },
          {
            "text": "            api_key=api_key,",
            "lineNumber": 810
          },
          {
            "text": "            base_url=base_url,",
            "lineNumber": 811
          },
          {
            "text": "            **kwargs,",
            "lineNumber": 812
          },
          {
            "text": "        )",
            "lineNumber": 813
          },
          {
            "lineNumber": 814
          },
          {
            "text": "    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):",
            "lineNumber": 815
          },
          {
            "text": "        # If messages format is provided (for multimodal VLM enhanced query), use it directly",
            "lineNumber": 816
          },
          {
            "text": "        if messages:",
            "lineNumber": 817
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 818
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 819
          },
          {
            "text": "                \"\",",
            "lineNumber": 820
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 821
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 822
          },
          {
            "text": "                messages=messages,",
            "lineNumber": 823
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 824
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 825
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 826
          },
          {
            "text": "            )",
            "lineNumber": 827
          },
          {
            "text": "        # Traditional single image format",
            "lineNumber": 828
          },
          {
            "text": "        elif image_data:",
            "lineNumber": 829
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 830
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 831
          },
          {
            "text": "                \"\",",
            "lineNumber": 832
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 833
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 834
          },
          {
            "text": "                messages=[",
            "lineNumber": 835
          },
          {
            "text": "                    {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,",
            "lineNumber": 836
          },
          {
            "text": "                    {",
            "lineNumber": 837
          },
          {
            "text": "                        \"role\": \"user\",",
            "lineNumber": 838
          },
          {
            "text": "                        \"content\": [",
            "lineNumber": 839
          },
          {
            "text": "                            {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 840
          },
          {
            "text": "                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}}",
            "lineNumber": 841
          },
          {
            "text": "                        ],",
            "lineNumber": 842
          },
          {
            "text": "                    } if image_data else {\"role\": \"user\", \"content\": prompt},",
            "lineNumber": 843
          },
          {
            "text": "                ],",
            "lineNumber": 844
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 845
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 846
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 847
          },
          {
            "text": "            )",
            "lineNumber": 848
          },
          {
            "text": "        # Pure text format",
            "lineNumber": 849
          },
          {
            "text": "        else:",
            "lineNumber": 850
          },
          {
            "text": "            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)",
            "lineNumber": 851
          },
          {
            "lineNumber": 852
          },
          {
            "text": "    embedding_func = EmbeddingFunc(",
            "lineNumber": 853
          },
          {
            "text": "        embedding_dim=3072,",
            "lineNumber": 854
          }
        ]
      },
      "score": 0.23884190618991852
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README.md",
        "range": {
          "startPosition": {
            "line": 703
          },
          "endPosition": {
            "line": 756
          }
        },
        "contents": "    await initialize_pipeline_status()\n\n    # Define vision model function for image processing\n    def vision_model_func(\n        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs\n    ):\n        # If messages format is provided (for multimodal VLM enhanced query), use it directly\n        if messages:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=messages,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # Traditional single image format\n        elif image_data:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt}\n                    if system_prompt\n                    else None,\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": prompt},\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{image_data}\"\n                                },\n                            },\n                        ],\n                    }\n                    if image_data\n                    else {\"role\": \"user\", \"content\": prompt},\n                ],\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # Pure text format\n        else:\n            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)\n\n    # Now use existing LightRAG instance to initialize RAGAnything",
        "signatures": {},
        "detailedLines": [
          {
            "text": "    await initialize_pipeline_status()",
            "lineNumber": 704
          },
          {
            "lineNumber": 705
          },
          {
            "text": "    # Define vision model function for image processing",
            "lineNumber": 706
          },
          {
            "text": "    def vision_model_func(",
            "lineNumber": 707
          },
          {
            "text": "        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs",
            "lineNumber": 708
          },
          {
            "text": "    ):",
            "lineNumber": 709
          },
          {
            "text": "        # If messages format is provided (for multimodal VLM enhanced query), use it directly",
            "lineNumber": 710
          },
          {
            "text": "        if messages:",
            "lineNumber": 711
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 712
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 713
          },
          {
            "text": "                \"\",",
            "lineNumber": 714
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 715
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 716
          },
          {
            "text": "                messages=messages,",
            "lineNumber": 717
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 718
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 719
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 720
          },
          {
            "text": "            )",
            "lineNumber": 721
          },
          {
            "text": "        # Traditional single image format",
            "lineNumber": 722
          },
          {
            "text": "        elif image_data:",
            "lineNumber": 723
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 724
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 725
          },
          {
            "text": "                \"\",",
            "lineNumber": 726
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 727
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 728
          },
          {
            "text": "                messages=[",
            "lineNumber": 729
          },
          {
            "text": "                    {\"role\": \"system\", \"content\": system_prompt}",
            "lineNumber": 730
          },
          {
            "text": "                    if system_prompt",
            "lineNumber": 731
          },
          {
            "text": "                    else None,",
            "lineNumber": 732
          },
          {
            "text": "                    {",
            "lineNumber": 733
          },
          {
            "text": "                        \"role\": \"user\",",
            "lineNumber": 734
          },
          {
            "text": "                        \"content\": [",
            "lineNumber": 735
          },
          {
            "text": "                            {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 736
          },
          {
            "text": "                            {",
            "lineNumber": 737
          },
          {
            "text": "                                \"type\": \"image_url\",",
            "lineNumber": 738
          },
          {
            "text": "                                \"image_url\": {",
            "lineNumber": 739
          },
          {
            "text": "                                    \"url\": f\"data:image/jpeg;base64,{image_data}\"",
            "lineNumber": 740
          },
          {
            "text": "                                },",
            "lineNumber": 741
          },
          {
            "text": "                            },",
            "lineNumber": 742
          },
          {
            "text": "                        ],",
            "lineNumber": 743
          },
          {
            "text": "                    }",
            "lineNumber": 744
          },
          {
            "text": "                    if image_data",
            "lineNumber": 745
          },
          {
            "text": "                    else {\"role\": \"user\", \"content\": prompt},",
            "lineNumber": 746
          },
          {
            "text": "                ],",
            "lineNumber": 747
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 748
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 749
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 750
          },
          {
            "text": "            )",
            "lineNumber": 751
          },
          {
            "text": "        # Pure text format",
            "lineNumber": 752
          },
          {
            "text": "        else:",
            "lineNumber": 753
          },
          {
            "text": "            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)",
            "lineNumber": 754
          },
          {
            "lineNumber": 755
          },
          {
            "text": "    # Now use existing LightRAG instance to initialize RAGAnything",
            "lineNumber": 756
          }
        ]
      },
      "score": 0.2319202423095703
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\modalprocessors.py",
        "range": {
          "startPosition": {
            "line": 749
          },
          "endPosition": {
            "line": 1032
          }
        },
        "contents": "class ImageModalProcessor(BaseModalProcessor):\nasync def process_multimodal_content(\n        self,\n        modal_content,\n        content_type: str,\n        file_path: str = \"manual_creation\",\n        entity_name: str = None,\n        item_info: Dict[str, Any] = None,\n        batch_mode: bool = False,\n        doc_id: str = None,\n        chunk_order_index: int = 0,\n    ) -&gt; Tuple[str, Dict[str, Any]]:\n\n        try:\n            # Generate description and entity info\n            enhanced_caption, entity_info = await self.generate_description_only(\n                modal_content, content_type, item_info, entity_name\n            )\n\n            # Build complete image content\n            if isinstance(modal_content, str):\n                try:\n                    content_data = json.loads(modal_content)\n                except json.JSONDecodeError:\n                    content_data = {\"description\": modal_content}\n            else:\n                content_data = modal_content\n\n            image_path = content_data.get(\"img_path\", \"\")\n            captions = content_data.get(\"image_caption\", content_data.get(\"img_caption\", []))\n            footnotes = content_data.get(\"image_footnote\", content_data.get(\"img_footnote\", []))\n\n            modal_chunk = PROMPTS[\"image_chunk\"].format(\n                image_path=image_path,\n                captions=\", \".join(captions) if captions else \"None\",\n                footnotes=\", \".join(footnotes) if footnotes else \"None\",\n                enhanced_caption=enhanced_caption,\n            )\n\n            return await self._create_entity_and_chunk(\n                modal_chunk,\n                entity_info,\n                file_path,\n                batch_mode,\n                doc_id,\n                chunk_order_index,\n            )\n\n        except Exception as e:\n            logger.error(f\"Error processing image content: {e}\")\n            # Fallback processing\n            fallback_entity = {\n                \"entity_name\": entity_name\n                if entity_name\n                else f\"image_{compute_mdhash_id(str(modal_content))}\",\n                \"entity_type\": \"image\",\n                \"summary\": f\"Image content: {str(modal_content)[:100]}\",\n            }\n            return str(modal_content), fallback_entity\n\n    def _parse_response(self, response: str, entity_name: str = None) -&gt; Tuple[str, Dict[str, Any]]:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 750,
                "column": 1
              },
              "endPosition": {
                "line": 751,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 842,
                "column": 5
              },
              "endPosition": {
                "line": 849,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class ImageModalProcessor(BaseModalProcessor):",
            "lineNumber": 750,
            "isSignature": true
          },
          {
            "text": "async def process_multimodal_content(",
            "lineNumber": 942
          },
          {
            "text": "        self,",
            "lineNumber": 943
          },
          {
            "text": "        modal_content,",
            "lineNumber": 944
          },
          {
            "text": "        content_type: str,",
            "lineNumber": 945
          },
          {
            "text": "        file_path: str = \"manual_creation\",",
            "lineNumber": 946
          },
          {
            "text": "        entity_name: str = None,",
            "lineNumber": 947
          },
          {
            "text": "        item_info: Dict[str, Any] = None,",
            "lineNumber": 948
          },
          {
            "text": "        batch_mode: bool = False,",
            "lineNumber": 949
          },
          {
            "text": "        doc_id: str = None,",
            "lineNumber": 950
          },
          {
            "text": "        chunk_order_index: int = 0,",
            "lineNumber": 951
          },
          {
            "text": "    ) -&gt; Tuple[str, Dict[str, Any]]:",
            "lineNumber": 952
          },
          {
            "lineNumber": 953
          },
          {
            "text": "        try:",
            "lineNumber": 954
          },
          {
            "text": "            # Generate description and entity info",
            "lineNumber": 955
          },
          {
            "text": "            enhanced_caption, entity_info = await self.generate_description_only(",
            "lineNumber": 956
          },
          {
            "text": "                modal_content, content_type, item_info, entity_name",
            "lineNumber": 957
          },
          {
            "text": "            )",
            "lineNumber": 958
          },
          {
            "lineNumber": 959
          },
          {
            "text": "            # Build complete image content",
            "lineNumber": 960
          },
          {
            "text": "            if isinstance(modal_content, str):",
            "lineNumber": 961
          },
          {
            "text": "                try:",
            "lineNumber": 962
          },
          {
            "text": "                    content_data = json.loads(modal_content)",
            "lineNumber": 963
          },
          {
            "text": "                except json.JSONDecodeError:",
            "lineNumber": 964
          },
          {
            "text": "                    content_data = {\"description\": modal_content}",
            "lineNumber": 965
          },
          {
            "text": "            else:",
            "lineNumber": 966
          },
          {
            "text": "                content_data = modal_content",
            "lineNumber": 967
          },
          {
            "lineNumber": 968
          },
          {
            "text": "            image_path = content_data.get(\"img_path\", \"\")",
            "lineNumber": 969
          },
          {
            "text": "            captions = content_data.get(\"image_caption\", content_data.get(\"img_caption\", []))",
            "lineNumber": 970
          },
          {
            "text": "            footnotes = content_data.get(\"image_footnote\", content_data.get(\"img_footnote\", []))",
            "lineNumber": 971
          },
          {
            "lineNumber": 972
          },
          {
            "text": "            modal_chunk = PROMPTS[\"image_chunk\"].format(",
            "lineNumber": 973
          },
          {
            "text": "                image_path=image_path,",
            "lineNumber": 974
          },
          {
            "text": "                captions=\", \".join(captions) if captions else \"None\",",
            "lineNumber": 975
          },
          {
            "text": "                footnotes=\", \".join(footnotes) if footnotes else \"None\",",
            "lineNumber": 976
          },
          {
            "text": "                enhanced_caption=enhanced_caption,",
            "lineNumber": 977
          },
          {
            "text": "            )",
            "lineNumber": 978
          },
          {
            "lineNumber": 979
          },
          {
            "text": "            return await self._create_entity_and_chunk(",
            "lineNumber": 980
          },
          {
            "text": "                modal_chunk,",
            "lineNumber": 981
          },
          {
            "text": "                entity_info,",
            "lineNumber": 982
          },
          {
            "text": "                file_path,",
            "lineNumber": 983
          },
          {
            "text": "                batch_mode,",
            "lineNumber": 984
          },
          {
            "text": "                doc_id,",
            "lineNumber": 985
          },
          {
            "text": "                chunk_order_index,",
            "lineNumber": 986
          },
          {
            "text": "            )",
            "lineNumber": 987
          },
          {
            "lineNumber": 988
          },
          {
            "text": "        except Exception as e:",
            "lineNumber": 989
          },
          {
            "text": "            logger.error(f\"Error processing image content: {e}\")",
            "lineNumber": 990
          },
          {
            "text": "            # Fallback processing",
            "lineNumber": 991
          },
          {
            "text": "            fallback_entity = {",
            "lineNumber": 992
          },
          {
            "text": "                \"entity_name\": entity_name",
            "lineNumber": 993
          },
          {
            "text": "                if entity_name",
            "lineNumber": 994
          },
          {
            "text": "                else f\"image_{compute_mdhash_id(str(modal_content))}\",",
            "lineNumber": 995
          },
          {
            "text": "                \"entity_type\": \"image\",",
            "lineNumber": 996
          },
          {
            "text": "                \"summary\": f\"Image content: {str(modal_content)[:100]}\",",
            "lineNumber": 997
          },
          {
            "text": "            }",
            "lineNumber": 998
          },
          {
            "text": "            return str(modal_content), fallback_entity",
            "lineNumber": 999
          },
          {
            "lineNumber": 1000
          },
          {
            "text": "    def _parse_response(self, response: str, entity_name: str = None) -&gt; Tuple[str, Dict[str, Any]]:",
            "lineNumber": 1001
          },
          {
            "text": "        ",
            "lineNumber": 1002
          }
        ]
      },
      "score": 0.23094916343688965
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\raganything\\modalprocessors.py",
        "range": {
          "startPosition": {
            "line": 749
          },
          "endPosition": {
            "line": 1032
          }
        },
        "contents": "class ImageModalProcessor(BaseModalProcessor):\ndef _check_func_supports_image_data(self, func) -&gt; bool:\n\n            return False\n\n    async def _call_vision_model_safe(\n        self, prompt: str, image_data: str = None, system_prompt: str = None, **kwargs\n    ):\n        \"\"\"Safely call vision model function, handling image_data parameter\"\"\"\n        if not image_data:\n            # No image data, call normally\n            return await self.modal_caption_func(prompt, system_prompt=system_prompt, **kwargs)\n\n        # Check if function supports image_data parameter\n        if not self._check_func_supports_image_data(self.modal_caption_func):\n            # Function doesn't support image_data, convert to messages format\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": prompt},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                        },\n                    ],\n                }\n            )\n            # Try calling with messages parameter\n            try:\n                sig = inspect.signature(self.modal_caption_func)\n                if \"messages\" in sig.parameters:\n                    # Remove image_data from kwargs to avoid passing it to underlying API\n                    kwargs_clean = {k: v for k, v in kwargs.items() if k != \"image_data\"}\n                    return await self.modal_caption_func(\n                        prompt=\"\", messages=messages, system_prompt=None, **kwargs_clean\n                    )\n            except (ValueError, TypeError):\n                pass\n            # If messages doesn't work, log warning and fallback to text-only\n            logger.warning(\n                \"",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 750,
                "column": 1
              },
              "endPosition": {
                "line": 751,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class ImageModalProcessor(BaseModalProcessor):",
            "lineNumber": 750,
            "isSignature": true
          },
          {
            "text": "def _check_func_supports_image_data(self, func) -&gt; bool:",
            "lineNumber": 768
          },
          {
            "lineNumber": 774
          },
          {
            "text": "            return False",
            "lineNumber": 775
          },
          {
            "lineNumber": 776
          },
          {
            "text": "    async def _call_vision_model_safe(",
            "lineNumber": 777
          },
          {
            "text": "        self, prompt: str, image_data: str = None, system_prompt: str = None, **kwargs",
            "lineNumber": 778
          },
          {
            "text": "    ):",
            "lineNumber": 779
          },
          {
            "text": "        \"\"\"Safely call vision model function, handling image_data parameter\"\"\"",
            "lineNumber": 780
          },
          {
            "text": "        if not image_data:",
            "lineNumber": 781
          },
          {
            "text": "            # No image data, call normally",
            "lineNumber": 782
          },
          {
            "text": "            return await self.modal_caption_func(prompt, system_prompt=system_prompt, **kwargs)",
            "lineNumber": 783
          },
          {
            "lineNumber": 784
          },
          {
            "text": "        # Check if function supports image_data parameter",
            "lineNumber": 785
          },
          {
            "text": "        if not self._check_func_supports_image_data(self.modal_caption_func):",
            "lineNumber": 786
          },
          {
            "text": "            # Function doesn't support image_data, convert to messages format",
            "lineNumber": 787
          },
          {
            "text": "            messages = []",
            "lineNumber": 788
          },
          {
            "text": "            if system_prompt:",
            "lineNumber": 789
          },
          {
            "text": "                messages.append({\"role\": \"system\", \"content\": system_prompt})",
            "lineNumber": 790
          },
          {
            "text": "            messages.append(",
            "lineNumber": 791
          },
          {
            "text": "                {",
            "lineNumber": 792
          },
          {
            "text": "                    \"role\": \"user\",",
            "lineNumber": 793
          },
          {
            "text": "                    \"content\": [",
            "lineNumber": 794
          },
          {
            "text": "                        {\"type\": \"text\", \"text\": prompt},",
            "lineNumber": 795
          },
          {
            "text": "                        {",
            "lineNumber": 796
          },
          {
            "text": "                            \"type\": \"image_url\",",
            "lineNumber": 797
          },
          {
            "text": "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},",
            "lineNumber": 798
          },
          {
            "text": "                        },",
            "lineNumber": 799
          },
          {
            "text": "                    ],",
            "lineNumber": 800
          },
          {
            "text": "                }",
            "lineNumber": 801
          },
          {
            "text": "            )",
            "lineNumber": 802
          },
          {
            "text": "            # Try calling with messages parameter",
            "lineNumber": 803
          },
          {
            "text": "            try:",
            "lineNumber": 804
          },
          {
            "text": "                sig = inspect.signature(self.modal_caption_func)",
            "lineNumber": 805
          },
          {
            "text": "                if \"messages\" in sig.parameters:",
            "lineNumber": 806
          },
          {
            "text": "                    # Remove image_data from kwargs to avoid passing it to underlying API",
            "lineNumber": 807
          },
          {
            "text": "                    kwargs_clean = {k: v for k, v in kwargs.items() if k != \"image_data\"}",
            "lineNumber": 808
          },
          {
            "text": "                    return await self.modal_caption_func(",
            "lineNumber": 809
          },
          {
            "text": "                        prompt=\"\", messages=messages, system_prompt=None, **kwargs_clean",
            "lineNumber": 810
          },
          {
            "text": "                    )",
            "lineNumber": 811
          },
          {
            "text": "            except (ValueError, TypeError):",
            "lineNumber": 812
          },
          {
            "text": "                pass",
            "lineNumber": 813
          },
          {
            "text": "            # If messages doesn't work, log warning and fallback to text-only",
            "lineNumber": 814
          },
          {
            "text": "            logger.warning(",
            "lineNumber": 815
          },
          {
            "text": "                \"",
            "lineNumber": 816
          }
        ]
      },
      "score": 0.2207953929901123
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": ".\\README.md",
        "range": {
          "startPosition": {
            "line": 321
          },
          "endPosition": {
            "line": 355,
            "column": 16
          }
        },
        "contents": "        enable_equation_processing=True,\n    )\n\n    # Define LLM model function\n    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):\n        return openai_complete_if_cache(\n            \"gpt-4o-mini\",\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            api_key=api_key,\n            base_url=base_url,\n            **kwargs,\n        )\n\n    # Define vision model function for image processing\n    def vision_model_func(\n        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs\n    ):\n        # If messages format is provided (for multimodal VLM enhanced query), use it directly\n        if messages:\n            return openai_complete_if_cache(\n                \"gpt-4o\",\n                \"\",\n                system_prompt=None,\n                history_messages=[],\n                messages=messages,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs,\n            )\n        # Traditional single image format\n        elif image_data:\n            return openai_complete_if_cache(\n                ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "        enable_equation_processing=True,",
            "lineNumber": 322
          },
          {
            "text": "    )",
            "lineNumber": 323
          },
          {
            "lineNumber": 324
          },
          {
            "text": "    # Define LLM model function",
            "lineNumber": 325
          },
          {
            "text": "    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):",
            "lineNumber": 326
          },
          {
            "text": "        return openai_complete_if_cache(",
            "lineNumber": 327
          },
          {
            "text": "            \"gpt-4o-mini\",",
            "lineNumber": 328
          },
          {
            "text": "            prompt,",
            "lineNumber": 329
          },
          {
            "text": "            system_prompt=system_prompt,",
            "lineNumber": 330
          },
          {
            "text": "            history_messages=history_messages,",
            "lineNumber": 331
          },
          {
            "text": "            api_key=api_key,",
            "lineNumber": 332
          },
          {
            "text": "            base_url=base_url,",
            "lineNumber": 333
          },
          {
            "text": "            **kwargs,",
            "lineNumber": 334
          },
          {
            "text": "        )",
            "lineNumber": 335
          },
          {
            "lineNumber": 336
          },
          {
            "text": "    # Define vision model function for image processing",
            "lineNumber": 337
          },
          {
            "text": "    def vision_model_func(",
            "lineNumber": 338
          },
          {
            "text": "        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs",
            "lineNumber": 339
          },
          {
            "text": "    ):",
            "lineNumber": 340
          },
          {
            "text": "        # If messages format is provided (for multimodal VLM enhanced query), use it directly",
            "lineNumber": 341
          },
          {
            "text": "        if messages:",
            "lineNumber": 342
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 343
          },
          {
            "text": "                \"gpt-4o\",",
            "lineNumber": 344
          },
          {
            "text": "                \"\",",
            "lineNumber": 345
          },
          {
            "text": "                system_prompt=None,",
            "lineNumber": 346
          },
          {
            "text": "                history_messages=[],",
            "lineNumber": 347
          },
          {
            "text": "                messages=messages,",
            "lineNumber": 348
          },
          {
            "text": "                api_key=api_key,",
            "lineNumber": 349
          },
          {
            "text": "                base_url=base_url,",
            "lineNumber": 350
          },
          {
            "text": "                **kwargs,",
            "lineNumber": 351
          },
          {
            "text": "            )",
            "lineNumber": 352
          },
          {
            "text": "        # Traditional single image format",
            "lineNumber": 353
          },
          {
            "text": "        elif image_data:",
            "lineNumber": 354
          },
          {
            "text": "            return openai_complete_if_cache(",
            "lineNumber": 355
          },
          {
            "text": "                ",
            "lineNumber": 356
          }
        ]
      },
      "score": 0.21419215202331543
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---



---


撰寫驗收報告。


---

<tool-use data-tool-type="unknown" data-tool-name="edit_file_v2">
<details>
<summary>Tool use: **edit_file_v2**</summary>

Additional data:

```json
{
  "codeblockId": "97dad232-2c97-46af-b3f1-225d703578a7"
}
```
Status: **loading**


</details>
</tool-use>

---

