[{"id":1,"annotations":[{"id":4,"completed_by":1,"result":[{"value":{"start":0,"end":12,"text":"Transformer ","labels":["TECHNICAL_TERM"]},"id":"FRfhvTnb1Y","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":35,"end":45,"text":"Vaswani 等人","labels":["PERSON"]},"id":"eesgUxNT6k","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":47,"end":53,"text":"2017 年","labels":["DATE"]},"id":"Vx0vayiM49","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":139,"end":142,"text":"RNN","labels":["TECHNICAL_TERM"]},"id":"DkJItkpbZV","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":152,"end":155,"text":"CNN","labels":["TECHNICAL_TERM"]},"id":"3We4pF23-O","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":186,"end":191,"text":"注意力機制","labels":["TECHNICAL_TERM"]},"id":"_kJdFKG86O","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"from_id":"FRfhvTnb1Y","to_id":"eesgUxNT6k","type":"relation","direction":"right","labels":["proposes"]},{"from_id":"FRfhvTnb1Y","to_id":"Vx0vayiM49","type":"relation","direction":"right","labels":["proposes"]},{"from_id":"FRfhvTnb1Y","to_id":"_kJdFKG86O","type":"relation","direction":"right","labels":["contains"]}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-12-12T01:19:36.859409Z","updated_at":"2025-12-12T01:19:36.859425Z","draft_created_at":"2025-12-12T01:15:57.633318Z","lead_time":229.513,"prediction":{},"result_count":7,"unique_id":"fbf56602-6aad-4183-be59-4e259ed19260","import_id":null,"last_action":null,"bulk_created":false,"task":1,"project":1,"updated_by":1,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"6777d94c-test-01.txt","drafts":[],"predictions":[3],"data":{"text":"Transformer 模型是一種基於深度學習的神經網路架構，最初由 Vaswani 等人在 2017 年的論文《Attention Is All You Need》中提出。這種模型以其強大的序列建模能力而著稱，特別是在自然語言處理（NLP）任務中取得了突破性的成就。與之前的 RNN（遞歸神經網路）和 CNN（卷積神經網路）架構不同，Transformer 引入了全新的注意力機制，使其能更高效地處理長距離依賴問題，並且更適合大規模的並行計算。"},"meta":{},"created_at":"2025-12-11T08:25:59.719586Z","updated_at":"2025-12-12T01:19:36.891751Z","inner_id":1,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":1,"updated_by":1,"comment_authors":[]},{"id":2,"annotations":[{"id":5,"completed_by":1,"result":[{"value":{"start":164,"end":167,"text":"卷積層","labels":["SYSTEM_COMPONENT"]},"id":"YnbnbQDmuw","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-12-12T01:19:54.439723Z","updated_at":"2025-12-12T01:19:54.439739Z","draft_created_at":null,"lead_time":13.062,"prediction":{},"result_count":1,"unique_id":"4f425102-2f6e-47a2-b0b3-486fb2b8a1e0","import_id":null,"last_action":null,"bulk_created":false,"task":2,"project":1,"updated_by":1,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"6777d94c-test-01.txt","drafts":[],"predictions":[],"data":{"text":"在 Transformer 出現之前，RNN（如 LSTM 和 GRU）是處理序列數據的主要方法。然而，RNN 的結構本質上是序列化的，無法同時處理輸入數據，導致計算效率較低。此外，隨著序列長度增加，RNN 在捕捉長距依賴關係時可能會面臨梯度消失或梯度爆炸的問題。而 CNN 雖然具有並行計算能力，但在建模長距離依賴時需要大量的卷積層，導致計算成本高昂。Transformer 的出現解決了這些問題。其核心創新在於使用了「自注意力機制」（Self-Attention），該機制能夠直接建模序列中任意兩個位置之間的關係，並且支持高度並行的計算。"},"meta":{},"created_at":"2025-12-11T08:25:59.719665Z","updated_at":"2025-12-12T01:19:54.467062Z","inner_id":2,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":1,"updated_by":1,"comment_authors":[]},{"id":3,"annotations":[{"id":6,"completed_by":1,"result":[{"value":{"start":28,"end":35,"text":"Encoder","labels":["SYSTEM_COMPONENT"]},"id":"GRICQfDG-U","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":41,"end":48,"text":"Decoder","labels":["SYSTEM_COMPONENT"]},"id":"PDcMSTTTUH","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-12-12T01:20:06.301002Z","updated_at":"2025-12-12T01:20:06.301017Z","draft_created_at":null,"lead_time":8.87,"prediction":{},"result_count":2,"unique_id":"78049314-9e73-4e72-9d71-161ea0eb1f21","import_id":null,"last_action":null,"bulk_created":false,"task":3,"project":1,"updated_by":1,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"6777d94c-test-01.txt","drafts":[{"id":5,"user":"user@ccc.com","created_username":"user@ccc.com, 1","created_ago":"3 minutes","result":[{"value":{"start":28,"end":35,"text":"Encoder","labels":["SYSTEM_COMPONENT"]},"id":"GRICQfDG-U","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":41,"end":48,"text":"Decoder","labels":["SYSTEM_COMPONENT"]},"id":"PDcMSTTTUH","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":24,"end":27,"text":"編碼器","labels":["SYSTEM_COMPONENT"]},"id":"4h4bUVBq86","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":37,"end":40,"text":"解碼器","labels":["SYSTEM_COMPONENT"]},"id":"7a47talISp","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"lead_time":6.303,"was_postponed":false,"import_id":null,"created_at":"2025-12-12T01:20:17.809033Z","updated_at":"2025-12-12T01:20:17.809053Z","task":3,"annotation":6}],"predictions":[],"data":{"text":"Transformer 模型由兩個主要部分組成：編碼器（Encoder）和解碼器（Decoder）。這兩部分可以分別堆疊多層，用於處理不同類型的任務。"},"meta":{},"created_at":"2025-12-11T08:25:59.719696Z","updated_at":"2025-12-12T01:20:06.328792Z","inner_id":3,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":1,"updated_by":1,"comment_authors":[]},{"id":4,"annotations":[{"id":7,"completed_by":1,"result":[{"value":{"start":44,"end":47,"text":"嵌入層","labels":["SYSTEM_COMPONENT"]},"id":"2A3px5gCCf","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":93,"end":97,"text":"位置編碼","labels":["SYSTEM_COMPONENT"]},"id":"xcFR8yyLjw","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":325,"end":328,"text":"FFN","labels":["TECHNICAL_TERM"]},"id":"qIgurMfyrW","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":296,"end":302,"text":"前饋神經網路","labels":["TECHNICAL_TERM"]},"id":"ooCkKNcGrx","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-12-12T01:21:01.444977Z","updated_at":"2025-12-12T01:21:01.444995Z","draft_created_at":"2025-12-12T01:20:33.911028Z","lead_time":44.76,"prediction":{},"result_count":4,"unique_id":"baa92ff8-5c76-43a4-a89a-5114e42d922e","import_id":null,"last_action":null,"bulk_created":false,"task":4,"project":1,"updated_by":1,"parent_prediction":4,"parent_annotation":null,"last_created_by":null}],"file_upload":"6777d94c-test-01.txt","drafts":[],"predictions":[4],"data":{"text":"編碼器的主要作用是將輸入序列轉換為一組上下文豐富的特徵表示。其結構包括以下幾個核心部分：嵌入層（Embedding Layer）用於將輸入的離散符號（如詞語或字符）轉換為連續的向量表示。位置編碼（Positional Encoding）則因 Transformer 不具備像 RNN 那樣的順序處理特性，被引入以提供序列中各位置的位置信息，通常採用正弦和餘弦函數來生成位置編碼。自注意力機制（Self-Attention Mechanism）是 Transformer 的核心。對於每個輸入位置，模型通過計算該位置與序列中所有其他位置的相似度（即注意力分數），來獲得一個加權的輸入表示。此外，前饋神經網路（Feed-Forward Network, FFN）和殘差連接與層歸一化（Residual Connection and Layer Normalization）進一步穩定訓練過程，增強模型表現力。"},"meta":{},"created_at":"2025-12-11T08:25:59.719724Z","updated_at":"2025-12-12T01:21:01.484777Z","inner_id":4,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":1,"updated_by":1,"comment_authors":[]},{"id":5,"annotations":[{"id":8,"completed_by":1,"result":[{"value":{"start":49,"end":53,"text":"遮罩機制","labels":["SYSTEM_COMPONENT"]},"id":"ehiAool5jK","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":98,"end":103,"text":"多頭注意力","labels":["SYSTEM_COMPONENT"]},"id":"Ti05CDbgGg","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-12-12T01:21:25.060034Z","updated_at":"2025-12-12T01:21:25.060050Z","draft_created_at":"2025-12-12T01:21:20.891334Z","lead_time":17.574,"prediction":{},"result_count":2,"unique_id":"420cbd67-0442-48fd-bbd8-0baf2a0cdcd5","import_id":null,"last_action":null,"bulk_created":false,"task":5,"project":1,"updated_by":1,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"6777d94c-test-01.txt","drafts":[],"predictions":[],"data":{"text":"解碼器的作用是基於編碼器輸出的特徵表示，生成輸出序列。其結構與編碼器類似，但加入了一些特定的設計：遮罩機制（Masked Attention）保證解碼時只能訪問當前時間步之前的輸出，以維持因果性；多頭注意力（Multi-Head Attention）則分為自注意力和用於整合來自編碼器輸出的跨注意力層。"},"meta":{},"created_at":"2025-12-11T08:25:59.719750Z","updated_at":"2025-12-12T01:21:25.087907Z","inner_id":5,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":1,"updated_by":1,"comment_authors":[]},{"id":6,"annotations":[{"id":9,"completed_by":1,"result":[{"value":{"start":58,"end":62,"text":"查詢向量","labels":["SYSTEM_COMPONENT"]},"id":"E6X87Xvwk0","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":64,"end":67,"text":"鍵向量","labels":["SYSTEM_COMPONENT"]},"id":"SC9gy76f-R","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":69,"end":72,"text":"值向量","labels":["SYSTEM_COMPONENT"]},"id":"5sMfKFAjlt","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-12-12T01:21:50.265184Z","updated_at":"2025-12-12T01:21:50.265198Z","draft_created_at":"2025-12-12T01:21:38.949111Z","lead_time":23.144,"prediction":{},"result_count":3,"unique_id":"fbcc26d4-9f7e-4258-8a42-ce5af2326827","import_id":null,"last_action":null,"bulk_created":false,"task":6,"project":1,"updated_by":1,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"6777d94c-test-01.txt","drafts":[],"predictions":[],"data":{"text":"Transformer 的核心創新是自注意力機制。其具體過程如下：對於輸入的每個位置，模型通過線性變換生成三個向量：查詢向量 、鍵向量 和值向量 。通過內積計算查詢向量 和鍵向量 的相似性，生成注意力分數，再經 softmax 函數歸一化為權重。公式為：。其中， 是鍵向量的維度，作為縮放因子以穩定梯度。多頭注意力（Multi-Head Attention）進一步捕捉不同的特徵關係，將多個頭的輸出拼接後經過線性變換。"},"meta":{},"created_at":"2025-12-11T08:25:59.719774Z","updated_at":"2025-12-12T01:21:50.292982Z","inner_id":6,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":1,"updated_by":1,"comment_authors":[]},{"id":7,"annotations":[{"id":11,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2025-12-12T01:23:26.800197Z","updated_at":"2025-12-12T01:23:26.800216Z","draft_created_at":null,"lead_time":26.415,"prediction":{},"result_count":0,"unique_id":"b53a26a3-4295-4d38-a12b-13f762a5df34","import_id":null,"last_action":null,"bulk_created":false,"task":7,"project":1,"updated_by":1,"parent_prediction":5,"parent_annotation":null,"last_created_by":null}],"file_upload":"6777d94c-test-01.txt","drafts":[],"predictions":[5],"data":{"text":"Transformer 的成功源於其多重優勢。首先是並行計算能力強，自注意力機制能同時處理整個序列，而不需要像 RNN 那樣逐步計算，極大提高了訓練效率。其次是適應長距離依賴，自注意力能直接計算序列中任意兩個位置之間的關聯，因此能有效建模長距依賴。此外，Transformer 的靈活性與可擴展性使其能夠適應各種下游任務。"},"meta":{},"created_at":"2025-12-11T08:25:59.719799Z","updated_at":"2025-12-12T01:23:26.825608Z","inner_id":7,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":1,"updated_by":1,"comment_authors":[]},{"id":8,"annotations":[{"id":10,"completed_by":1,"result":[{"value":{"start":166,"end":169,"text":"GPT","labels":["TECHNICAL_TERM"]},"id":"OZ7SKoeml7","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":234,"end":236,"text":"T5","labels":["TECHNICAL_TERM"]},"id":"stVZoWDxFC","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":323,"end":326,"text":"ViT","labels":["TECHNICAL_TERM"]},"id":"6gC5T3SkIl","from_name":"label","to_name":"text","type":"labels","origin":"manual"},{"value":{"start":75,"end":79,"text":"BERT","labels":["TECHNICAL_TERM"]},"id":"DZ6D2hnjza","from_name":"label","to_name":"text","type":"labels","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-12-12T01:22:57.340449Z","updated_at":"2025-12-12T01:22:57.340467Z","draft_created_at":"2025-12-12T01:22:28.544135Z","lead_time":49.554,"prediction":{},"result_count":4,"unique_id":"4762afab-3cf3-4207-af45-a4aab7d06116","import_id":null,"last_action":null,"bulk_created":false,"task":8,"project":1,"updated_by":1,"parent_prediction":6,"parent_annotation":null,"last_created_by":null}],"file_upload":"6777d94c-test-01.txt","drafts":[],"predictions":[6],"data":{"text":"Transformer 在 NLP 領域的應用非常廣泛，從最初的機器翻譯到後來的文本生成和分類等任務。著名的基於 Transformer 的模型包括 BERT（Bidirectional Encoder Representations from Transformers），其採用雙向編碼器來學習上下文信息，適合於分類和問答等任務；GPT（Generative Pre-trained Transformer），專注於生成任務，採用單向解碼器結構，擅長文本生成；以及 T5（Text-To-Text Transfer Transformer），統一了 NLP 任務為文本到文本的形式，進一步提升靈活性。此外，Vision Transformer（ViT）將 Transformer 應用於計算機視覺領域，通過將圖像分割成小塊後嵌入模型中處理"},"meta":{},"created_at":"2025-12-11T08:25:59.719825Z","updated_at":"2025-12-12T01:22:57.383169Z","inner_id":8,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":1,"updated_by":1,"comment_authors":[]},{"id":9,"annotations":[{"id":12,"completed_by":1,"result":[],"was_cancelled":false,"ground_truth":false,"created_at":"2025-12-12T01:23:46.897374Z","updated_at":"2025-12-12T01:23:46.897391Z","draft_created_at":null,"lead_time":13.399,"prediction":{},"result_count":0,"unique_id":"fc21ce5b-5759-4745-bf45-546d81d5a11a","import_id":null,"last_action":null,"bulk_created":false,"task":9,"project":1,"updated_by":1,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"6777d94c-test-01.txt","drafts":[],"predictions":[],"data":{"text":"Transformer 的未來發展方向包括提高計算效率、減少能耗，以及探索其他應用領域如生物信息學和強化學習等。儘管目前面臨著高計算成本和硬體要求的挑戰，研究人員正在開發更高效的變體（如 Sparse Transformer）以及新的硬體支持來應對這些問題。總之，Transformer 作為一種突破性的神經網路架構，徹底改變了深度學習的格局，並有望在更多領域中發揮關鍵作用。"},"meta":{},"created_at":"2025-12-11T08:25:59.719849Z","updated_at":"2025-12-12T01:23:46.922266Z","inner_id":9,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":1,"updated_by":1,"comment_authors":[]}]